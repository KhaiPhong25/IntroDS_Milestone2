{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0fac0d6",
   "metadata": {},
   "source": [
    "## Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7be47b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List\n",
    "from collections import defaultdict\n",
    "\n",
    "# Add project root to path for imports\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Import pipeline modules\n",
    "from src.scanner.dataset_scanner import scan_dataset\n",
    "from src.parser.version_resolver import resolve_version\n",
    "from src.parser.hierarchy_parser import parse_tex_files, save_tree_to_cache, load_tree_from_cache\n",
    "from src.parser.id_assigner import fast_normalize_and_id, build_content_index, deduplicate_tree\n",
    "\n",
    "# Configuration\n",
    "RAW_ROOT = \"../30-paper\"\n",
    "OUTPUT_DIR = \"23127453\"\n",
    "CACHE_DIR = \".cache\"\n",
    "USING_SHA256_HASH = False\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beb22ab",
   "metadata": {},
   "source": [
    "## STEP 0 - Dataset Scanning\n",
    "\n",
    "Scan the raw dataset to identify valid publications with LaTeX sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69aca721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATASET SCAN RESULTS\n",
      "============================================================\n",
      "Total publications:     30\n",
      "  ├─ READY:             29\n",
      "  ├─ NO_TEX:            1\n",
      "  └─ INVALID:           0\n",
      "============================================================\n",
      "\n",
      "Sample publications:\n",
      "  2211-13747: NO_TEX - 0 version(s)\n",
      "  2211-13748: READY - 1 version(s)\n",
      "  2211-13749: READY - 1 version(s)\n",
      "  2211-13750: READY - 2 version(s)\n",
      "  2211-13751: READY - 1 version(s)\n"
     ]
    }
   ],
   "source": [
    "# Scan the dataset\n",
    "scan_result = scan_dataset(RAW_ROOT)\n",
    "\n",
    "# Print statistics\n",
    "total_pubs = len(scan_result)\n",
    "ready_pubs = sum(1 for info in scan_result.values() if info[\"status\"] == \"READY\")\n",
    "no_tex_pubs = sum(1 for info in scan_result.values() if info[\"status\"] == \"NO_TEX\")\n",
    "invalid_pubs = sum(1 for info in scan_result.values() if info[\"status\"] == \"INVALID\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"DATASET SCAN RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total publications:     {total_pubs}\")\n",
    "print(f\"  ├─ READY:             {ready_pubs}\")\n",
    "print(f\"  ├─ NO_TEX:            {no_tex_pubs}\")\n",
    "print(f\"  └─ INVALID:           {invalid_pubs}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Preview first few\n",
    "print(\"Sample publications:\")\n",
    "for i, (pub_id, info) in enumerate(list(scan_result.items())[:5]):\n",
    "    print(f\"  {pub_id}: {info['status']} - {len(info['versions'])} version(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde1af6d",
   "metadata": {},
   "source": [
    "## STEP 1 - Version Resolution\n",
    "\n",
    "Resolve LaTeX file structure for each version of each publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c9af8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STEP 1: Version Resolution: 100%|██████████| 29/29 [00:00<00:00, 437.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resolved 42/42 versions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "step1_results = []\n",
    "\n",
    "# Filter READY publications\n",
    "ready_items = [\n",
    "    (pub_id, info)\n",
    "    for pub_id, info in scan_result.items()\n",
    "    if info[\"status\"] == \"READY\"\n",
    "]\n",
    "\n",
    "# Resolve each version\n",
    "for pub_id, info in tqdm(ready_items, desc=\"STEP 1: Version Resolution\"):\n",
    "    for version in info[\"versions\"]:\n",
    "        version_path = os.path.join(RAW_ROOT, pub_id, \"tex\", version)\n",
    "        \n",
    "        result = resolve_version(\n",
    "            publication_id=pub_id,\n",
    "            version_name=version,\n",
    "            version_path=version_path\n",
    "        )\n",
    "        \n",
    "        step1_results.append(result)\n",
    "\n",
    "# Statistics\n",
    "resolved_count = sum(1 for r in step1_results if r[\"status\"] == \"RESOLVED\")\n",
    "print(f\"\\nResolved {resolved_count}/{len(step1_results)} versions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e77ac2",
   "metadata": {},
   "source": [
    "## STEP 2 - LaTeX Hierarchy Parsing\n",
    "\n",
    "Parse each LaTeX version into a hierarchical tree structure and cache to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "165177da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STEP 2: Parsing LaTeX: 100%|██████████| 42/42 [00:01<00:00, 24.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsed 42 versions successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "step2_results = []\n",
    "\n",
    "for version_info in tqdm(step1_results, desc=\"STEP 2: Parsing LaTeX\"):\n",
    "    if version_info[\"status\"] != \"RESOLVED\":\n",
    "        continue\n",
    "    \n",
    "    version_path = os.path.join(\n",
    "        RAW_ROOT,\n",
    "        version_info[\"publication_id\"],\n",
    "        \"tex\",\n",
    "        version_info[\"version\"]\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Parse LaTeX files into hierarchy tree\n",
    "        root_node = parse_tex_files(\n",
    "            version_path=version_path,\n",
    "            tex_files=version_info[\"used_tex_files\"]\n",
    "        )\n",
    "        \n",
    "        # Save to disk cache (memory efficient)\n",
    "        cache_path = save_tree_to_cache(\n",
    "            pub_id=version_info[\"publication_id\"],\n",
    "            version=version_info[\"version\"],\n",
    "            root=root_node\n",
    "        )\n",
    "        \n",
    "        step2_results.append({\n",
    "            \"publication_id\": version_info[\"publication_id\"],\n",
    "            \"version\": version_info[\"version\"],\n",
    "            \"cache_path\": cache_path\n",
    "        })\n",
    "        \n",
    "        # Free memory\n",
    "        del root_node\n",
    "        gc.collect()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR] {version_info['publication_id']}/{version_info['version']}: {e}\")\n",
    "\n",
    "print(f\"\\nParsed {len(step2_results)} versions successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f27abdf",
   "metadata": {},
   "source": [
    "## STEP 3 - Deduplication & ID Assignment\n",
    "\n",
    "Merge versions per publication with content-based deduplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af7970a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STEP 3: Deduplication: 100%|██████████| 29/29 [00:02<00:00, 12.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merged 29 publications with deduplication\n"
     ]
    }
   ],
   "source": [
    "# Initialize hash cache if using SHA256\n",
    "if USING_SHA256_HASH:\n",
    "    _hash_cache = {}\n",
    "\n",
    "# Group versions by publication\n",
    "pub_groups = defaultdict(list)\n",
    "for item in step2_results:\n",
    "    pub_groups[item[\"publication_id\"]].append(item)\n",
    "\n",
    "final_trees = {}\n",
    "\n",
    "# Process each publication\n",
    "for pub_id, versions in tqdm(pub_groups.items(), desc=\"STEP 3: Deduplication\"):\n",
    "    if not versions:\n",
    "        continue\n",
    "    \n",
    "    # Sort versions chronologically\n",
    "    versions.sort(\n",
    "        key=lambda x: int(x[\"version\"].split(\"v\")[-1]) if \"v\" in x[\"version\"] else 0\n",
    "    )\n",
    "    \n",
    "    # Load and normalize base tree (first version)\n",
    "    base_info = versions[0]\n",
    "    base_root = load_tree_from_cache(base_info[\"cache_path\"])\n",
    "    fast_normalize_and_id(base_root, pub_id, base_info[\"version\"])\n",
    "    \n",
    "    # Build content index for deduplication\n",
    "    content_index = build_content_index(base_root, USING_SHA256_HASH)\n",
    "    \n",
    "    # Merge subsequent versions\n",
    "    for v_info in versions[1:]:\n",
    "        root = load_tree_from_cache(v_info[\"cache_path\"])\n",
    "        fast_normalize_and_id(root, pub_id, v_info[\"version\"])\n",
    "        deduplicate_tree(root, content_index, USING_SHA256_HASH)\n",
    "        del root\n",
    "    \n",
    "    final_trees[pub_id] = base_root\n",
    "    \n",
    "    # Periodic garbage collection\n",
    "    if len(final_trees) % 50 == 0:\n",
    "        gc.collect()\n",
    "\n",
    "# Cleanup\n",
    "if USING_SHA256_HASH:\n",
    "    del _hash_cache\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\nMerged {len(final_trees)} publications with deduplication\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f167aeb",
   "metadata": {},
   "source": [
    "## Validation - Quick Check\n",
    "\n",
    "Verify the parsing results for a sample publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b676ef5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Publication: 2211-13748\n",
      "  Root type:        document\n",
      "  Title:            None\n",
      "  Children:         16\n",
      "  ID:               2211-13748_2211-13748v1_000000\n",
      "\n",
      "  Node type distribution:\n",
      "    document       :     1\n",
      "    enumerate      :     4\n",
      "    figure         :     4\n",
      "    section        :     7\n",
      "    sentence       :   209\n",
      "    subsection     :    10\n",
      "    subsubsection  :    14\n",
      "    table          :     3\n"
     ]
    }
   ],
   "source": [
    "# Sample one publication for inspection\n",
    "if final_trees:\n",
    "    sample_pub = next(iter(final_trees))\n",
    "    root = final_trees[sample_pub]\n",
    "    \n",
    "    print(f\"Sample Publication: {sample_pub}\")\n",
    "    print(f\"  Root type:        {root.node_type}\")\n",
    "    print(f\"  Title:            {root.title if hasattr(root, 'title') else 'N/A'}\")\n",
    "    print(f\"  Children:         {len(root.children)}\")\n",
    "    print(f\"  ID:               {root.id if hasattr(root, 'id') else 'N/A'}\")\n",
    "    \n",
    "    # Count node types\n",
    "    def count_nodes(node, counts=None):\n",
    "        if counts is None:\n",
    "            counts = defaultdict(int)\n",
    "        counts[node.node_type] += 1\n",
    "        for child in node.children:\n",
    "            count_nodes(child, counts)\n",
    "        return counts\n",
    "    \n",
    "    node_counts = count_nodes(root)\n",
    "    print(\"\\n  Node type distribution:\")\n",
    "    for node_type, count in sorted(node_counts.items()):\n",
    "        print(f\"    {node_type:15s}: {count:5d}\")\n",
    "else:\n",
    "    print(\"No publications parsed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a236c062",
   "metadata": {},
   "source": [
    "## STEP 4 - Export to JSON\n",
    "\n",
    "Export final trees to structured JSON files for downstream processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8adaafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting publications to output directory...\n",
      "Target: c:\\Users\\Feng Wang\\OneDrive\\Documents\\HK7 HCMUS\\Introduction to Data\\Project\\Milestone 2\\-IntroDS-_milestone2\\src\\23127453\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STEP 4: Exporting: 100%|██████████| 29/29 [00:00<00:00, 227.05pub/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported 29 publications to: 23127453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Serialization helper\n",
    "def serialize_node(node):\n",
    "    \"\"\"Recursively serialize a node to dictionary.\"\"\"\n",
    "    return {\n",
    "        \"id\": node.id,\n",
    "        \"type\": node.node_type,\n",
    "        \"full_text\": getattr(node, \"full_text\", \"\"),\n",
    "        \"children\": [serialize_node(child) for child in node.children]\n",
    "    }\n",
    "\n",
    "# Export statistics\n",
    "export_count = 0\n",
    "missing_metadata = []\n",
    "missing_references = []\n",
    "\n",
    "print(\"Exporting publications to output directory...\")\n",
    "print(f\"Target: {os.path.abspath(OUTPUT_DIR)}\\n\")\n",
    "\n",
    "for pub_id, root in tqdm(final_trees.items(), desc=\"STEP 4: Exporting\", unit=\"pub\"):\n",
    "    \n",
    "    # Create publication subdirectory\n",
    "    pub_output_dir = os.path.join(OUTPUT_DIR, pub_id)\n",
    "    os.makedirs(pub_output_dir, exist_ok=True)\n",
    "    \n",
    "    # ===== 1. Export Content Tree (Parsed Hierarchy) =====\n",
    "    content_json = {\n",
    "        \"publication_id\": pub_id,\n",
    "        \"content_tree\": serialize_node(root)\n",
    "    }\n",
    "    \n",
    "    content_path = os.path.join(pub_output_dir, f\"{pub_id}.json\")\n",
    "    with open(content_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(content_json, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # ===== 2. Copy Metadata (Original from Semantic Scholar) =====\n",
    "    raw_metadata_path = os.path.join(RAW_ROOT, pub_id, \"metadata.json\")\n",
    "    metadata_path = os.path.join(pub_output_dir, \"metadata.json\")\n",
    "    \n",
    "    if os.path.exists(raw_metadata_path):\n",
    "        with open(raw_metadata_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            metadata = json.load(f)\n",
    "        with open(metadata_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "    else:\n",
    "        missing_metadata.append(pub_id)\n",
    "    \n",
    "    # ===== 3. Copy References (Original from Semantic Scholar) =====\n",
    "    raw_references_path = os.path.join(RAW_ROOT, pub_id, \"references.json\")\n",
    "    references_path = os.path.join(pub_output_dir, \"references.json\")\n",
    "    \n",
    "    if os.path.exists(raw_references_path):\n",
    "        with open(raw_references_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            references = json.load(f)\n",
    "        with open(references_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(references, f, ensure_ascii=False, indent=2)\n",
    "    else:\n",
    "        missing_references.append(pub_id)\n",
    "    \n",
    "    export_count += 1\n",
    "\n",
    "print(f\"Exported {export_count} publications to: {OUTPUT_DIR}\")\n",
    "\n",
    "if missing_metadata:\n",
    "    print(f\"\\nWARNING: {len(missing_metadata)} publication(s) missing metadata.json\")\n",
    "    print(f\"   {', '.join(missing_metadata[:5])}\" + (\" ...\" if len(missing_metadata) > 5 else \"\"))\n",
    "\n",
    "if missing_references:\n",
    "    print(f\"\\nWARNING: {len(missing_references)} publication(s) missing references.json\")\n",
    "    print(f\"   {', '.join(missing_references[:5])}\" + (\" ...\" if len(missing_references) > 5 else \"\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
