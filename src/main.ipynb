{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccc8a3f4",
   "metadata": {},
   "source": [
    "# <center>**Milestone 2**<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b11d9f",
   "metadata": {},
   "source": [
    "## **Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7be47b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Feng\n",
      "[nltk_data]     Wang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Feng\n",
      "[nltk_data]     Wang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Feng\n",
      "[nltk_data]     Wang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Feng\n",
      "[nltk_data]     Wang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Feng\n",
      "[nltk_data]     Wang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import gc\n",
    "import pickle\n",
    "import sys\n",
    "import random\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from __future__ import annotations\n",
    "from collections import defaultdict\n",
    "\n",
    "from my_scanner import *\n",
    "from my_parser import *\n",
    "from my_matcher import *\n",
    "from my_featureEngineering import *\n",
    "from my_modeling import *\n",
    "from my_evaluation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beb22ab",
   "metadata": {},
   "source": [
    "## **Configuration**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69aca721",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_ROOT = \"../30-paper\"\n",
    "USING_SHA256_HASH = False\n",
    "\n",
    "# 1. Output directory\n",
    "OUTPUT_DIR = \"23127453\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Configuration for caching\n",
    "CACHE_DIR = \".cache\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "sys.setrecursionlimit(20000)\n",
    "\n",
    "MANUAL_DIR = \"manual_labeling\"\n",
    "os.makedirs(MANUAL_DIR, exist_ok=True)\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde1af6d",
   "metadata": {},
   "source": [
    "## **Dataset rescan**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb2d3ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2211-13747 NO_TEX []\n",
      "2211-13748 READY ['2211-13748v1']\n",
      "2211-13749 READY ['2211-13749v1']\n",
      "2211-13750 READY ['2211-13750v1', '2211-13750v2']\n",
      "2211-13751 READY ['2211-13751v1']\n",
      "2211-13752 READY ['2211-13752v1']\n",
      "2211-13753 READY ['2211-13753v1']\n",
      "2211-13754 READY ['2211-13754v1']\n",
      "2211-13755 READY ['2211-13755v1', '2211-13755v2']\n",
      "2211-13756 READY ['2211-13756v1', '2211-13756v2']\n",
      "2211-13757 READY ['2211-13757v1', '2211-13757v2']\n",
      "2211-13758 READY ['2211-13758v1']\n",
      "2211-13759 READY ['2211-13759v1', '2211-13759v2']\n",
      "2211-13760 READY ['2211-13760v1', '2211-13760v2']\n",
      "2211-13761 READY ['2211-13761v1']\n",
      "2211-13762 READY ['2211-13762v1', '2211-13762v2']\n",
      "2211-13763 READY ['2211-13763v1']\n",
      "2211-13764 READY ['2211-13764v1']\n",
      "2211-13765 READY ['2211-13765v1']\n",
      "2211-13766 READY ['2211-13766v1', '2211-13766v2', '2211-13766v3']\n",
      "2211-13767 READY ['2211-13767v1']\n",
      "2211-13768 READY ['2211-13768v1', '2211-13768v2']\n",
      "2211-13769 READY ['2211-13769v1', '2211-13769v2']\n",
      "2211-13770 READY ['2211-13770v1']\n",
      "2211-13771 READY ['2211-13771v1']\n",
      "2211-13772 READY ['2211-13772v1']\n",
      "2211-13773 READY ['2211-13773v1', '2211-13773v2']\n",
      "2211-13774 READY ['2211-13774v1']\n",
      "2211-13775 READY ['2211-13775v1', '2211-13775v2']\n",
      "2211-13776 READY ['2211-13776v1']\n"
     ]
    }
   ],
   "source": [
    "scan_result = scan_dataset(RAW_ROOT)\n",
    "\n",
    "for k, v in scan_result.items():\n",
    "    print(k, v[\"status\"], v[\"versions\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857cf672",
   "metadata": {},
   "source": [
    "## **Version-level Multi-file Resolver**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c3907b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Publications: 100%|██████████| 29/29 [00:00<00:00, 191.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2211-13748 2211-13748v1 ['weibo.tex']\n",
      "2211-13749 2211-13749v1 ['On_projections_of_tailsvA.tex']\n",
      "2211-13750 2211-13750v1 ['comparingsinglettestingschemes.tex']\n",
      "2211-13750 2211-13750v2 ['comparingsinglettestingschemes4.tex']\n",
      "2211-13751 2211-13751v1 ['Kane_et_al_JFM_v1.tex']\n",
      "2211-13752 2211-13752v1 ['main.tex', 'macros.tex', '00_abstract.tex', '01_intro.tex', '02_related.tex', '03_method.tex', '04_results.tex', '05_conclusion.tex', '07_acc.tex', '06_appendix.tex']\n",
      "2211-13753 2211-13753v1 ['0-title.tex']\n",
      "2211-13754 2211-13754v1 ['main.tex']\n",
      "2211-13755 2211-13755v1 ['main.tex']\n",
      "2211-13755 2211-13755v2 ['main.tex']\n",
      "2211-13756 2211-13756v1 ['main.tex']\n",
      "2211-13756 2211-13756v2 ['main.tex']\n",
      "2211-13757 2211-13757v1 ['main.tex']\n",
      "2211-13757 2211-13757v2 ['main.tex']\n",
      "2211-13758 2211-13758v1 ['main.tex']\n",
      "2211-13759 2211-13759v1 ['main.tex']\n",
      "2211-13759 2211-13759v2 ['main.tex']\n",
      "2211-13760 2211-13760v1 ['main.tex']\n",
      "2211-13760 2211-13760v2 ['main.tex']\n",
      "2211-13761 2211-13761v1 ['main.tex']\n",
      "2211-13762 2211-13762v1 ['main.tex', 'overall.tex', 'splits.tex']\n",
      "2211-13762 2211-13762v2 ['main.tex', 'overall.tex', 'splits.tex']\n",
      "2211-13763 2211-13763v1 ['main.tex', 'table.tex']\n",
      "2211-13764 2211-13764v1 ['accepted.tex']\n",
      "2211-13765 2211-13765v1 ['main.tex']\n",
      "2211-13766 2211-13766v1 ['PRL_Version.tex']\n",
      "2211-13766 2211-13766v2 ['arXiv_Version.tex']\n",
      "2211-13766 2211-13766v3 ['arXiv_Version.tex']\n",
      "2211-13767 2211-13767v1 ['main_arxiv.tex']\n",
      "2211-13768 2211-13768v1 ['main.tex', 'hyperlink-year-only-natbib-patch.tex']\n",
      "2211-13768 2211-13768v2 ['main.tex', 'hyperlink-year-only-natbib-patch.tex']\n",
      "2211-13769 2211-13769v1 ['main.tex', 'abstract.tex', 'intro.tex', 'conclude.tex', 'siamfc.tex']\n",
      "2211-13769 2211-13769v2 ['Appendix.tex', 'ostrack.tex', 'dimp_new.tex', 'stark.tex']\n",
      "2211-13770 2211-13770v1 ['skeleton.tex']\n",
      "2211-13771 2211-13771v1 ['paper.tex', 'penrose.tex']\n",
      "2211-13772 2211-13772v1 ['main.tex']\n",
      "2211-13773 2211-13773v1 ['draft_single.tex']\n",
      "2211-13773 2211-13773v2 ['draft_single.tex']\n",
      "2211-13774 2211-13774v1 ['Appendices.tex']\n",
      "2211-13775 2211-13775v1 ['main.tex', 'macros.tex', '00_abstract.tex', '01_introduction.tex', 'teaser_pdf.tex', '02_related_work.tex', '03_method.tex', 'diagram_pdf.tex', '04_results.tex', 'attack_pdf.tex', 'attacks_comp_pdf.tex', 'pc_comparison.tex', 'classifier_evaluation.tex', 'detector_evaluation.tex', 'coma_beta_pdf.tex', 'transferability_pdf.tex', '05_conclusions.tex', 'supplementary.tex', 'supp_analysis.tex', 'coma_tsne_pdf.tex', 'smal_beta_pdf.tex', 'oods_pdf.tex', 'oods.tex', 'transferability_geometric.tex', 'transferability_semantic.tex', 'semantic_comp_pdf.tex', 'coma_freq_comp_pdf.tex', 'supp_ablation.tex', 'freq_ablation_pdf.tex', 'reg_comp_pdf.tex', 'reg_comp.tex', 'euclidean_comp.tex', 'self_basis_comp.tex', 'random_targets.tex', 'supp_settings.tex', 'supp_results.tex', 'stability.tex', 'coma_stability_pdf.tex', 'smal_stability_pdf.tex', 'coma_failure_pdf.tex', 'smal_failure_pdf.tex', 'coma_evolution_pdf.tex', 'smal_evolution_pdf.tex', 'quiz_pdf.tex', 'coma_concat_1_pdf.tex', 'coma_concat_2_pdf.tex', 'smal_concat_1_pdf.tex', 'smal_concat_2_pdf.tex']\n",
      "2211-13775 2211-13775v2 ['egpaper_final.tex', 'macros.tex', '00_abstract.tex', '01_introduction.tex', 'teaser_pdf.tex', '02_related_work.tex', '03_method.tex', 'diagram_pdf.tex', 'quiz_main_pdf.tex', '04_results.tex', 'attack_pdf.tex', 'attacks_comp_pdf.tex', 'classifier_evaluation.tex', 'detector_evaluation.tex', 'semantic_comp_pdf.tex', 'transferability_pdf.tex', 'lpf_defense_pdf.tex', 'beta_pdf.tex', '05_conclusions.tex', 'supplementary.tex', '01_supp_analysis.tex', 'pc_comparison.tex', 'coma_tsne_pdf.tex', 'oods_pdf.tex', 'oods.tex', 'transferability_geometric.tex', 'transferability_semantic.tex', '02_supp_ablation.tex', 'coma_freq_comp_pdf.tex', 'freq_ablation_pdf.tex', 'reg_comp_pdf.tex', 'reg_comp.tex', 'euclidean_comp.tex', 'self_basis_comp.tex', 'random_targets.tex', '03_supp_settings.tex', '04_supp_results.tex', 'stability.tex', 'quiz_answers_pdf.tex', 'coma_stability_pdf.tex', 'smal_stability_pdf.tex', 'coma_failure_pdf.tex', 'smal_failure_pdf.tex', 'coma_evolution_pdf.tex', 'smal_evolution_pdf.tex', 'coma_concat_1_pdf.tex', 'coma_concat_2_pdf.tex', 'smal_concat_1_pdf.tex', 'smal_concat_2_pdf.tex']\n",
      "2211-13776 2211-13776v1 ['acl2020.tex']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pub_results = []\n",
    "\n",
    "ready_items = [\n",
    "    (pub_id, info)\n",
    "    for pub_id, info in scan_result.items()\n",
    "    if info[\"status\"] == \"READY\"\n",
    "]\n",
    "\n",
    "for pub_id, info in tqdm(ready_items, desc=\"Publications\"):\n",
    "    for version in info[\"versions\"]:\n",
    "        version_path = f\"{RAW_ROOT}/{pub_id}/tex/{version}\"\n",
    "\n",
    "        result = resolve_version(\n",
    "            publication_id=pub_id,\n",
    "            version_name=version,\n",
    "            version_path=version_path\n",
    "        )\n",
    "\n",
    "        pub_results.append(result)\n",
    "\n",
    "# Xem kết quả\n",
    "for r in pub_results:\n",
    "    print(r[\"publication_id\"], r[\"version\"], r[\"used_tex_files\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0e8b68",
   "metadata": {},
   "source": [
    "## **LaTeX Hierarchy Parser**\n",
    "\n",
    "We parsed each LaTeX version into a hierarchical tree structure. Sectioning commands were treated as internal nodes, while only sentences, tables, and figures were considered leaf nodes, strictly following the seminar constraints. No deduplication or identifier assignment was performed at this stage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ce533b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tree_to_cache(pub_id: str, version: str, root: FileNode) -> str:\n",
    "    \"\"\"\n",
    "    Serialize tree to disk and return cache path.\n",
    "    Returns the cache file path for later retrieval.\n",
    "    \"\"\"\n",
    "    cache_path = os.path.join(CACHE_DIR, f\"{pub_id}_{version}.pkl\")\n",
    "    with open(cache_path, \"wb\") as f:\n",
    "        pickle.dump(root, f, protocol=4)\n",
    "    return cache_path\n",
    "\n",
    "def load_tree_from_cache(cache_path: str) -> FileNode:\n",
    "    \"\"\"Load tree from cache file.\"\"\"\n",
    "    with open(cache_path, \"rb\") as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "257bdcc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing LaTeX versions: 100%|██████████| 42/42 [00:03<00:00, 12.14version(s)/s]\n"
     ]
    }
   ],
   "source": [
    "parsed_versions = []\n",
    "\n",
    "\n",
    "for version_info in tqdm(\n",
    "    pub_results,\n",
    "    desc=\"Parsing LaTeX versions\",\n",
    "    unit=\"version(s)\"\n",
    "):\n",
    "    if version_info[\"status\"] != \"RESOLVED\":\n",
    "        continue\n",
    "\n",
    "    version_path = (\n",
    "        f\"{RAW_ROOT}/\"\n",
    "        f\"{version_info['publication_id']}/tex/\"\n",
    "        f\"{version_info['version']}\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        root_node = parse_tex_files(\n",
    "            version_path=version_path,\n",
    "            tex_files=version_info[\"used_tex_files\"]\n",
    "        )\n",
    "\n",
    "        # Save to disk instead of keeping in memory\n",
    "        cache_path = save_tree_to_cache(\n",
    "            pub_id=version_info[\"publication_id\"],\n",
    "            version=version_info[\"version\"],\n",
    "            root=root_node\n",
    "        )\n",
    "\n",
    "        parsed_versions.append({\n",
    "            \"publication_id\": version_info[\"publication_id\"],\n",
    "            \"version\": version_info[\"version\"],\n",
    "            \"cache_path\": cache_path,  # Store path, not tree\n",
    "            \"root\": None  # Placeholder\n",
    "        })\n",
    "\n",
    "        # Explicitly free memory\n",
    "        del root_node\n",
    "        gc.collect()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"[ERROR] {version_info['publication_id']} \"\n",
    "            f\"{version_info['version']}: {e}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f27abdf",
   "metadata": {},
   "source": [
    "## **Deduplication & ID Assignment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af7970a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deduplicating: 100%|██████████| 29/29 [00:02<00:00, 12.20publication(s)/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: merged 29 publications\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 3 - Deduplication & Merge trees per publication\n",
    "\n",
    "Input:\n",
    "    parsed_versions: list of {\n",
    "        publication_id,\n",
    "        version,\n",
    "        root (hierarchy tree)\n",
    "    }\n",
    "\n",
    "Output:\n",
    "    final_trees: dict[publication_id] -> merged root tree\n",
    "\"\"\"\n",
    "\n",
    "# Group trees by publication_id\n",
    "pub_groups = defaultdict(list)\n",
    "for item in parsed_versions:\n",
    "    pub_groups[item[\"publication_id\"]].append(item)\n",
    "\n",
    "final_trees = {}\n",
    "\n",
    "for pub_id, versions in tqdm(pub_groups.items(), desc=\"Deduplicating\", unit=\"publication(s)\"):\n",
    "    if not versions:\n",
    "        continue\n",
    "        \n",
    "    # Sort versions\n",
    "    versions.sort(key=lambda x: int(x[\"version\"].split(\"v\")[-1]) if \"v\" in x[\"version\"] else 0)\n",
    "\n",
    "    # Load Base Tree\n",
    "    base_info = versions[0]\n",
    "    with open(base_info[\"cache_path\"], \"rb\") as f:\n",
    "        base_root = pickle.load(f)\n",
    "\n",
    "    # Normalize Base\n",
    "    fast_normalize_and_id(base_root, pub_id, base_info[\"version\"])\n",
    "    \n",
    "    # Build Index\n",
    "    content_index = build_content_index(base_root, USING_SHA256_HASH)\n",
    "\n",
    "    # Merge subsequent versions\n",
    "    for v_info in versions[1:]:\n",
    "        with open(v_info[\"cache_path\"], \"rb\") as f:\n",
    "            root = pickle.load(f)\n",
    "\n",
    "        # Normalize Source\n",
    "        fast_normalize_and_id(root, pub_id, v_info[\"version\"])\n",
    "        \n",
    "        # Deduplicate\n",
    "        deduplicate_tree(root, content_index, USING_SHA256_HASH)\n",
    "        \n",
    "        # Explicit cleanup\n",
    "        del root\n",
    "    \n",
    "    final_trees[pub_id] = base_root\n",
    "    \n",
    "    # Periodic GC to prevent memory fragmentation on large datasets\n",
    "    if len(final_trees) % 50 == 0:\n",
    "        gc.collect()\n",
    "\n",
    "print(f\"DONE: merged {len(final_trees)} publications\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3d4e584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample publication: 2211-13748\n",
      "Root node type: document\n",
      "Number of children: 16\n"
     ]
    }
   ],
   "source": [
    "# Kiểm tra nhanh\n",
    "sample_pub = next(iter(final_trees))\n",
    "root = final_trees[sample_pub]\n",
    "\n",
    "print(\"Sample publication:\", sample_pub)\n",
    "print(\"Root node type:\", root.node_type)\n",
    "print(\"Number of children:\", len(root.children))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0581f8",
   "metadata": {},
   "source": [
    "## **Export to JSON**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b676ef5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exporting JSON: 100%|██████████| 29/29 [00:00<00:00, 157.03pub/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported 29 publications to: 23127453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Export Structured Publication JSON\n",
    "\n",
    "Input:\n",
    "    final_trees: dict[publication_id] -> root_node\n",
    "\n",
    "Output:\n",
    "    Student ID/\n",
    "        <pub_id>/\n",
    "            <pub_id>.json\n",
    "            metadata.json\n",
    "            references.json\n",
    "        ...\n",
    "\"\"\"\n",
    "\n",
    "# Serialization helper\n",
    "def serialize_node(node):\n",
    "    \"\"\"Recursively serialize a node to dictionary.\"\"\"\n",
    "    return {\n",
    "        \"id\": node.id,\n",
    "        \"type\": node.node_type,\n",
    "        \"full_text\": getattr(node, \"full_text\", \"\"),\n",
    "        \"children\": [serialize_node(child) for child in node.children]\n",
    "    }\n",
    "\n",
    "# Export statistics\n",
    "export_count = 0\n",
    "missing_metadata = []\n",
    "missing_references = []\n",
    "\n",
    "for pub_id, root in tqdm(final_trees.items(), desc=\"Exporting JSON\", unit=\"pub\"):\n",
    "    \n",
    "    # Create publication subdirectory\n",
    "    pub_output_dir = os.path.join(OUTPUT_DIR, pub_id)\n",
    "    os.makedirs(pub_output_dir, exist_ok=True)\n",
    "    \n",
    "    # ===== 1. Export Content Tree (Parsed Hierarchy) =====\n",
    "    content_json = {\n",
    "        \"publication_id\": pub_id,\n",
    "        \"content_tree\": serialize_node(root)\n",
    "    }\n",
    "    \n",
    "    content_path = os.path.join(pub_output_dir, f\"{pub_id}.json\")\n",
    "    with open(content_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(content_json, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # ===== 2. Copy Metadata (Original from Semantic Scholar) =====\n",
    "    raw_metadata_path = os.path.join(RAW_ROOT, pub_id, \"metadata.json\")\n",
    "    metadata_path = os.path.join(pub_output_dir, \"metadata.json\")\n",
    "    \n",
    "    if os.path.exists(raw_metadata_path):\n",
    "        with open(raw_metadata_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            metadata = json.load(f)\n",
    "        with open(metadata_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "    else:\n",
    "        missing_metadata.append(pub_id)\n",
    "    \n",
    "    # ===== 3. Copy References (Original from Semantic Scholar) =====\n",
    "    raw_references_path = os.path.join(RAW_ROOT, pub_id, \"references.json\")\n",
    "    references_path = os.path.join(pub_output_dir, \"references.json\")\n",
    "    \n",
    "    if os.path.exists(raw_references_path):\n",
    "        with open(raw_references_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            references = json.load(f)\n",
    "        with open(references_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(references, f, ensure_ascii=False, indent=2)\n",
    "    else:\n",
    "        missing_references.append(pub_id)\n",
    "    \n",
    "    export_count += 1\n",
    "\n",
    "print(f\"Exported {export_count} publications to: {OUTPUT_DIR}\")\n",
    "\n",
    "if missing_metadata:\n",
    "    print(f\"\\nWARNING: {len(missing_metadata)} publication(s) missing metadata.json\")\n",
    "    print(f\"   {', '.join(missing_metadata[:5])}\" + (\" ...\" if len(missing_metadata) > 5 else \"\"))\n",
    "\n",
    "if missing_references:\n",
    "    print(f\"\\nWARNING: {len(missing_references)} publication(s) missing references.json\")\n",
    "    print(f\"   {', '.join(missing_references[:5])}\" + (\" ...\" if len(missing_references) > 5 else \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc87624e",
   "metadata": {},
   "source": [
    "## **Extract References**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74254e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting references: 100%|██████████| 42/42 [00:00<00:00, 70.99version/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Publications found: 26/42\n",
      "Total unique references: 2817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "raw_references = {}\n",
    "extract_errors = 0\n",
    "\n",
    "for version_info in tqdm(pub_results, desc=\"Extracting references\", unit=\"version\"):\n",
    "    \n",
    "    if version_info.get(\"status\") == \"NO_TEX\":\n",
    "        continue\n",
    "\n",
    "    pub_id = version_info[\"publication_id\"]\n",
    "    version = version_info[\"version\"]\n",
    "    \n",
    "    version_path = f\"{RAW_ROOT}/{pub_id}/tex/{version}\"\n",
    "    \n",
    "    if not os.path.exists(version_path):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        all_files_in_dir = os.listdir(version_path)\n",
    "        \n",
    "        target_files = [\n",
    "            f for f in all_files_in_dir \n",
    "            if f.lower().endswith(('.bib', '.bbl', '.tex'))\n",
    "        ]\n",
    "        \n",
    "        references = extract_references_from_tex_files(\n",
    "            version_path=version_path,\n",
    "            tex_files=target_files \n",
    "        )\n",
    "        \n",
    "        if references:\n",
    "            if pub_id not in raw_references:\n",
    "                raw_references[pub_id] = []\n",
    "            raw_references[pub_id].extend(references)\n",
    "            \n",
    "    except Exception as e:\n",
    "        extract_errors += 1\n",
    "        pass\n",
    "\n",
    "deduplicated_references = {}\n",
    "global_key_mapping = {} \n",
    "\n",
    "# Deduplicate references per publication\n",
    "for pub_id, refs in raw_references.items():\n",
    "    dedup_list, key_map = deduplicate_references_with_mapping(refs)\n",
    "    \n",
    "    deduplicated_references[pub_id] = dedup_list\n",
    "    \n",
    "    global_key_mapping.update(key_map)\n",
    "\n",
    "# Assign Semantic IDs\n",
    "global_used_ids = set()\n",
    "\n",
    "for pub_id in deduplicated_references:\n",
    "    for ref in deduplicated_references[pub_id]:\n",
    "        ref['ref_id'] = generate_semantic_id(ref, global_used_ids)\n",
    "\n",
    "count_pubs = len(deduplicated_references)\n",
    "total_refs = sum(len(refs) for refs in deduplicated_references.values())\n",
    "\n",
    "print(f\"\\nPublications found: {count_pubs}/{len(pub_results)}\")\n",
    "print(f\"Total unique references: {total_refs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea53510c",
   "metadata": {},
   "source": [
    "## **Export to BIB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ee3bc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exporting .bib: 100%|██████████| 26/26 [00:00<00:00, 568.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated 'refs.bib' for 26 publications.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "count_exported = 0\n",
    "\n",
    "for pub_id, refs in tqdm(deduplicated_references.items(), desc=\"Exporting .bib\"):\n",
    "    \n",
    "    save_dir = os.path.join(OUTPUT_DIR, pub_id)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    output_path = os.path.join(save_dir, \"refs.bib\")\n",
    "    \n",
    "    try:\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            for ref in refs:\n",
    "                bib_string = export_to_bibtex(ref)\n",
    "                f.write(bib_string)\n",
    "                \n",
    "        count_exported += 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Could not write refs.bib for {pub_id}: {e}\")\n",
    "\n",
    "print(f\"\\nGenerated 'refs.bib' for {count_exported} publications.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58f4370",
   "metadata": {},
   "source": [
    "## **Reference Cleaning**\n",
    "\n",
    "Clean and normalize BibTeX and arXiv references for matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9731aac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning BibTeX: 100%|██████████| 26/26 [00:02<00:00,  9.65pub/s]\n",
      "Loading & Cleaning arXiv Truth: 100%|██████████| 26/26 [00:00<00:00, 141.64pub/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DONE: 2817 cleaned BibTeX entries vs 708 cleaned arXiv entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned_bibtex = {}\n",
    "\n",
    "for pub_id in tqdm(deduplicated_references.keys(), desc=\"Cleaning BibTeX\", unit=\"pub\"):\n",
    "    cleaned_entries = []\n",
    "    for ref in deduplicated_references[pub_id]:\n",
    "        try:\n",
    "            cleaned_ref = clean_bibtex_entry(ref)\n",
    "            cleaned_ref['ref_id'] = ref.get('ref_id', '')\n",
    "            cleaned_ref['key'] = ref.get('key', '')\n",
    "            cleaned_ref['all_keys'] = ref.get('all_keys', [])\n",
    "            \n",
    "            cleaned_entries.append(cleaned_ref)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "            \n",
    "    cleaned_bibtex[pub_id] = cleaned_entries\n",
    "\n",
    "cleaned_arxiv = {}\n",
    "\n",
    "for pub_id in tqdm(cleaned_bibtex.keys(), desc=\"Loading & Cleaning arXiv Truth\", unit=\"pub\"):\n",
    "    ref_json_path = os.path.join(RAW_ROOT, pub_id, \"references.json\")\n",
    "    \n",
    "    if os.path.exists(ref_json_path):\n",
    "        try:\n",
    "            with open(ref_json_path, 'r', encoding='utf-8') as f:\n",
    "                arxiv_raw = json.load(f)\n",
    "            \n",
    "            cleaned_entries = []\n",
    "            for arxiv_id, meta in arxiv_raw.items():\n",
    "                meta['arxiv_id'] = arxiv_id \n",
    "                cleaned_ref = clean_arxiv_reference(meta)\n",
    "                cleaned_entries.append(cleaned_ref)\n",
    "            \n",
    "            cleaned_arxiv[pub_id] = cleaned_entries\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Error loading references.json for {pub_id}: {e}\")\n",
    "            cleaned_arxiv[pub_id] = []\n",
    "    else:\n",
    "        cleaned_arxiv[pub_id] = []\n",
    "\n",
    "# Summary\n",
    "total_bib = sum(len(x) for x in cleaned_bibtex.values())\n",
    "total_arxiv = sum(len(x) for x in cleaned_arxiv.values())\n",
    "print(f\"\\nDONE: {total_bib} cleaned BibTeX entries vs {total_arxiv} cleaned arXiv entries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c3c237",
   "metadata": {},
   "source": [
    "## **Labeling & Dataset Construction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6847920a",
   "metadata": {},
   "source": [
    "### **Manual Labeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1d4b4a",
   "metadata": {},
   "source": [
    "#### **Build Manual Pairs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97201e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 file JSON in folder 'manual_labeling'\n"
     ]
    }
   ],
   "source": [
    "generated = True\n",
    "\n",
    "if os.path.exists(MANUAL_DIR):\n",
    "    existing_files = glob.glob(os.path.join(MANUAL_DIR, \"*.json\"))\n",
    "    \n",
    "    if len(existing_files) >= 5:\n",
    "        print(f\"Found {len(existing_files)} file JSON in folder '{MANUAL_DIR}'\")\n",
    "    else:\n",
    "        print(f\"Folder exists but only has {len(existing_files)} files. Will regenerate\")\n",
    "        generated = False\n",
    "\n",
    "if not generated:\n",
    "    \n",
    "    candidate_papers = []\n",
    "\n",
    "    for pub_id, bib_entries in tqdm(cleaned_bibtex.items(), desc=\"Filtering Papers\"):\n",
    "        arxiv_entries = cleaned_arxiv.get(pub_id, [])\n",
    "        \n",
    "        if len(bib_entries) < 20 or len(arxiv_entries) < 20:\n",
    "            continue\n",
    "\n",
    "        potential_matches = 0\n",
    "        for bib in bib_entries:\n",
    "            match = find_best_match(bib, arxiv_entries, threshold=0.7) \n",
    "            if match:\n",
    "                potential_matches += 1\n",
    "            \n",
    "        if potential_matches >= 20:\n",
    "            candidate_papers.append({\n",
    "                'pub_id': pub_id,\n",
    "                'bib_count': len(bib_entries),\n",
    "                'arxiv_count': len(arxiv_entries),\n",
    "                'potential_matches': potential_matches\n",
    "            })\n",
    "\n",
    "    candidate_papers.sort(key=lambda x: x['potential_matches'], reverse=True)\n",
    "    selected_papers = candidate_papers[1:6]\n",
    "\n",
    "    if len(selected_papers) < 5:\n",
    "        print(f\"Only found {len(selected_papers)} papers meeting the criteria\")\n",
    "\n",
    "    os.makedirs(MANUAL_DIR, exist_ok=True)\n",
    "\n",
    "    for paper in selected_papers:\n",
    "        pub_id = paper['pub_id']\n",
    "        bib_pool = cleaned_bibtex[pub_id]\n",
    "        arxiv_pool = cleaned_arxiv[pub_id]\n",
    "        \n",
    "        tasks = []\n",
    "        \n",
    "        for bib in bib_pool:\n",
    "            \n",
    "            current_bib_candidates = []\n",
    "            \n",
    "            for arxiv in arxiv_pool:\n",
    "                \n",
    "                score = compute_match_score(bib, arxiv)\n",
    "                \n",
    "                pair_data = {\n",
    "                    \"label\": '', \n",
    "                    \"score\": round(score, 4),\n",
    "                    \n",
    "                    # Info BibTeX (Source)\n",
    "                    \"bib_key\": bib['key'],\n",
    "                    \"bib_title\": bib.get('normalized_title', ''),\n",
    "                    \"bib_author\": \", \".join(bib.get('normalized_authors', [])),\n",
    "                    \"bib_year\": bib.get('normalized_year', ''),\n",
    "                    \n",
    "                    # Info ArXiv (Target)\n",
    "                    \"arxiv_id\": arxiv['arxiv_id'],\n",
    "                    \"arxiv_title\": arxiv.get('normalized_title', ''),\n",
    "                    \"arxiv_author\": \", \".join(arxiv.get('normalized_authors', [])),\n",
    "                    \"arxiv_year\": arxiv.get('normalized_year', '')\n",
    "                }\n",
    "                \n",
    "                current_bib_candidates.append(pair_data)\n",
    "            \n",
    "            current_bib_candidates.sort(key=lambda x: x['score'], reverse=True)\n",
    "            \n",
    "            tasks.extend(current_bib_candidates)\n",
    "\n",
    "        file_path = os.path.join(MANUAL_DIR, f\"{pub_id}.json\")\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(tasks, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "        print(f\"  -> Created: {file_path} (Total pairs: {len(tasks)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81215cd",
   "metadata": {},
   "source": [
    "#### **Read Manual Pairs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89aac0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2211-13755: 33 bib entries → 33 pos, 1584 neg pairs\n",
      "Loaded 2211-13757: 37 bib entries → 37 pos, 2035 neg pairs\n",
      "Loaded 2211-13760: 22 bib entries → 22 pos, 1122 neg pairs\n",
      "Loaded 2211-13766: 21 bib entries → 21 pos, 1008 neg pairs\n",
      "Loaded 2211-13767: 23 bib entries → 23 pos, 1150 neg pairs\n",
      "\n",
      "Total Manual Pairs Loaded: 7035\n",
      "Positive Labels (1): 136\n",
      "Negative Labels (0): 6899\n",
      "Ratio (pos:neg): 1:50.7\n"
     ]
    }
   ],
   "source": [
    "manual_pairs = []\n",
    "json_files = glob.glob(\"manual_labeling/*.json\")\n",
    "\n",
    "if not json_files:\n",
    "    raise FileNotFoundError(\"Not found any JSON files in folder 'manual_labeling'\")\n",
    "\n",
    "for file_path in json_files:\n",
    "    filename = os.path.basename(file_path)\n",
    "    pub_id = filename.replace(\".json\", \"\")\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        labeled_data = json.load(f)\n",
    "    \n",
    "    bib_pool = cleaned_bibtex.get(pub_id, [])\n",
    "    arxiv_pool = cleaned_arxiv.get(pub_id, [])\n",
    "    \n",
    "    bib_dict = {b['key']: b for b in bib_pool}\n",
    "    arxiv_dict = {a['arxiv_id']: a for a in arxiv_pool}\n",
    "    \n",
    "    positive_matches = {}  # {bib_key: arxiv_id}\n",
    "    \n",
    "    for item in labeled_data:\n",
    "        if item['label'] == 1:\n",
    "            bib_key = item['bib_key']\n",
    "            arxiv_id = item['arxiv_id']\n",
    "            \n",
    "            if bib_key in bib_dict and arxiv_id in arxiv_dict:\n",
    "                positive_matches[bib_key] = arxiv_id\n",
    "    \n",
    "    count_pos = 0\n",
    "    count_neg = 0\n",
    "    \n",
    "    for bib_key, matched_arxiv_id in positive_matches.items():\n",
    "        bib_entry = bib_dict[bib_key]\n",
    "        \n",
    "        base = {\n",
    "            'pub_id': pub_id,\n",
    "            'bib_key': bib_key,\n",
    "            'bib_ref_id': bib_entry.get('ref_id'),\n",
    "            'all_keys': bib_entry.get('all_keys', []),\n",
    "            'bib_title_clean': bib_entry.get('normalized_title'),\n",
    "            'bib_authors_clean': \", \".join(bib_entry.get('normalized_authors', [])),\n",
    "            'bib_author_tokens': str(bib_entry.get('author_tokens', [])),\n",
    "            'bib_year': bib_entry.get('normalized_year'),\n",
    "            'source': 'manual',\n",
    "        }\n",
    "        \n",
    "        for arxiv_entry in arxiv_pool:\n",
    "            arxiv_id = arxiv_entry['arxiv_id']\n",
    "            \n",
    "            # Xác định label\n",
    "            if arxiv_id == matched_arxiv_id:\n",
    "                label = 1\n",
    "                pair_type = 'manual_verified'\n",
    "                count_pos += 1\n",
    "            else:\n",
    "                label = 0\n",
    "                pair_type = 'manual_negative'\n",
    "                count_neg += 1\n",
    "            \n",
    "            row = base.copy()\n",
    "            row.update({\n",
    "                'candidate_arxiv_id': arxiv_id,\n",
    "                'candidate_title_clean': arxiv_entry.get('normalized_title'),\n",
    "                'candidate_authors_clean': \", \".join(arxiv_entry.get('normalized_authors', [])),\n",
    "                'candidate_author_tokens': str(arxiv_entry.get('author_tokens', [])),\n",
    "                'candidate_year': arxiv_entry.get('normalized_year'),\n",
    "                'pair_type': pair_type,\n",
    "                'label': label\n",
    "            })\n",
    "            \n",
    "            manual_pairs.append(row)\n",
    "    \n",
    "    print(f\"Loaded {pub_id}: {len(positive_matches)} bib entries → {count_pos} pos, {count_neg} neg pairs\")\n",
    "\n",
    "FIXED_MANUAL_DATA = {pid.replace(\".json\", \"\"): {} for pid in [os.path.basename(f) for f in json_files]}\n",
    "\n",
    "print(f\"\\nTotal Manual Pairs Loaded: {len(manual_pairs)}\")\n",
    "print(f\"Positive Labels (1): {len([p for p in manual_pairs if p['label'] == 1])}\")\n",
    "print(f\"Negative Labels (0): {len([p for p in manual_pairs if p['label'] == 0])}\")\n",
    "print(f\"Ratio (pos:neg): 1:{len([p for p in manual_pairs if p['label'] == 0]) / max(1, len([p for p in manual_pairs if p['label'] == 1])):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fa24ce",
   "metadata": {},
   "source": [
    "### **Automatic Labeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b97ab7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total auto publications: 25\n",
      "Target 10%: 3 publications\n"
     ]
    }
   ],
   "source": [
    "# Configuration for automatic labeling\n",
    "POSITIVE_THRESHOLD = 0.8\n",
    "TARGET_PERCENTAGE = 0.10  # 10% of publications\n",
    "\n",
    "# Get list publications without manual labeling data\n",
    "auto_pub_ids = [p for p in scan_result.keys() if p not in FIXED_MANUAL_DATA]\n",
    "\n",
    "# Calculate target number of publications\n",
    "target_num_pubs = math.ceil(len(auto_pub_ids) * TARGET_PERCENTAGE)\n",
    "\n",
    "print(f\"Total auto publications: {len(auto_pub_ids)}\")\n",
    "print(f\"Target 10%: {target_num_pubs} publications\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e3f8a3",
   "metadata": {},
   "source": [
    "#### **Scan all Publications**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9f35568",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning all publications: 100%|██████████| 25/25 [00:06<00:00,  3.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total valid publications (with positive matches): 15\n",
      "Total valid entries: 399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Scan ALL publications first, then select valid ones\n",
    "\n",
    "all_valid_entries = {}  # {(pub_id, bib_key): {'bib': ..., 'arxiv_scores': [...]}}\n",
    "valid_pub_ids = set()   # Track publications with at least 1 valid entry\n",
    "\n",
    "for pub_id in tqdm(auto_pub_ids, desc=\"Scanning all publications\"):\n",
    "    arxiv_pool = cleaned_arxiv.get(pub_id, [])\n",
    "    bib_pool = cleaned_bibtex.get(pub_id, [])\n",
    "    \n",
    "    # Skip if missing data\n",
    "    if not arxiv_pool or not bib_pool:\n",
    "        continue\n",
    "    \n",
    "    for bib in bib_pool:\n",
    "        bib_key = bib.get('key')\n",
    "        arxiv_scores = []\n",
    "        \n",
    "        # Calculate score with ALL arxiv in pool\n",
    "        for arxiv in arxiv_pool:\n",
    "            match = find_best_match(bib, [arxiv], threshold=0.0)\n",
    "            \n",
    "            if match:\n",
    "                _, score, _ = match\n",
    "            else:\n",
    "                score = 0.0\n",
    "            \n",
    "            arxiv_scores.append((arxiv, score))\n",
    "        \n",
    "        # Check if this entry has at least 1 positive\n",
    "        has_positive = any(score > POSITIVE_THRESHOLD for _, score in arxiv_scores)\n",
    "        \n",
    "        if has_positive:\n",
    "            all_valid_entries[(pub_id, bib_key)] = {\n",
    "                'bib': bib,\n",
    "                'arxiv_pool': arxiv_pool,\n",
    "                'arxiv_scores': arxiv_scores\n",
    "            }\n",
    "            valid_pub_ids.add(pub_id)\n",
    "\n",
    "print(f\"\\nTotal valid publications (with positive matches): {len(valid_pub_ids)}\")\n",
    "print(f\"Total valid entries: {len(all_valid_entries)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20a7c3f",
   "metadata": {},
   "source": [
    "#### **Select valid Publications**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9005456b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target publications: 3\n",
      "Available valid publications: 15\n",
      "Selected publications: 3\n",
      "Filtered entries: 85\n",
      "\n",
      "Selected pub_ids: ['2211-13761', '2211-13762', '2211-13756']\n"
     ]
    }
   ],
   "source": [
    "# Select exactly target_num_pubs from valid publications\n",
    "valid_pub_list = list(valid_pub_ids)\n",
    "\n",
    "# Use random sampling for unbiased selection\n",
    "random.shuffle(valid_pub_list)\n",
    "selected_pub_ids = valid_pub_list[:target_num_pubs]\n",
    "\n",
    "# Filter entries to only include selected publications\n",
    "filtered_entries = {\n",
    "    key: data \n",
    "    for key, data in all_valid_entries.items() \n",
    "    if key[0] in selected_pub_ids\n",
    "}\n",
    "\n",
    "print(f\"Target publications: {target_num_pubs}\")\n",
    "print(f\"Available valid publications: {len(valid_pub_ids)}\")\n",
    "print(f\"Selected publications: {len(selected_pub_ids)}\")\n",
    "print(f\"Filtered entries: {len(filtered_entries)}\")\n",
    "print(f\"\\nSelected pub_ids: {selected_pub_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d3e903",
   "metadata": {},
   "source": [
    "#### **Build automatic pairs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73ff08a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Auto Dataset: 100%|██████████| 85/85 [00:00<00:00, 5072.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated 3052 automatic pairs.\n",
      "- Publications labeled: 3\n",
      "- BibTeX entries labeled: 85\n",
      "- Total pairs: 3052\n",
      "- Positive pairs: 85\n",
      "- Negative pairs: 2967\n",
      "- Ratio (pos:neg): 1:34.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "automatic_pairs = []\n",
    "\n",
    "for (pub_id, bib_key), data in tqdm(filtered_entries.items(), desc=\"Building Auto Dataset\"):\n",
    "    bib = data['bib']\n",
    "    arxiv_pool = data['arxiv_pool']\n",
    "\n",
    "    # Find the index of the arxiv with the highest score\n",
    "    scores = [score for _, score in data['arxiv_scores']]\n",
    "    if not scores:\n",
    "        continue\n",
    "    max_idx = scores.index(max(scores))\n",
    "\n",
    "    base = {\n",
    "        'pub_id': pub_id,\n",
    "        'bib_key': bib.get('key'),\n",
    "        'all_keys': bib.get('all_keys', []),\n",
    "        'bib_ref_id': bib.get('ref_id'),\n",
    "        'bib_title_clean': bib.get('normalized_title'),\n",
    "        'bib_authors_clean': \", \".join(bib.get('normalized_authors', [])),\n",
    "        'bib_author_tokens': str(bib.get('author_tokens', [])),\n",
    "        'bib_year': bib.get('normalized_year'),\n",
    "        'source': 'automatic'\n",
    "    }\n",
    "\n",
    "    for idx, (arxiv, score) in enumerate(data['arxiv_scores']):\n",
    "        pair = base.copy()\n",
    "        pair.update({\n",
    "            'candidate_arxiv_id': arxiv['arxiv_id'],\n",
    "            'candidate_title_clean': arxiv.get('normalized_title'),\n",
    "            'candidate_authors_clean': \", \".join(arxiv.get('normalized_authors', [])),\n",
    "            'candidate_author_tokens': str(arxiv.get('author_tokens', [])),\n",
    "            'candidate_year': arxiv.get('normalized_year'),\n",
    "            'pair_type': 'positive_auto' if idx == max_idx else 'negative_auto',\n",
    "            'label': 1 if idx == max_idx else 0\n",
    "        })\n",
    "        automatic_pairs.append(pair)\n",
    "\n",
    "print(f\"\\nGenerated {len(automatic_pairs)} automatic pairs.\")\n",
    "\n",
    "df_auto = pd.DataFrame(automatic_pairs)\n",
    "num_positives = len(df_auto[df_auto['label'] == 1])\n",
    "num_negatives = len(df_auto[df_auto['label'] == 0])\n",
    "num_entries = df_auto['bib_key'].nunique()\n",
    "\n",
    "print(f\"- Publications labeled: {len(selected_pub_ids)}\")\n",
    "print(f\"- BibTeX entries labeled: {num_entries}\")\n",
    "print(f\"- Total pairs: {len(automatic_pairs)}\")\n",
    "print(f\"- Positive pairs: {num_positives}\")\n",
    "print(f\"- Negative pairs: {num_negatives}\")\n",
    "print(f\"- Ratio (pos:neg): 1:{num_negatives/max(1, num_positives):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1591d4e",
   "metadata": {},
   "source": [
    "## **Export CSV**\n",
    "\n",
    "Export labeled dataset for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33b20b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positives: 221\n",
      "Negatives: 9866\n",
      "Total pairs: 10087\n",
      "Saved labeled dataset to: ../src/labeled_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "final_df = pd.DataFrame(manual_pairs + automatic_pairs)\n",
    "\n",
    "cols = [\n",
    "    'pub_id', 'bib_key', 'all_keys', 'bib_ref_id', \n",
    "    'bib_title_clean', 'bib_authors_clean', 'bib_author_tokens', 'bib_year',\n",
    "    'candidate_arxiv_id', \n",
    "    'candidate_title_clean', 'candidate_authors_clean', 'candidate_author_tokens', 'candidate_year',\n",
    "    'source', 'pair_type', 'label'\n",
    "]\n",
    "\n",
    "for c in cols: \n",
    "    if c not in final_df.columns: final_df[c] = None\n",
    "\n",
    "labeled_df = final_df[cols]\n",
    "\n",
    "print(f\"Positives: {len(labeled_df[labeled_df['label']==1])}\")\n",
    "print(f\"Negatives: {len(labeled_df[labeled_df['label']==0])}\")\n",
    "print(f\"Total pairs: {len(labeled_df)}\")\n",
    "\n",
    "csv_output_path = \"../src/labeled_dataset.csv\"\n",
    "labeled_df.to_csv(csv_output_path, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Saved labeled dataset to: {csv_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c4f1bb",
   "metadata": {},
   "source": [
    "## **Feature Engineering**\n",
    "\n",
    "Create features from labeled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42c76a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labeled dataset if not already in memory\n",
    "if final_df is None:\n",
    "    print(\"final_df is None, loading from CSV...\")\n",
    "    labeled_path = \"../src/labeled_dataset.csv\"\n",
    "    if not labeled_path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {labeled_path}. Run Notebook 02 first!\")\n",
    "\n",
    "    final_df = pd.read_csv(labeled_path)\n",
    "\n",
    "# Apply feature engineering\n",
    "df_features = feature_engineering(final_df)\n",
    "\n",
    "# Save featured dataset\n",
    "featured_path = \"../src/featured_dataset.csv\"\n",
    "df_features.to_csv(featured_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "564eb0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Feature Engineering Complete\n",
      "============================================================\n",
      "  Total samples: 10087\n",
      "  Features created: 5\n",
      "  NaN values: 0\n",
      "  Inf values: 0\n",
      "  Saved to: ../src/featured_dataset.csv\n",
      "============================================================\n",
      "\n",
      " Feature columns:\n",
      "   - label\n",
      "   - pub_id\n",
      "   - bib_key\n",
      "   - candidate_arxiv_id\n",
      "   - source\n",
      "   - Title_Soft_Jaccard\n",
      "   - Title_Length_Diff\n",
      "   - Author_Overlap_Score\n",
      "   - Author_Levenshtein_Ratio\n",
      "   - Year_Diff\n",
      "\n",
      " Sample of featured dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pub_id</th>\n",
       "      <th>bib_key</th>\n",
       "      <th>candidate_arxiv_id</th>\n",
       "      <th>source</th>\n",
       "      <th>Title_Soft_Jaccard</th>\n",
       "      <th>Title_Length_Diff</th>\n",
       "      <th>Author_Overlap_Score</th>\n",
       "      <th>Author_Levenshtein_Ratio</th>\n",
       "      <th>Year_Diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2211-13755</td>\n",
       "      <td>zhong2018open</td>\n",
       "      <td>2203-11483</td>\n",
       "      <td>manual</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.507594</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2211-13755</td>\n",
       "      <td>zhong2018open</td>\n",
       "      <td>2203-02146</td>\n",
       "      <td>manual</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397661</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2211-13755</td>\n",
       "      <td>zhong2018open</td>\n",
       "      <td>2108-10869</td>\n",
       "      <td>manual</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.233410</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2211-13755</td>\n",
       "      <td>zhong2018open</td>\n",
       "      <td>2108-05773</td>\n",
       "      <td>manual</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>39</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.447958</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2211-13755</td>\n",
       "      <td>zhong2018open</td>\n",
       "      <td>2104-04314</td>\n",
       "      <td>manual</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>13</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.323810</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label      pub_id        bib_key candidate_arxiv_id  source  \\\n",
       "0      0  2211-13755  zhong2018open         2203-11483  manual   \n",
       "1      0  2211-13755  zhong2018open         2203-02146  manual   \n",
       "2      0  2211-13755  zhong2018open         2108-10869  manual   \n",
       "3      0  2211-13755  zhong2018open         2108-05773  manual   \n",
       "4      0  2211-13755  zhong2018open         2104-04314  manual   \n",
       "\n",
       "   Title_Soft_Jaccard  Title_Length_Diff  Author_Overlap_Score  \\\n",
       "0            0.153846                 36                   0.0   \n",
       "1            0.181818                 24                   0.0   \n",
       "2            0.166667                 16                   0.0   \n",
       "3            0.153846                 39                   0.0   \n",
       "4            0.166667                 13                   0.2   \n",
       "\n",
       "   Author_Levenshtein_Ratio  Year_Diff  \n",
       "0                  0.507594        4.0  \n",
       "1                  0.397661        4.0  \n",
       "2                  0.233410        3.0  \n",
       "3                  0.447958        3.0  \n",
       "4                  0.323810        3.0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Feature Engineering Complete\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  Total samples: {len(df_features)}\")\n",
    "print(f\"  Features created: {len([c for c in df_features.columns if c not in ['label', 'pub_id', 'bib_key', 'candidate_arxiv_id', 'source']])}\")\n",
    "# Check for NaN/Inf\n",
    "print(f\"  NaN values: {df_features.isna().sum().sum()}\")\n",
    "print(f\"  Inf values: {np.isinf(df_features.select_dtypes(include=[np.number])).sum().sum()}\")\n",
    "print(f\"  Saved to: {featured_path}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Show feature columns\n",
    "print(\"\\n Feature columns:\")\n",
    "for col in df_features.columns:\n",
    "    print(f\"   - {col}\")\n",
    "print(\"\\n Sample of featured dataset:\")\n",
    "df_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbb8661",
   "metadata": {},
   "source": [
    "## **Train/Valid/Test Split**\n",
    "\n",
    "Split dataset into Train, Validation, and Test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c144bec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual labeled publications: 5\n",
      "Automatic labeled publications: 3\n",
      "Split summary:\n",
      "  Train: 4980 rows, 4 publications\n",
      "  Valid: 3058 rows, 2 publications\n",
      "  Test:  2049 rows, 2 publications\n"
     ]
    }
   ],
   "source": [
    "# Get paper IDs for manual and automatic sources\n",
    "manual_pub_ids = df_features[df_features['source'] == 'manual']['pub_id'].unique().tolist()\n",
    "auto_pub_ids = df_features[df_features['source'] == 'automatic']['pub_id'].unique().tolist()\n",
    "\n",
    "print(f\"Manual labeled publications: {len(manual_pub_ids)}\")\n",
    "print(f\"Automatic labeled publications: {len(auto_pub_ids)}\")\n",
    "\n",
    "# Split data\n",
    "df_train, df_val, df_test = split_data(df_features, manual_pub_ids=manual_pub_ids, auto_pub_ids=auto_pub_ids)\n",
    "\n",
    "train_path = \"../src/train_dataset.csv\"\n",
    "val_path = \"../src/val_dataset.csv\"\n",
    "test_path = \"../src/test_dataset.csv\"\n",
    "df_train.to_csv(train_path, index=False)\n",
    "df_val.to_csv(val_path, index=False)\n",
    "df_test.to_csv(test_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58933d3",
   "metadata": {},
   "source": [
    "## **Model Training**\n",
    "\n",
    "Training model with Negative Sampling to handle data imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88c53852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting Model Training\n",
      "============================================================\n",
      "Using 5 features: ['Title_Soft_Jaccard', 'Title_Length_Diff', 'Author_Overlap_Score', 'Author_Levenshtein_Ratio', 'Year_Diff']\n",
      "\n",
      "Data imbalance BEFORE sampling:\n",
      "  Positives: 104\n",
      "  Negatives: 4876\n",
      "  Ratio (pos:neg): 1:46.9\n",
      "\n",
      "Applying negative sampling to training data...\n",
      "\n",
      "Negative Sampling Summary:\n",
      "  Original: 4980 rows\n",
      "  Sampled:  1664 rows\n",
      "  Reduction: 66.6%\n",
      "  Positives: 104\n",
      "  Negatives: 1560\n",
      "  Ratio (pos:neg): 1:15.0\n",
      "Best iteration: 0\n",
      "Best score: 0.9763091364923363\n",
      "Model saved to: ../src\\xgb_ranker.joblib\n",
      "\n",
      "============================================================\n",
      "Training Complete\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Starting Model Training\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "USE_NEGATIVE_SAMPLING = True\n",
    "\n",
    "K_NEGATIVES = 15\n",
    "\n",
    "HARD_RATIO = 0.6\n",
    "\n",
    "# Define feature columns\n",
    "FEATURE_COLS = [\n",
    "    # Title features\n",
    "    \"Title_Soft_Jaccard\",\n",
    "    \"Title_Length_Diff\",\n",
    "    # Author features\n",
    "    \"Author_Overlap_Score\",\n",
    "    \"Author_Levenshtein_Ratio\",\n",
    "    # Year features\n",
    "    \"Year_Diff\"\n",
    "]\n",
    "\n",
    "# Get available features in the DataFrame\n",
    "available_features = [f for f in FEATURE_COLS if f in df_train.columns]\n",
    "print(f\"Using {len(available_features)} features: {available_features}\")\n",
    "\n",
    "TARGET = \"label\"\n",
    "\n",
    "# Check data imbalance before sampling\n",
    "print(f\"\\nData imbalance BEFORE sampling:\")\n",
    "print(f\"  Positives: {len(df_train[df_train[TARGET]==1])}\")\n",
    "print(f\"  Negatives: {len(df_train[df_train[TARGET]==0])}\")\n",
    "print(f\"  Ratio (pos:neg): 1:{len(df_train[df_train[TARGET]==0])/max(1, len(df_train[df_train[TARGET]==1])):.1f}\")\n",
    "\n",
    "# Train with configured negative sampling\n",
    "model = train_ranker(\n",
    "    df_train, \n",
    "    df_val, \n",
    "    features=available_features, \n",
    "    target=TARGET,\n",
    "    use_negative_sampling=USE_NEGATIVE_SAMPLING,\n",
    "    k_negatives=K_NEGATIVES,\n",
    "    hard_ratio=HARD_RATIO\n",
    ")\n",
    "\n",
    "# Save model\n",
    "model_path = save_model(model, \"../src\", filename=\"xgb_ranker.joblib\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Training Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0171fadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Importance:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Title_Soft_Jaccard</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Title_Length_Diff</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Author_Overlap_Score</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Author_Levenshtein_Ratio</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Year_Diff</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    feature  importance\n",
       "0        Title_Soft_Jaccard         1.0\n",
       "1         Title_Length_Diff         0.0\n",
       "2      Author_Overlap_Score         0.0\n",
       "3  Author_Levenshtein_Ratio         0.0\n",
       "4                 Year_Diff         0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Feature Importance\n",
    "print(\"\\nFeature Importance:\")\n",
    "importance_df = get_feature_importance(model, available_features)\n",
    "display(importance_df)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['feature'], importance_df['importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1989cb42",
   "metadata": {},
   "source": [
    "## **Prediction**\n",
    "\n",
    "Predict rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d8e045e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting on test set...\n",
      "Predicting on validation set...\n",
      "Predicting on train set...\n",
      "\n",
      "Groundtruth extracted:\n",
      "  Test: 51 references across 2 publications\n",
      "  Valid: 66 references across 2 publications\n",
      "  Train: 104 references across 4 publications\n"
     ]
    }
   ],
   "source": [
    "# Predict on test set (returns nested dict: {pub_id: {bib_key: [candidates]}})\n",
    "print(\"Predicting on test set...\")\n",
    "test_predictions = predict_rankings(model, df_test, available_features, top_k=5)\n",
    "\n",
    "# Predict on validation set\n",
    "print(\"Predicting on validation set...\")\n",
    "val_predictions = predict_rankings(model, df_val, available_features, top_k=5)\n",
    "\n",
    "# Predict on train set\n",
    "print(\"Predicting on train set...\")\n",
    "train_predictions = predict_rankings(model, df_train, available_features, top_k=5)\n",
    "\n",
    "test_groundtruth = extract_groundtruth(df_test)\n",
    "val_groundtruth = extract_groundtruth(df_val)\n",
    "train_groundtruth = extract_groundtruth(df_train)\n",
    "\n",
    "print(f\"\\nGroundtruth extracted:\")\n",
    "print(f\"  Test: {sum(len(v) for v in test_groundtruth.values())} references across {len(test_groundtruth)} publications\")\n",
    "print(f\"  Valid: {sum(len(v) for v in val_groundtruth.values())} references across {len(val_groundtruth)} publications\")\n",
    "print(f\"  Train: {sum(len(v) for v in train_groundtruth.values())} references across {len(train_groundtruth)} publications\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9d07e5",
   "metadata": {},
   "source": [
    "## **Evaluation**\n",
    "\n",
    "Evaluate using MRR metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b662ba28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Evaluating on TEST set\n",
      "======================================================================\n",
      "  Test MRR: 0.9706\n"
     ]
    }
   ],
   "source": [
    "# Flatten for evaluation\n",
    "test_predictions_flat = flatten_nested_dict(test_predictions)\n",
    "test_groundtruth_flat = flatten_groundtruth(test_groundtruth)\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"=\"*70)\n",
    "print(\"Evaluating on TEST set\")\n",
    "print(\"=\"*70)\n",
    "test_mrr = calculate_mrr(test_predictions_flat, test_groundtruth_flat)\n",
    "print(f\"  Test MRR: {test_mrr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619a70c5",
   "metadata": {},
   "source": [
    "## **Export pred.json files**\n",
    "Export pred.json files for all publications in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a875c639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partition(pub_id, test_pub_ids, val_pub_ids, train_pub_ids):\n",
    "    \"\"\"Determine which partition a publication belongs to\"\"\"\n",
    "    if pub_id in test_pub_ids:\n",
    "        return \"test\"\n",
    "    elif pub_id in val_pub_ids:\n",
    "        return \"valid\"\n",
    "    elif pub_id in train_pub_ids:\n",
    "        return \"train\"\n",
    "    else:\n",
    "        return \"unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e02211cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting on all data (Train, Valid and Test set)...\n",
      "\n",
      "============================================================\n",
      "Export Complete\n",
      "============================================================\n",
      "  pred.json files exported:\n",
      "    - Train: 4\n",
      "    - Valid: 2\n",
      "    - Test:  2\n",
      "  Output folder: 23127453/\n"
     ]
    }
   ],
   "source": [
    "# Load all data (Train, Valid and Test set) to predict\n",
    "df_all = pd.read_csv(\"../src/featured_dataset.csv\")\n",
    "\n",
    "# Get pub_ids for each partition\n",
    "train_pub_ids = set(df_train['pub_id'].unique())\n",
    "val_pub_ids = set(df_val['pub_id'].unique())\n",
    "test_pub_ids = set(df_test['pub_id'].unique())\n",
    "\n",
    "# Predict rankings for ALL data (returns nested dict: {pub_id: {bib_key: [candidates]}})\n",
    "print(\"Predicting on all data (Train, Valid and Test set)...\")\n",
    "all_predictions = predict_rankings(model, df_all, available_features, top_k=5)\n",
    "\n",
    "# Extract groundtruth (nested dict: {pub_id: {bib_key: arxiv_id}})\n",
    "all_groundtruth = extract_groundtruth(df_all)\n",
    "\n",
    "# Export pred.json for each publication\n",
    "exported = {\"train\": 0, \"valid\": 0, \"test\": 0, \"unknown\": 0}\n",
    "for pub_id, predictions in all_predictions.items():\n",
    "    pub_folder = Path(OUTPUT_DIR + \"/\" + pub_id)\n",
    "    pub_folder.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Determine correct partition for this publication\n",
    "    partition = get_partition(pub_id, test_pub_ids, val_pub_ids, train_pub_ids)\n",
    "    \n",
    "    # Build pred.json\n",
    "    pred_json = {\n",
    "        \"partition\": partition,\n",
    "        \"groundtruth\": all_groundtruth.get(pub_id, {}),\n",
    "        \"prediction\": predictions\n",
    "    }\n",
    "    \n",
    "    # Export to file\n",
    "    output_file = pub_folder / \"pred.json\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(pred_json, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    exported[partition] += 1\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Export Complete\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  pred.json files exported:\")\n",
    "print(f\"    - Train: {exported['train']}\")\n",
    "print(f\"    - Valid: {exported['valid']}\")\n",
    "print(f\"    - Test:  {exported['test']}\")\n",
    "print(f\"  Output folder: {OUTPUT_DIR}/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
