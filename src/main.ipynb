{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccc8a3f4",
   "metadata": {},
   "source": [
    "# <center>**Milestone 2**<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b11d9f",
   "metadata": {},
   "source": [
    "## **Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7be47b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Feng\n",
      "[nltk_data]     Wang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Feng\n",
      "[nltk_data]     Wang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Feng\n",
      "[nltk_data]     Wang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Feng\n",
      "[nltk_data]     Wang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Feng\n",
      "[nltk_data]     Wang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import gc\n",
    "import pickle\n",
    "import sys\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from __future__ import annotations\n",
    "from collections import defaultdict\n",
    "\n",
    "from my_scanner import *\n",
    "from my_parser import *\n",
    "from my_matcher import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beb22ab",
   "metadata": {},
   "source": [
    "## **Configuration**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69aca721",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_ROOT = \"../30-paper\"\n",
    "USING_SHA256_HASH = False\n",
    "\n",
    "# 1. Output directory\n",
    "OUTPUT_DIR = \"23127453\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Configuration for caching\n",
    "CACHE_DIR = \".cache\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "sys.setrecursionlimit(20000)\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde1af6d",
   "metadata": {},
   "source": [
    "## **Dataset rescan**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb2d3ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2211-13747 NO_TEX []\n",
      "2211-13748 READY ['2211-13748v1']\n",
      "2211-13749 READY ['2211-13749v1']\n",
      "2211-13750 READY ['2211-13750v1', '2211-13750v2']\n",
      "2211-13751 READY ['2211-13751v1']\n",
      "2211-13752 READY ['2211-13752v1']\n",
      "2211-13753 READY ['2211-13753v1']\n",
      "2211-13754 READY ['2211-13754v1']\n",
      "2211-13755 READY ['2211-13755v1', '2211-13755v2']\n",
      "2211-13756 READY ['2211-13756v1', '2211-13756v2']\n",
      "2211-13757 READY ['2211-13757v1', '2211-13757v2']\n",
      "2211-13758 READY ['2211-13758v1']\n",
      "2211-13759 READY ['2211-13759v1', '2211-13759v2']\n",
      "2211-13760 READY ['2211-13760v1', '2211-13760v2']\n",
      "2211-13761 READY ['2211-13761v1']\n",
      "2211-13762 READY ['2211-13762v1', '2211-13762v2']\n",
      "2211-13763 READY ['2211-13763v1']\n",
      "2211-13764 READY ['2211-13764v1']\n",
      "2211-13765 READY ['2211-13765v1']\n",
      "2211-13766 READY ['2211-13766v1', '2211-13766v2', '2211-13766v3']\n",
      "2211-13767 READY ['2211-13767v1']\n",
      "2211-13768 READY ['2211-13768v1', '2211-13768v2']\n",
      "2211-13769 READY ['2211-13769v1', '2211-13769v2']\n",
      "2211-13770 READY ['2211-13770v1']\n",
      "2211-13771 READY ['2211-13771v1']\n",
      "2211-13772 READY ['2211-13772v1']\n",
      "2211-13773 READY ['2211-13773v1', '2211-13773v2']\n",
      "2211-13774 READY ['2211-13774v1']\n",
      "2211-13775 READY ['2211-13775v1', '2211-13775v2']\n",
      "2211-13776 READY ['2211-13776v1']\n"
     ]
    }
   ],
   "source": [
    "scan_result = scan_dataset(RAW_ROOT)\n",
    "\n",
    "for k, v in scan_result.items():\n",
    "    print(k, v[\"status\"], v[\"versions\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857cf672",
   "metadata": {},
   "source": [
    "## **Version-level Multi-file Resolver**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c3907b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Publications: 100%|██████████| 29/29 [00:00<00:00, 625.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2211-13748 2211-13748v1 ['weibo.tex']\n",
      "2211-13749 2211-13749v1 ['On_projections_of_tailsvA.tex']\n",
      "2211-13750 2211-13750v1 ['comparingsinglettestingschemes.tex']\n",
      "2211-13750 2211-13750v2 ['comparingsinglettestingschemes4.tex']\n",
      "2211-13751 2211-13751v1 ['Kane_et_al_JFM_v1.tex']\n",
      "2211-13752 2211-13752v1 ['main.tex', 'macros.tex', '00_abstract.tex', '01_intro.tex', '02_related.tex', '03_method.tex', '04_results.tex', '05_conclusion.tex', '07_acc.tex', '06_appendix.tex']\n",
      "2211-13753 2211-13753v1 ['0-title.tex']\n",
      "2211-13754 2211-13754v1 ['main.tex']\n",
      "2211-13755 2211-13755v1 ['main.tex']\n",
      "2211-13755 2211-13755v2 ['main.tex']\n",
      "2211-13756 2211-13756v1 ['main.tex']\n",
      "2211-13756 2211-13756v2 ['main.tex']\n",
      "2211-13757 2211-13757v1 ['main.tex']\n",
      "2211-13757 2211-13757v2 ['main.tex']\n",
      "2211-13758 2211-13758v1 ['main.tex']\n",
      "2211-13759 2211-13759v1 ['main.tex']\n",
      "2211-13759 2211-13759v2 ['main.tex']\n",
      "2211-13760 2211-13760v1 ['main.tex']\n",
      "2211-13760 2211-13760v2 ['main.tex']\n",
      "2211-13761 2211-13761v1 ['main.tex']\n",
      "2211-13762 2211-13762v1 ['main.tex', 'overall.tex', 'splits.tex']\n",
      "2211-13762 2211-13762v2 ['main.tex', 'overall.tex', 'splits.tex']\n",
      "2211-13763 2211-13763v1 ['main.tex', 'table.tex']\n",
      "2211-13764 2211-13764v1 ['accepted.tex']\n",
      "2211-13765 2211-13765v1 ['main.tex']\n",
      "2211-13766 2211-13766v1 ['PRL_Version.tex']\n",
      "2211-13766 2211-13766v2 ['arXiv_Version.tex']\n",
      "2211-13766 2211-13766v3 ['arXiv_Version.tex']\n",
      "2211-13767 2211-13767v1 ['main_arxiv.tex']\n",
      "2211-13768 2211-13768v1 ['main.tex', 'hyperlink-year-only-natbib-patch.tex']\n",
      "2211-13768 2211-13768v2 ['main.tex', 'hyperlink-year-only-natbib-patch.tex']\n",
      "2211-13769 2211-13769v1 ['main.tex', 'abstract.tex', 'intro.tex', 'conclude.tex', 'siamfc.tex']\n",
      "2211-13769 2211-13769v2 ['Appendix.tex', 'ostrack.tex', 'dimp_new.tex', 'stark.tex']\n",
      "2211-13770 2211-13770v1 ['skeleton.tex']\n",
      "2211-13771 2211-13771v1 ['paper.tex', 'penrose.tex']\n",
      "2211-13772 2211-13772v1 ['main.tex']\n",
      "2211-13773 2211-13773v1 ['draft_single.tex']\n",
      "2211-13773 2211-13773v2 ['draft_single.tex']\n",
      "2211-13774 2211-13774v1 ['Appendices.tex']\n",
      "2211-13775 2211-13775v1 ['main.tex', 'macros.tex', '00_abstract.tex', '01_introduction.tex', 'teaser_pdf.tex', '02_related_work.tex', '03_method.tex', 'diagram_pdf.tex', '04_results.tex', 'attack_pdf.tex', 'attacks_comp_pdf.tex', 'pc_comparison.tex', 'classifier_evaluation.tex', 'detector_evaluation.tex', 'coma_beta_pdf.tex', 'transferability_pdf.tex', '05_conclusions.tex', 'supplementary.tex', 'supp_analysis.tex', 'coma_tsne_pdf.tex', 'smal_beta_pdf.tex', 'oods_pdf.tex', 'oods.tex', 'transferability_geometric.tex', 'transferability_semantic.tex', 'semantic_comp_pdf.tex', 'coma_freq_comp_pdf.tex', 'supp_ablation.tex', 'freq_ablation_pdf.tex', 'reg_comp_pdf.tex', 'reg_comp.tex', 'euclidean_comp.tex', 'self_basis_comp.tex', 'random_targets.tex', 'supp_settings.tex', 'supp_results.tex', 'stability.tex', 'coma_stability_pdf.tex', 'smal_stability_pdf.tex', 'coma_failure_pdf.tex', 'smal_failure_pdf.tex', 'coma_evolution_pdf.tex', 'smal_evolution_pdf.tex', 'quiz_pdf.tex', 'coma_concat_1_pdf.tex', 'coma_concat_2_pdf.tex', 'smal_concat_1_pdf.tex', 'smal_concat_2_pdf.tex']\n",
      "2211-13775 2211-13775v2 ['egpaper_final.tex', 'macros.tex', '00_abstract.tex', '01_introduction.tex', 'teaser_pdf.tex', '02_related_work.tex', '03_method.tex', 'diagram_pdf.tex', 'quiz_main_pdf.tex', '04_results.tex', 'attack_pdf.tex', 'attacks_comp_pdf.tex', 'classifier_evaluation.tex', 'detector_evaluation.tex', 'semantic_comp_pdf.tex', 'transferability_pdf.tex', 'lpf_defense_pdf.tex', 'beta_pdf.tex', '05_conclusions.tex', 'supplementary.tex', '01_supp_analysis.tex', 'pc_comparison.tex', 'coma_tsne_pdf.tex', 'oods_pdf.tex', 'oods.tex', 'transferability_geometric.tex', 'transferability_semantic.tex', '02_supp_ablation.tex', 'coma_freq_comp_pdf.tex', 'freq_ablation_pdf.tex', 'reg_comp_pdf.tex', 'reg_comp.tex', 'euclidean_comp.tex', 'self_basis_comp.tex', 'random_targets.tex', '03_supp_settings.tex', '04_supp_results.tex', 'stability.tex', 'quiz_answers_pdf.tex', 'coma_stability_pdf.tex', 'smal_stability_pdf.tex', 'coma_failure_pdf.tex', 'smal_failure_pdf.tex', 'coma_evolution_pdf.tex', 'smal_evolution_pdf.tex', 'coma_concat_1_pdf.tex', 'coma_concat_2_pdf.tex', 'smal_concat_1_pdf.tex', 'smal_concat_2_pdf.tex']\n",
      "2211-13776 2211-13776v1 ['acl2020.tex']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pub_results = []\n",
    "\n",
    "ready_items = [\n",
    "    (pub_id, info)\n",
    "    for pub_id, info in scan_result.items()\n",
    "    if info[\"status\"] == \"READY\"\n",
    "]\n",
    "\n",
    "for pub_id, info in tqdm(ready_items, desc=\"Publications\"):\n",
    "    for version in info[\"versions\"]:\n",
    "        version_path = f\"{RAW_ROOT}/{pub_id}/tex/{version}\"\n",
    "\n",
    "        result = resolve_version(\n",
    "            publication_id=pub_id,\n",
    "            version_name=version,\n",
    "            version_path=version_path\n",
    "        )\n",
    "\n",
    "        pub_results.append(result)\n",
    "\n",
    "# Xem kết quả\n",
    "for r in pub_results:\n",
    "    print(r[\"publication_id\"], r[\"version\"], r[\"used_tex_files\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0e8b68",
   "metadata": {},
   "source": [
    "## **LaTeX Hierarchy Parser**\n",
    "\n",
    "We parsed each LaTeX version into a hierarchical tree structure. Sectioning commands were treated as internal nodes, while only sentences, tables, and figures were considered leaf nodes, strictly following the seminar constraints. No deduplication or identifier assignment was performed at this stage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ce533b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tree_to_cache(pub_id: str, version: str, root: FileNode) -> str:\n",
    "    \"\"\"\n",
    "    Serialize tree to disk and return cache path.\n",
    "    Returns the cache file path for later retrieval.\n",
    "    \"\"\"\n",
    "    cache_path = os.path.join(CACHE_DIR, f\"{pub_id}_{version}.pkl\")\n",
    "    with open(cache_path, \"wb\") as f:\n",
    "        pickle.dump(root, f, protocol=4)\n",
    "    return cache_path\n",
    "\n",
    "def load_tree_from_cache(cache_path: str) -> FileNode:\n",
    "    \"\"\"Load tree from cache file.\"\"\"\n",
    "    with open(cache_path, \"rb\") as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "257bdcc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing LaTeX versions: 100%|██████████| 42/42 [00:02<00:00, 15.29version(s)/s]\n"
     ]
    }
   ],
   "source": [
    "parsed_versions = []\n",
    "\n",
    "\n",
    "for version_info in tqdm(\n",
    "    pub_results,\n",
    "    desc=\"Parsing LaTeX versions\",\n",
    "    unit=\"version(s)\"\n",
    "):\n",
    "    if version_info[\"status\"] != \"RESOLVED\":\n",
    "        continue\n",
    "\n",
    "    version_path = (\n",
    "        f\"{RAW_ROOT}/\"\n",
    "        f\"{version_info['publication_id']}/tex/\"\n",
    "        f\"{version_info['version']}\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        root_node = parse_tex_files(\n",
    "            version_path=version_path,\n",
    "            tex_files=version_info[\"used_tex_files\"]\n",
    "        )\n",
    "\n",
    "        # Save to disk instead of keeping in memory\n",
    "        cache_path = save_tree_to_cache(\n",
    "            pub_id=version_info[\"publication_id\"],\n",
    "            version=version_info[\"version\"],\n",
    "            root=root_node\n",
    "        )\n",
    "\n",
    "        parsed_versions.append({\n",
    "            \"publication_id\": version_info[\"publication_id\"],\n",
    "            \"version\": version_info[\"version\"],\n",
    "            \"cache_path\": cache_path,  # Store path, not tree\n",
    "            \"root\": None  # Placeholder\n",
    "        })\n",
    "\n",
    "        # Explicitly free memory\n",
    "        del root_node\n",
    "        gc.collect()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"[ERROR] {version_info['publication_id']} \"\n",
    "            f\"{version_info['version']}: {e}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f27abdf",
   "metadata": {},
   "source": [
    "## **Deduplication & ID Assignment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af7970a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deduplicating: 100%|██████████| 29/29 [00:00<00:00, 1049.63publication(s)/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: merged 29 publications\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 3 - Deduplication & Merge trees per publication\n",
    "\n",
    "Input:\n",
    "    parsed_versions: list of {\n",
    "        publication_id,\n",
    "        version,\n",
    "        root (hierarchy tree)\n",
    "    }\n",
    "\n",
    "Output:\n",
    "    final_trees: dict[publication_id] -> merged root tree\n",
    "\"\"\"\n",
    "\n",
    "# Group trees by publication_id\n",
    "pub_groups = defaultdict(list)\n",
    "for item in parsed_versions:\n",
    "    pub_groups[item[\"publication_id\"]].append(item)\n",
    "\n",
    "final_trees = {}\n",
    "\n",
    "for pub_id, versions in tqdm(pub_groups.items(), desc=\"Deduplicating\", unit=\"publication(s)\"):\n",
    "    if not versions:\n",
    "        continue\n",
    "        \n",
    "    # Sort versions\n",
    "    versions.sort(key=lambda x: int(x[\"version\"].split(\"v\")[-1]) if \"v\" in x[\"version\"] else 0)\n",
    "\n",
    "    # Load Base Tree\n",
    "    base_info = versions[0]\n",
    "    with open(base_info[\"cache_path\"], \"rb\") as f:\n",
    "        base_root = pickle.load(f)\n",
    "\n",
    "    # Normalize Base\n",
    "    fast_normalize_and_id(base_root, pub_id, base_info[\"version\"])\n",
    "    \n",
    "    # Build Index\n",
    "    content_index = build_content_index(base_root, USING_SHA256_HASH)\n",
    "\n",
    "    # Merge subsequent versions\n",
    "    for v_info in versions[1:]:\n",
    "        with open(v_info[\"cache_path\"], \"rb\") as f:\n",
    "            root = pickle.load(f)\n",
    "\n",
    "        # Normalize Source\n",
    "        fast_normalize_and_id(root, pub_id, v_info[\"version\"])\n",
    "        \n",
    "        # Deduplicate\n",
    "        deduplicate_tree(root, content_index, USING_SHA256_HASH)\n",
    "        \n",
    "        # Explicit cleanup\n",
    "        del root\n",
    "    \n",
    "    final_trees[pub_id] = base_root\n",
    "    \n",
    "    # Periodic GC to prevent memory fragmentation on large datasets\n",
    "    if len(final_trees) % 50 == 0:\n",
    "        gc.collect()\n",
    "\n",
    "print(f\"DONE: merged {len(final_trees)} publications\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3d4e584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample publication: 2211-13748\n",
      "Root node type: document\n",
      "Number of children: 16\n"
     ]
    }
   ],
   "source": [
    "# Kiểm tra nhanh\n",
    "sample_pub = next(iter(final_trees))\n",
    "root = final_trees[sample_pub]\n",
    "\n",
    "print(\"Sample publication:\", sample_pub)\n",
    "print(\"Root node type:\", root.node_type)\n",
    "print(\"Number of children:\", len(root.children))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0581f8",
   "metadata": {},
   "source": [
    "## **Export to JSON**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b676ef5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exporting JSON: 100%|██████████| 29/29 [00:00<00:00, 203.65pub/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported 29 publications to: 23127453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Export Structured Publication JSON\n",
    "\n",
    "Input:\n",
    "    final_trees: dict[publication_id] -> root_node\n",
    "\n",
    "Output:\n",
    "    Student ID/\n",
    "        <pub_id>/\n",
    "            <pub_id>.json\n",
    "            metadata.json\n",
    "            references.json\n",
    "        ...\n",
    "\"\"\"\n",
    "\n",
    "# Serialization helper\n",
    "def serialize_node(node):\n",
    "    \"\"\"Recursively serialize a node to dictionary.\"\"\"\n",
    "    return {\n",
    "        \"id\": node.id,\n",
    "        \"type\": node.node_type,\n",
    "        \"full_text\": getattr(node, \"full_text\", \"\"),\n",
    "        \"children\": [serialize_node(child) for child in node.children]\n",
    "    }\n",
    "\n",
    "# Export statistics\n",
    "export_count = 0\n",
    "missing_metadata = []\n",
    "missing_references = []\n",
    "\n",
    "for pub_id, root in tqdm(final_trees.items(), desc=\"Exporting JSON\", unit=\"pub\"):\n",
    "    \n",
    "    # Create publication subdirectory\n",
    "    pub_output_dir = os.path.join(OUTPUT_DIR, pub_id)\n",
    "    os.makedirs(pub_output_dir, exist_ok=True)\n",
    "    \n",
    "    # ===== 1. Export Content Tree (Parsed Hierarchy) =====\n",
    "    content_json = {\n",
    "        \"publication_id\": pub_id,\n",
    "        \"content_tree\": serialize_node(root)\n",
    "    }\n",
    "    \n",
    "    content_path = os.path.join(pub_output_dir, f\"{pub_id}.json\")\n",
    "    with open(content_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(content_json, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # ===== 2. Copy Metadata (Original from Semantic Scholar) =====\n",
    "    raw_metadata_path = os.path.join(RAW_ROOT, pub_id, \"metadata.json\")\n",
    "    metadata_path = os.path.join(pub_output_dir, \"metadata.json\")\n",
    "    \n",
    "    if os.path.exists(raw_metadata_path):\n",
    "        with open(raw_metadata_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            metadata = json.load(f)\n",
    "        with open(metadata_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "    else:\n",
    "        missing_metadata.append(pub_id)\n",
    "    \n",
    "    # ===== 3. Copy References (Original from Semantic Scholar) =====\n",
    "    raw_references_path = os.path.join(RAW_ROOT, pub_id, \"references.json\")\n",
    "    references_path = os.path.join(pub_output_dir, \"references.json\")\n",
    "    \n",
    "    if os.path.exists(raw_references_path):\n",
    "        with open(raw_references_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            references = json.load(f)\n",
    "        with open(references_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(references, f, ensure_ascii=False, indent=2)\n",
    "    else:\n",
    "        missing_references.append(pub_id)\n",
    "    \n",
    "    export_count += 1\n",
    "\n",
    "print(f\"Exported {export_count} publications to: {OUTPUT_DIR}\")\n",
    "\n",
    "if missing_metadata:\n",
    "    print(f\"\\nWARNING: {len(missing_metadata)} publication(s) missing metadata.json\")\n",
    "    print(f\"   {', '.join(missing_metadata[:5])}\" + (\" ...\" if len(missing_metadata) > 5 else \"\"))\n",
    "\n",
    "if missing_references:\n",
    "    print(f\"\\nWARNING: {len(missing_references)} publication(s) missing references.json\")\n",
    "    print(f\"   {', '.join(missing_references[:5])}\" + (\" ...\" if len(missing_references) > 5 else \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc87624e",
   "metadata": {},
   "source": [
    "## **Extract references**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74254e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting references:   0%|          | 0/42 [00:00<?, ?version/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Parsing .bib file: ../30-paper/2211-13748/tex/2211-13748v1\\mybibliography.bib\n",
      "[INFO] Parsing .bib file: ../30-paper/2211-13750/tex/2211-13750v1\\comparingsinglettestingschemes.bib\n",
      "[INFO] Parsing .bib file: ../30-paper/2211-13750/tex/2211-13750v2\\comparingsinglettestingschemes4.bib\n",
      "[INFO] Parsing .bib file: ../30-paper/2211-13751/tex/2211-13751v1\\bibliography_v3.bib\n",
      "[INFO] Parsing .bib file: ../30-paper/2211-13752/tex/2211-13752v1\\egbib.bib\n",
      "[INFO] Parsing .bib file: ../30-paper/2211-13754/tex/2211-13754v1\\refs.bib\n",
      "[WARN] Bibliography file not found: ../30-paper/2211-13755/tex/2211-13755v1\\egbib.bib\n",
      "[INFO] Parsing .bib file: ../30-paper/2211-13755/tex/2211-13755v2\\egbib.bib\n",
      "[INFO] Parsing .bib file: ../30-paper/2211-13757/tex/2211-13757v1\\bib.bib\n",
      "[INFO] Parsing .bib file: ../30-paper/2211-13757/tex/2211-13757v2\\bib.bib\n",
      "[WARN] Bibliography file not found: ../30-paper/2211-13758/tex/2211-13758v1\\example.bib\n",
      "[INFO] Parsing .bib file: ../30-paper/2211-13758/tex/2211-13758v1\\bibliography.bib\n",
      "[INFO] Parsing .bib file: ../30-paper/2211-13759/tex/2211-13759v1\\thisbibliography.bib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting references: 100%|██████████| 42/42 [00:00<00:00, 289.59version/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Parsing .bib file: ../30-paper/2211-13759/tex/2211-13759v2\\thisbibliography.bib\n",
      "[INFO] Parsing .bib file: ../30-paper/2211-13760/tex/2211-13760v1\\references.bib\n",
      "[INFO] Parsing .bib file: ../30-paper/2211-13760/tex/2211-13760v2\\references.bib\n",
      "[INFO] Parsing .bib file: ../30-paper/2211-13761/tex/2211-13761v1\\references.bib\n",
      "[INFO] Parsing .bib file: ../30-paper/2211-13762/tex/2211-13762v1\\egbib.bib\n",
      "[INFO] Parsing .bib file: ../30-paper/2211-13762/tex/2211-13762v2\\egbib.bib\n",
      "[INFO] Parsing .bib file: ../30-paper/2211-13764/tex/2211-13764v1\\biblio.bib\n",
      "[WARN] Bibliography file not found: ../30-paper/2211-13765/tex/2211-13765v1\\references.bib\n",
      "[INFO] Parsing .bib file: ../30-paper/2211-13766/tex/2211-13766v1\\apssamp.bib\n",
      "[INFO] Parsing .bib file: ../30-paper/2211-13766/tex/2211-13766v2\\apssamp.bib\n",
      "[INFO] Parsing .bib file: ../30-paper/2211-13766/tex/2211-13766v3\\apssamp.bib\n",
      "[INFO] Parsing .bib file: ../30-paper/2211-13767/tex/2211-13767v1\\references.bib\n",
      "[INFO] Parsing .bib file: ../30-paper/2211-13768/tex/2211-13768v1\\reference.bib\n",
      "[INFO] Parsing .bib file: ../30-paper/2211-13768/tex/2211-13768v2\\reference.bib\n",
      "[WARN] Bibliography file not found: ../30-paper/2211-13769/tex/2211-13769v1\\strings,refs.bib\n",
      "[INFO] Parsing .bib file: ../30-paper/2211-13769/tex/2211-13769v1\\egbib.bib\n",
      "[WARN] Bibliography file not found: ../30-paper/2211-13769/tex/2211-13769v2\\strings,refs.bib\n",
      "[INFO] Parsing .bib file: ../30-paper/2211-13769/tex/2211-13769v2\\egbib.bib\n",
      "[INFO] Parsing .bib file: ../30-paper/2211-13771/tex/2211-13771v1\\paper.bib\n",
      "[WARN] Bibliography file not found: ../30-paper/2211-13772/tex/2211-13772v1\\refs.bib\n",
      "[WARN] Bibliography file not found: ../30-paper/2211-13773/tex/2211-13773v1\\IEEEfull,trasfer.bib\n",
      "[WARN] Bibliography file not found: ../30-paper/2211-13773/tex/2211-13773v2\\IEEEfull,trasfer.bib\n",
      "[INFO] Parsing .bib file: ../30-paper/2211-13774/tex/2211-13774v1\\References.bib\n",
      "[INFO] Parsing .bib file: ../30-paper/2211-13774/tex/2211-13774v1\\References.bib\n",
      "[INFO] Parsing .bib file: ../30-paper/2211-13775/tex/2211-13775v1\\references.bib\n",
      "[WARN] Bibliography file not found: ../30-paper/2211-13775/tex/2211-13775v1\\egbib.bib\n",
      "[INFO] Parsing .bib file: ../30-paper/2211-13775/tex/2211-13775v2\\egbib.bib\n",
      "[INFO] Parsing .bib file: ../30-paper/2211-13776/tex/2211-13776v1\\acl2020.bib\n",
      "\n",
      "Publications found: 25/42\n",
      "Total unique references: 2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "raw_references = {}\n",
    "extract_errors = 0\n",
    "\n",
    "for version_info in tqdm(pub_results, desc=\"Extracting references\", unit=\"version\"):\n",
    "    \n",
    "    if version_info.get(\"status\") == \"NO_TEX\":\n",
    "        continue\n",
    "\n",
    "    pub_id = version_info[\"publication_id\"]\n",
    "    version = version_info[\"version\"]\n",
    "    \n",
    "    version_path = f\"{RAW_ROOT}/{pub_id}/tex/{version}\"\n",
    "    \n",
    "    if not os.path.exists(version_path):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        all_files_in_dir = os.listdir(version_path)\n",
    "        \n",
    "        target_files = [\n",
    "            f for f in all_files_in_dir \n",
    "            if f.lower().endswith(('.bib', '.bbl', '.tex'))\n",
    "        ]\n",
    "        \n",
    "        references = extract_references_from_tex_files(\n",
    "            version_path=version_path,\n",
    "            tex_files=target_files \n",
    "        )\n",
    "        \n",
    "        if references:\n",
    "            if pub_id not in raw_references:\n",
    "                raw_references[pub_id] = []\n",
    "            raw_references[pub_id].extend(references)\n",
    "            \n",
    "    except Exception as e:\n",
    "        extract_errors += 1\n",
    "        pass\n",
    "\n",
    "# Deduplicate & Report\n",
    "deduplicated_references = {\n",
    "    pub_id: deduplicate_references(refs)\n",
    "    for pub_id, refs in raw_references.items()\n",
    "}\n",
    "\n",
    "count_pubs = len(deduplicated_references)\n",
    "total_refs = sum(len(refs) for refs in deduplicated_references.values())\n",
    "\n",
    "print(f\"\\nPublications found: {count_pubs}/{len(pub_results)}\")\n",
    "print(f\"Total unique references: {total_refs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58f4370",
   "metadata": {},
   "source": [
    "## **Reference Cleaning**\n",
    "\n",
    "Clean and normalize BibTeX and arXiv references for matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9731aac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning BibTeX: 100%|██████████| 25/25 [00:01<00:00, 14.13pub/s]\n",
      "Loading & Cleaning arXiv Truth: 100%|██████████| 25/25 [00:00<00:00, 141.32pub/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DONE: 2018 cleaned BibTeX entries vs 684 cleaned arXiv entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned_bibtex = {}\n",
    "\n",
    "for pub_id in tqdm(deduplicated_references.keys(), desc=\"Cleaning BibTeX\", unit=\"pub\"):\n",
    "    cleaned_entries = []\n",
    "    for ref in deduplicated_references[pub_id]:\n",
    "        try:\n",
    "            cleaned_ref = clean_bibtex_entry(ref)\n",
    "            cleaned_ref['ref_id'] = ref.get('ref_id', '')\n",
    "            cleaned_ref['key'] = ref.get('key', '')\n",
    "            \n",
    "            cleaned_entries.append(cleaned_ref)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "            \n",
    "    cleaned_bibtex[pub_id] = cleaned_entries\n",
    "\n",
    "cleaned_arxiv = {}\n",
    "\n",
    "for pub_id in tqdm(cleaned_bibtex.keys(), desc=\"Loading & Cleaning arXiv Truth\", unit=\"pub\"):\n",
    "    ref_json_path = os.path.join(RAW_ROOT, pub_id, \"references.json\")\n",
    "    \n",
    "    if os.path.exists(ref_json_path):\n",
    "        try:\n",
    "            with open(ref_json_path, 'r', encoding='utf-8') as f:\n",
    "                arxiv_raw = json.load(f)\n",
    "            \n",
    "            cleaned_entries = []\n",
    "            for arxiv_id, meta in arxiv_raw.items():\n",
    "                meta['arxiv_id'] = arxiv_id \n",
    "                cleaned_ref = clean_arxiv_reference(meta)\n",
    "                cleaned_entries.append(cleaned_ref)\n",
    "            \n",
    "            cleaned_arxiv[pub_id] = cleaned_entries\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Error loading references.json for {pub_id}: {e}\")\n",
    "            cleaned_arxiv[pub_id] = []\n",
    "    else:\n",
    "        cleaned_arxiv[pub_id] = []\n",
    "\n",
    "# Summary\n",
    "total_bib = sum(len(x) for x in cleaned_bibtex.values())\n",
    "total_arxiv = sum(len(x) for x in cleaned_arxiv.values())\n",
    "print(f\"\\nDONE: {total_bib} cleaned BibTeX entries vs {total_arxiv} cleaned arXiv entries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c3c237",
   "metadata": {},
   "source": [
    "## **Labeling & Dataset Construction**\n",
    "\n",
    "Generate labeled dataset using heuristic matching and manual ground truth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6847920a",
   "metadata": {},
   "source": [
    "### **Manual Labeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97201e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1147 manual pairs.\n"
     ]
    }
   ],
   "source": [
    "# MANUAL PAIRS\n",
    "FIXED_MANUAL_DATA = {\n",
    "    \"2211-13768\": {\n",
    "        \"2008MNRAS.391.1685S\": \"0809-0898\",\n",
    "        \"2014MNRAS.441.3359D\": \"1402-7073\",\n",
    "        \"2021MNRAS.503..920C\": \"2007-02958\",\n",
    "        \"2022arXiv220405981K\": \"2204-05981\",\n",
    "        \"Agrawal11611004611\": \"1610-04611\"\n",
    "    },\n",
    "    \"2211-13757\": {\n",
    "        \"3DiM\": \"2210-04628\",\n",
    "        \"acronym\": \"2011-09584\",\n",
    "        \"attention\": \"1706-03762\",\n",
    "        \"autosdf\": \"2203-09516\",\n",
    "        \"cascaded-point-completion\": \"2004-03327\"\n",
    "    },\n",
    "    \"2211-13767\": {\n",
    "        \"Bapat2018\": \"1812-02746\",\n",
    "        \"Bittel_2021\": \"2101-07267\",\n",
    "        \"Brady2021\": \"2107-01218\",\n",
    "        \"Crosson_2021\": \"2008-09913\",\n",
    "        \"Farhi2016\": \"1602-07674\"\n",
    "    },\n",
    "    \"2211-13755\": {\n",
    "        \"AANet\": \"2004-09548\",\n",
    "        \"ACVNet\": \"2203-02146\",\n",
    "        \"AcfNet\": \"1909-03751\",\n",
    "        \"AnyNet\": \"1810-11408\",\n",
    "        \"BI3D\": \"2005-07274\"\n",
    "    },\n",
    "    \"2211-13766\": {\n",
    "        \"BittencourtDamping2022\": \"2301-11920\",\n",
    "        \"Marius_Schrodinger_2022\": \"2211-00449\",\n",
    "        \"asjad2022magnon\": \"2203-10767\",\n",
    "        \"bourcin2022strong\": \"2209-14643\",\n",
    "        \"chan2011laser\": \"1106-3614\"\n",
    "    }\n",
    "}\n",
    "\n",
    "manual_pairs = []\n",
    "for pub_id, labels in FIXED_MANUAL_DATA.items():\n",
    "    arxiv_pool = cleaned_arxiv.get(pub_id, [])\n",
    "    bib_pool = cleaned_bibtex.get(pub_id, [])\n",
    "    \n",
    "    if not arxiv_pool or not bib_pool: continue\n",
    "    \n",
    "    for bib_key, target_id in labels.items():\n",
    "        bib_entry = next((e for e in bib_pool if e.get('key') == bib_key), None)\n",
    "        target_arxiv = next((e for e in arxiv_pool if e.get('arxiv_id') == target_id), None)\n",
    "        \n",
    "        if bib_entry and target_arxiv:\n",
    "            # Base record structure\n",
    "            base = {\n",
    "                'pub_id': pub_id,\n",
    "                'bib_key': bib_key,\n",
    "                'bib_ref_id': bib_entry.get('ref_id'), \n",
    "                'bib_title_clean': bib_entry.get('normalized_title'),\n",
    "                'bib_authors_clean': \", \".join(bib_entry.get('normalized_authors', [])),\n",
    "                'bib_author_tokens': str(bib_entry.get('author_tokens', [])),\n",
    "                'bib_year': bib_entry.get('normalized_year'),\n",
    "                'source': 'manual'\n",
    "            }\n",
    "            \n",
    "            # Positive Pair\n",
    "            pos = base.copy()\n",
    "            pos.update({\n",
    "                'candidate_arxiv_id': target_id,\n",
    "                'candidate_title_clean': target_arxiv.get('normalized_title'),\n",
    "                'candidate_authors_clean': \", \".join(target_arxiv.get('normalized_authors', [])),\n",
    "                'candidate_author_tokens': str(target_arxiv.get('author_tokens', [])),\n",
    "                'candidate_year': target_arxiv.get('normalized_year'),\n",
    "                'pair_type': 'positive_manual', 'label': 1\n",
    "            })\n",
    "            manual_pairs.append(pos)\n",
    "            \n",
    "            # Negative Pairs (Exhaustive)\n",
    "            for neg in [a for a in arxiv_pool if a['arxiv_id'] != target_id]:\n",
    "                neg_row = base.copy()\n",
    "                neg_row.update({\n",
    "                    'candidate_arxiv_id': neg['arxiv_id'],\n",
    "                    'candidate_title_clean': neg.get('normalized_title'),\n",
    "                    'candidate_authors_clean': \", \".join(neg.get('normalized_authors', [])),\n",
    "                    'candidate_author_tokens': str(neg.get('author_tokens', [])),\n",
    "                    'candidate_year': neg.get('normalized_year'),\n",
    "                    'pair_type': 'negative_manual', 'label': 0\n",
    "                })\n",
    "                manual_pairs.append(neg_row)\n",
    "\n",
    "print(f\"Generated {len(manual_pairs)} manual pairs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fa24ce",
   "metadata": {},
   "source": [
    "### **Automatic Labeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b97ab7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Quota (10%): 156 pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning candidates: 100%|██████████| 20/20 [00:09<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Candidates > 0.4: 552\n",
      "Selected Top 156 (Min Score: 0.831)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Auto Dataset: 100%|██████████| 156/156 [00:00<00:00, 11676.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated 4641 automatic pairs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "auto_pub_ids = [p for p in cleaned_bibtex.keys() if p not in FIXED_MANUAL_DATA]\n",
    "total_refs = sum(len(cleaned_bibtex[p]) for p in auto_pub_ids)\n",
    "target_quota = int(total_refs * 0.10)\n",
    "print(f\"Target Quota (10%): {target_quota} pairs\")\n",
    "\n",
    "# 2. Collect candidates\n",
    "all_potential_matches = []\n",
    "for pub_id in tqdm(auto_pub_ids, desc=\"Scanning candidates\"):\n",
    "    arxiv_pool = cleaned_arxiv.get(pub_id, [])\n",
    "    bib_pool = cleaned_bibtex.get(pub_id, [])\n",
    "    if not arxiv_pool or not bib_pool: continue\n",
    "    \n",
    "    for bib in bib_pool:\n",
    "        # Match\n",
    "        match = find_best_match(bib, arxiv_pool, threshold=0.0)\n",
    "        \n",
    "        if match:\n",
    "            best_id, score, _ = match\n",
    "            \n",
    "            if score > 0.4:\n",
    "                best_arxiv = next(a for a in arxiv_pool if a['arxiv_id'] == best_id)\n",
    "                all_potential_matches.append({\n",
    "                    'pub_id': pub_id, 'bib': bib, 'best_arxiv': best_arxiv,\n",
    "                    'score': score, 'pool': arxiv_pool\n",
    "                })\n",
    "\n",
    "# 3. Sort & Select Top K\n",
    "all_potential_matches.sort(key=lambda x: x['score'], reverse=True)\n",
    "selected_matches = all_potential_matches[:target_quota]\n",
    "\n",
    "print(f\"\\nCandidates > 0.4: {len(all_potential_matches)}\")\n",
    "print(f\"Selected Top {len(selected_matches)} (Min Score: {selected_matches[-1]['score']:.3f})\")\n",
    "\n",
    "# 4. Generate Auto Pairs\n",
    "automatic_pairs = []\n",
    "for item in tqdm(selected_matches, desc=\"Building Auto Dataset\"):\n",
    "    bib = item['bib']\n",
    "    best_arxiv = item['best_arxiv']\n",
    "    \n",
    "    base = {\n",
    "        'pub_id': item['pub_id'],\n",
    "        'bib_key': bib.get('key'),\n",
    "        'bib_ref_id': bib.get('ref_id'), \n",
    "        'bib_title_clean': bib.get('normalized_title'),\n",
    "        'bib_authors_clean': \", \".join(bib.get('normalized_authors', [])),\n",
    "        'bib_author_tokens': str(bib.get('author_tokens', [])), \n",
    "        'bib_year': bib.get('normalized_year'),\n",
    "        'source': 'automatic'\n",
    "    }\n",
    "    \n",
    "    # Positive\n",
    "    pos = base.copy()\n",
    "    pos.update({\n",
    "        'candidate_arxiv_id': best_arxiv['arxiv_id'],\n",
    "        'candidate_title_clean': best_arxiv.get('normalized_title'),\n",
    "        'candidate_authors_clean': \", \".join(best_arxiv.get('normalized_authors', [])),\n",
    "        'candidate_author_tokens': str(best_arxiv.get('author_tokens', [])),\n",
    "        'candidate_year': best_arxiv.get('normalized_year'),\n",
    "        'pair_type': 'positive_auto', 'label': 1\n",
    "    })\n",
    "    automatic_pairs.append(pos)\n",
    "    \n",
    "    # Negatives\n",
    "    for neg in [a for a in item['pool'] if a['arxiv_id'] != best_arxiv['arxiv_id']]:\n",
    "        neg_row = base.copy()\n",
    "        neg_row.update({\n",
    "            'candidate_arxiv_id': neg['arxiv_id'],\n",
    "            'candidate_title_clean': neg.get('normalized_title'),\n",
    "            'candidate_authors_clean': \", \".join(neg.get('normalized_authors', [])),\n",
    "            'candidate_author_tokens': str(neg.get('author_tokens', [])),\n",
    "            'candidate_year': neg.get('normalized_year'),\n",
    "            'pair_type': 'negative_auto', 'label': 0\n",
    "        })\n",
    "        automatic_pairs.append(neg_row)\n",
    "\n",
    "print(f\"\\nGenerated {len(automatic_pairs)} automatic pairs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1591d4e",
   "metadata": {},
   "source": [
    "## **Export CSV**\n",
    "\n",
    "Export labeled dataset for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33b20b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positives: 177\n",
      "Negatives: 5611\n",
      "Total pairs: 5788\n",
      "Saved labeled dataset to: ../src/labeled_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "final_df = pd.DataFrame(manual_pairs + automatic_pairs)\n",
    "\n",
    "cols = [\n",
    "    'pub_id', 'bib_key', 'bib_ref_id', \n",
    "    'bib_title_clean', 'bib_authors_clean', 'bib_author_tokens', 'bib_year',\n",
    "    'candidate_arxiv_id', \n",
    "    'candidate_title_clean', 'candidate_authors_clean', 'candidate_author_tokens', 'candidate_year',\n",
    "    'source', 'pair_type', 'label'\n",
    "]\n",
    "\n",
    "for c in cols: \n",
    "    if c not in final_df.columns: final_df[c] = None\n",
    "\n",
    "labeled_df = final_df[cols]\n",
    "\n",
    "print(f\"Positives: {len(labeled_df[labeled_df['label']==1])}\")\n",
    "print(f\"Negatives: {len(labeled_df[labeled_df['label']==0])}\")\n",
    "print(f\"Total pairs: {len(labeled_df)}\")\n",
    "\n",
    "csv_output_path = \"../src/labeled_dataset.csv\"\n",
    "labeled_df.to_csv(csv_output_path, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Saved labeled dataset to: {csv_output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
