{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccc8a3f4",
   "metadata": {},
   "source": [
    "# <center>**Milestone 2**<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b11d9f",
   "metadata": {},
   "source": [
    "## **Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e7be47b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import gc\n",
    "import csv\n",
    "import pickle\n",
    "import sys\n",
    "import random\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from __future__ import annotations\n",
    "from collections import defaultdict\n",
    "\n",
    "from my_scanner import *\n",
    "from my_parser import *\n",
    "from my_matcher import *\n",
    "from my_featureEngineering import *\n",
    "from my_modeling import *\n",
    "from my_evaluation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beb22ab",
   "metadata": {},
   "source": [
    "## **Configuration**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "69aca721",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_ROOT = \"../30-paper\"\n",
    "USING_SHA256_HASH = False\n",
    "\n",
    "# 1. Output directory\n",
    "OUTPUT_DIR = \"23127453\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Configuration for caching\n",
    "CACHE_DIR = \".cache\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "sys.setrecursionlimit(20000)\n",
    "\n",
    "MANUAL_DIR = \"manual_labeling\"\n",
    "os.makedirs(MANUAL_DIR, exist_ok=True)\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde1af6d",
   "metadata": {},
   "source": [
    "## **Dataset rescan**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "cb2d3ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2211-13747 NO_TEX []\n",
      "2211-13748 READY ['2211-13748v1']\n",
      "2211-13749 READY ['2211-13749v1']\n",
      "2211-13750 READY ['2211-13750v1', '2211-13750v2']\n",
      "2211-13751 READY ['2211-13751v1']\n",
      "2211-13752 READY ['2211-13752v1']\n",
      "2211-13753 READY ['2211-13753v1']\n",
      "2211-13754 READY ['2211-13754v1']\n",
      "2211-13755 READY ['2211-13755v1', '2211-13755v2']\n",
      "2211-13756 READY ['2211-13756v1', '2211-13756v2']\n",
      "2211-13757 READY ['2211-13757v1', '2211-13757v2']\n",
      "2211-13758 READY ['2211-13758v1']\n",
      "2211-13759 READY ['2211-13759v1', '2211-13759v2']\n",
      "2211-13760 READY ['2211-13760v1', '2211-13760v2']\n",
      "2211-13761 READY ['2211-13761v1']\n",
      "2211-13762 READY ['2211-13762v1', '2211-13762v2']\n",
      "2211-13763 READY ['2211-13763v1']\n",
      "2211-13764 READY ['2211-13764v1']\n",
      "2211-13765 READY ['2211-13765v1']\n",
      "2211-13766 READY ['2211-13766v1', '2211-13766v2', '2211-13766v3']\n",
      "2211-13767 READY ['2211-13767v1']\n",
      "2211-13768 READY ['2211-13768v1', '2211-13768v2']\n",
      "2211-13769 READY ['2211-13769v1', '2211-13769v2']\n",
      "2211-13770 READY ['2211-13770v1']\n",
      "2211-13771 READY ['2211-13771v1']\n",
      "2211-13772 READY ['2211-13772v1']\n",
      "2211-13773 READY ['2211-13773v1', '2211-13773v2']\n",
      "2211-13774 READY ['2211-13774v1']\n",
      "2211-13775 READY ['2211-13775v1', '2211-13775v2']\n",
      "2211-13776 READY ['2211-13776v1']\n",
      "2211-13777 READY ['2211-13777v1', '2211-13777v2', '2211-13777v3']\n",
      "2211-13778 READY ['2211-13778v1', '2211-13778v2']\n",
      "2211-13779 READY ['2211-13779v1', '2211-13779v2', '2211-13779v3']\n",
      "2211-13780 READY ['2211-13780v1', '2211-13780v2']\n",
      "2211-13781 READY ['2211-13781v1']\n",
      "2211-13782 READY ['2211-13782v1']\n",
      "2211-13783 READY ['2211-13783v1']\n",
      "2211-13784 READY ['2211-13784v1']\n",
      "2211-13785 READY ['2211-13785v1', '2211-13785v2', '2211-13785v3']\n",
      "2211-13786 READY ['2211-13786v1']\n",
      "2211-13787 READY ['2211-13787v1', '2211-13787v2']\n",
      "2211-13788 READY ['2211-13788v1']\n",
      "2211-13789 READY ['2211-13789v1', '2211-13789v2', '2211-13789v3']\n",
      "2211-13790 READY ['2211-13790v1', '2211-13790v2']\n",
      "2211-13791 READY ['2211-13791v1']\n",
      "2211-13792 READY ['2211-13792v1', '2211-13792v2']\n",
      "2211-13793 READY ['2211-13793v1', '2211-13793v2']\n",
      "2211-13794 READY ['2211-13794v1']\n",
      "2211-13795 READY ['2211-13795v1', '2211-13795v2']\n",
      "2211-13796 READY ['2211-13796v1', '2211-13796v2']\n",
      "2211-13797 READY ['2211-13797v1']\n",
      "2211-13798 READY ['2211-13798v1', '2211-13798v2']\n",
      "2211-13799 NO_TEX []\n",
      "2211-13800 READY ['2211-13800v1']\n",
      "2211-13801 READY ['2211-13801v1']\n",
      "2211-13802 READY ['2211-13802v1', '2211-13802v2']\n",
      "2211-13803 READY ['2211-13803v1']\n",
      "2211-13804 NO_TEX []\n",
      "2211-13805 READY ['2211-13805v1']\n",
      "2211-13806 READY ['2211-13806v1']\n",
      "2211-13807 READY ['2211-13807v1', '2211-13807v2']\n",
      "2211-13808 READY ['2211-13808v1']\n",
      "2211-13809 READY ['2211-13809v1']\n",
      "2211-13810 READY ['2211-13810v1']\n",
      "2211-13811 READY ['2211-13811v1']\n",
      "2211-13812 NO_TEX []\n",
      "2211-13813 READY ['2211-13813v1', '2211-13813v2']\n",
      "2211-13814 READY ['2211-13814v1', '2211-13814v2']\n",
      "2211-13815 READY ['2211-13815v1']\n",
      "2211-13816 READY ['2211-13816v1']\n",
      "2211-13817 READY ['2211-13817v2', '2211-13817v3']\n",
      "2211-13818 READY ['2211-13818v1']\n",
      "2211-13819 READY ['2211-13819v1']\n",
      "2211-13820 READY ['2211-13820v1']\n",
      "2211-13821 READY ['2211-13821v1']\n",
      "2211-13822 READY ['2211-13822v1', '2211-13822v2']\n",
      "2211-13823 READY ['2211-13823v1']\n",
      "2211-13824 READY ['2211-13824v1', '2211-13824v2', '2211-13824v3', '2211-13824v4']\n",
      "2211-13825 INVALID ['2211-13825v1']\n",
      "2211-13826 READY ['2211-13826v1']\n",
      "2211-13827 NO_TEX []\n",
      "2211-13828 READY ['2211-13828v1']\n",
      "2211-13829 READY ['2211-13829v1', '2211-13829v2']\n",
      "2211-13830 READY ['2211-13830v1']\n",
      "2211-13831 READY ['2211-13831v1']\n",
      "2211-13832 READY ['2211-13832v1', '2211-13832v2']\n",
      "2211-13833 READY ['2211-13833v1']\n",
      "2211-13834 READY ['2211-13834v1', '2211-13834v2']\n",
      "2211-13835 READY ['2211-13835v1']\n",
      "2211-13836 READY ['2211-13836v1']\n",
      "2211-13837 READY ['2211-13837v1']\n",
      "2211-13838 READY ['2211-13838v1', '2211-13838v2']\n",
      "2211-13839 READY ['2211-13839v1', '2211-13839v2']\n",
      "2211-13840 READY ['2211-13840v1']\n",
      "2211-13841 READY ['2211-13841v1']\n",
      "2211-13842 READY ['2211-13842v1']\n",
      "2211-13843 READY ['2211-13843v1', '2211-13843v2']\n",
      "2211-13844 READY ['2211-13844v1']\n",
      "2211-13845 READY ['2211-13845v1']\n",
      "2211-13846 READY ['2211-13846v1', '2211-13846v2']\n"
     ]
    }
   ],
   "source": [
    "scan_result = scan_dataset(RAW_ROOT)\n",
    "\n",
    "for k, v in scan_result.items():\n",
    "    print(k, v[\"status\"], v[\"versions\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857cf672",
   "metadata": {},
   "source": [
    "## **Version-level Multi-file Resolver**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7c3907b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Publications: 100%|██████████| 94/94 [00:15<00:00,  5.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2211-13748 2211-13748v1 ['weibo.tex']\n",
      "2211-13749 2211-13749v1 ['On_projections_of_tailsvA.tex']\n",
      "2211-13750 2211-13750v1 ['comparingsinglettestingschemes.tex']\n",
      "2211-13750 2211-13750v2 ['comparingsinglettestingschemes4.tex']\n",
      "2211-13751 2211-13751v1 ['Kane_et_al_JFM_v1.tex']\n",
      "2211-13752 2211-13752v1 ['main.tex', 'macros.tex', '00_abstract.tex', '01_intro.tex', '02_related.tex', '03_method.tex', '04_results.tex', '05_conclusion.tex', '07_acc.tex', '06_appendix.tex']\n",
      "2211-13753 2211-13753v1 ['0-title.tex']\n",
      "2211-13754 2211-13754v1 ['main.tex']\n",
      "2211-13755 2211-13755v1 ['main.tex']\n",
      "2211-13755 2211-13755v2 ['main.tex']\n",
      "2211-13756 2211-13756v1 ['main.tex']\n",
      "2211-13756 2211-13756v2 ['main.tex']\n",
      "2211-13757 2211-13757v1 ['main.tex']\n",
      "2211-13757 2211-13757v2 ['main.tex']\n",
      "2211-13758 2211-13758v1 ['main.tex']\n",
      "2211-13759 2211-13759v1 ['main.tex']\n",
      "2211-13759 2211-13759v2 ['main.tex']\n",
      "2211-13760 2211-13760v1 ['main.tex']\n",
      "2211-13760 2211-13760v2 ['main.tex']\n",
      "2211-13761 2211-13761v1 ['main.tex']\n",
      "2211-13762 2211-13762v1 ['main.tex', 'overall.tex', 'splits.tex']\n",
      "2211-13762 2211-13762v2 ['main.tex', 'overall.tex', 'splits.tex']\n",
      "2211-13763 2211-13763v1 ['main.tex', 'table.tex']\n",
      "2211-13764 2211-13764v1 ['accepted.tex']\n",
      "2211-13765 2211-13765v1 ['main.tex']\n",
      "2211-13766 2211-13766v1 ['PRL_Version.tex']\n",
      "2211-13766 2211-13766v2 ['arXiv_Version.tex']\n",
      "2211-13766 2211-13766v3 ['arXiv_Version.tex']\n",
      "2211-13767 2211-13767v1 ['main_arxiv.tex']\n",
      "2211-13768 2211-13768v1 ['main.tex', 'hyperlink-year-only-natbib-patch.tex']\n",
      "2211-13768 2211-13768v2 ['main.tex', 'hyperlink-year-only-natbib-patch.tex']\n",
      "2211-13769 2211-13769v1 ['main.tex', 'abstract.tex', 'intro.tex', 'conclude.tex', 'siamfc.tex']\n",
      "2211-13769 2211-13769v2 ['Appendix.tex', 'ostrack.tex', 'dimp_new.tex', 'stark.tex']\n",
      "2211-13770 2211-13770v1 ['skeleton.tex']\n",
      "2211-13771 2211-13771v1 ['paper.tex', 'penrose.tex']\n",
      "2211-13772 2211-13772v1 ['main.tex']\n",
      "2211-13773 2211-13773v1 ['draft_single.tex']\n",
      "2211-13773 2211-13773v2 ['draft_single.tex']\n",
      "2211-13774 2211-13774v1 ['Appendices.tex']\n",
      "2211-13775 2211-13775v1 ['main.tex', 'macros.tex', '00_abstract.tex', '01_introduction.tex', 'teaser_pdf.tex', '02_related_work.tex', '03_method.tex', 'diagram_pdf.tex', '04_results.tex', 'attack_pdf.tex', 'attacks_comp_pdf.tex', 'pc_comparison.tex', 'classifier_evaluation.tex', 'detector_evaluation.tex', 'coma_beta_pdf.tex', 'transferability_pdf.tex', '05_conclusions.tex', 'supplementary.tex', 'supp_analysis.tex', 'coma_tsne_pdf.tex', 'smal_beta_pdf.tex', 'oods_pdf.tex', 'oods.tex', 'transferability_geometric.tex', 'transferability_semantic.tex', 'semantic_comp_pdf.tex', 'coma_freq_comp_pdf.tex', 'supp_ablation.tex', 'freq_ablation_pdf.tex', 'reg_comp_pdf.tex', 'reg_comp.tex', 'euclidean_comp.tex', 'self_basis_comp.tex', 'random_targets.tex', 'supp_settings.tex', 'supp_results.tex', 'stability.tex', 'coma_stability_pdf.tex', 'smal_stability_pdf.tex', 'coma_failure_pdf.tex', 'smal_failure_pdf.tex', 'coma_evolution_pdf.tex', 'smal_evolution_pdf.tex', 'quiz_pdf.tex', 'coma_concat_1_pdf.tex', 'coma_concat_2_pdf.tex', 'smal_concat_1_pdf.tex', 'smal_concat_2_pdf.tex']\n",
      "2211-13775 2211-13775v2 ['egpaper_final.tex', 'macros.tex', '00_abstract.tex', '01_introduction.tex', 'teaser_pdf.tex', '02_related_work.tex', '03_method.tex', 'diagram_pdf.tex', 'quiz_main_pdf.tex', '04_results.tex', 'attack_pdf.tex', 'attacks_comp_pdf.tex', 'classifier_evaluation.tex', 'detector_evaluation.tex', 'semantic_comp_pdf.tex', 'transferability_pdf.tex', 'lpf_defense_pdf.tex', 'beta_pdf.tex', '05_conclusions.tex', 'supplementary.tex', '01_supp_analysis.tex', 'pc_comparison.tex', 'coma_tsne_pdf.tex', 'oods_pdf.tex', 'oods.tex', 'transferability_geometric.tex', 'transferability_semantic.tex', '02_supp_ablation.tex', 'coma_freq_comp_pdf.tex', 'freq_ablation_pdf.tex', 'reg_comp_pdf.tex', 'reg_comp.tex', 'euclidean_comp.tex', 'self_basis_comp.tex', 'random_targets.tex', '03_supp_settings.tex', '04_supp_results.tex', 'stability.tex', 'quiz_answers_pdf.tex', 'coma_stability_pdf.tex', 'smal_stability_pdf.tex', 'coma_failure_pdf.tex', 'smal_failure_pdf.tex', 'coma_evolution_pdf.tex', 'smal_evolution_pdf.tex', 'coma_concat_1_pdf.tex', 'coma_concat_2_pdf.tex', 'smal_concat_1_pdf.tex', 'smal_concat_2_pdf.tex']\n",
      "2211-13776 2211-13776v1 ['acl2020.tex']\n",
      "2211-13777 2211-13777v1 ['project.tex']\n",
      "2211-13777 2211-13777v2 ['project.tex']\n",
      "2211-13777 2211-13777v3 ['project.tex']\n",
      "2211-13778 2211-13778v1 ['conference_101719.tex']\n",
      "2211-13778 2211-13778v2 ['conference_101719.tex']\n",
      "2211-13779 2211-13779v1 ['liu2022ral.tex']\n",
      "2211-13779 2211-13779v2 ['liu2022ral.tex']\n",
      "2211-13779 2211-13779v3 ['liu2022ral.tex', 'highwayTable.tex']\n",
      "2211-13780 2211-13780v1 ['paper.tex']\n",
      "2211-13780 2211-13780v2 ['Paper_2pages.tex']\n",
      "2211-13781 2211-13781v1 ['paper.tex']\n",
      "2211-13782 2211-13782v1 ['NJPManuscript.tex']\n",
      "2211-13783 2211-13783v1 ['Holographic_transport.tex']\n",
      "2211-13784 2211-13784v1 ['ifacwc.tex']\n",
      "2211-13785 2211-13785v1 ['PaperForReview.tex', '0_abstract.tex', '1_intro.tex', '2_related_work.tex', '3_dataset.tex', '4_preliminaries.tex', '5_method.tex', '7_experiments.tex', '8_conclusion.tex']\n",
      "2211-13785 2211-13785v2 ['neu.tex', '0_abstract.tex', '1_intro.tex', '2_related_work.tex', '3_dataset.tex', '4_preliminaries.tex', '5_method.tex', '7_experiments.tex', 'main_quantitative_fp.tex', 'puzzle_main.tex', 'ablation_attention.tex', 'ablation_roomtype.tex', '8_conclusion.tex', 'sups-method.tex', 'sups-dataset.tex', 'sups-exps.tex']\n",
      "2211-13785 2211-13785v3 ['neu.tex', '0_abstract.tex', '1_intro.tex', '2_related_work.tex', '3_dataset.tex', '4_preliminaries.tex', '5_method.tex', '7_experiments.tex', 'main_quantitative_fp.tex', 'puzzle_main.tex', 'ablation_attention.tex', 'ablation_roomtype.tex', '8_conclusion.tex', 'sups-method.tex', 'sups-dataset.tex', 'sups-exps.tex']\n",
      "2211-13786 2211-13786v1 ['main.tex']\n",
      "2211-13787 2211-13787v1 ['IoT.tex']\n",
      "2211-13787 2211-13787v2 ['IoT.tex']\n",
      "2211-13788 2211-13788v1 ['Main.tex']\n",
      "2211-13789 2211-13789v1 ['mnras_template.tex']\n",
      "2211-13789 2211-13789v2 ['mnras_template.tex']\n",
      "2211-13789 2211-13789v3 ['mnras_template.tex']\n",
      "2211-13790 2211-13790v1 ['Approximating_the_chromatic_polynomial.tex']\n",
      "2211-13790 2211-13790v2 ['main.tex']\n",
      "2211-13791 2211-13791v1 ['BLL6submit.tex']\n",
      "2211-13792 2211-13792v1 ['main.tex']\n",
      "2211-13792 2211-13792v2 ['main.tex']\n",
      "2211-13793 2211-13793v1 ['main_paper.tex', 'abstract.tex', 'introduction.tex', 'related_work.tex', 'data.tex', 'methodology.tex', 'results.tex', 'discussion.tex', 'conclusion.tex']\n",
      "2211-13793 2211-13793v2 ['main_paper.tex', 'abstract.tex', 'introduction.tex', 'related_work.tex', 'data.tex', 'methodology.tex', 'results.tex', 'discussion.tex', 'conclusion.tex', 'acks.tex']\n",
      "2211-13794 2211-13794v1 ['QA_paper_acl.tex']\n",
      "2211-13795 2211-13795v1 ['main.tex']\n",
      "2211-13795 2211-13795v2 ['rsc-articletemplate-softmatter.tex']\n",
      "2211-13796 2211-13796v1 ['RPC2022_Bilki.tex']\n",
      "2211-13796 2211-13796v2 ['RPC2022_Bilki.tex']\n",
      "2211-13797 2211-13797v1 ['for_reference.tex', 'sec_01intro.tex', 'sec_02related_work.tex', 'sec_03prob_formulation.tex', 'sup_07general_problem_form.tex', 'sec_04algorithm.tex', 'sec_05theory.tex', 'sec_06experiments.tex', 'sup_03tab_data_demo.tex', 'sup_06tabs_alg_performance.tex', 'sup_04fig_exp_performance.tex', 'sec_07conclusion.tex', 'sec_08appendix.tex', 'sup_009_non_robust.tex']\n",
      "2211-13798 2211-13798v1 ['main.tex']\n",
      "2211-13798 2211-13798v2 ['main.tex']\n",
      "2211-13800 2211-13800v1 ['Paper.tex']\n",
      "2211-13801 2211-13801v1 ['main.tex']\n",
      "2211-13802 2211-13802v1 ['seq_grad_coding_ICLR_2023_V3.tex', 'math_commands.tex']\n",
      "2211-13802 2211-13802v2 ['seq_grad_coding_ICLR_2023_V3.tex', 'math_commands.tex']\n",
      "2211-13803 2211-13803v1 ['Magnaterra_Ba3TiIr2O9_RIXS.tex']\n",
      "2211-13805 2211-13805v1 ['RHEA-CN-SRO-PRL-Final-rename.tex']\n",
      "2211-13806 2211-13806v1 ['chiral_magnons_RuO2_fin.tex']\n",
      "2211-13807 2211-13807v1 ['main.tex', 'macros.tex', 'abstract.tex', 'introduction.tex', 'related_works.tex', 'method.tex', 'gallery-enrichment.tex', 'face_and_pose_match.tex', 'track_to_score.tex', 'evaluation.tex', 'pipeline.tex', '42street_datasample.tex', 'experiments.tex', 'threshold-selection.tex', 'existing_datasets_results.tex', 'CAL_generalization.tex', '42_street_overview_results_test.tex', 'limitations.tex', 'ethics.tex', 'conclusion.tex', 'appendix.tex', 'existing-benchmarks-more-models.tex', '42_street_overview_results_test_extended_sup.tex', 'extended_existing_datasets_results.tex', 'query_enriched_growth.tex', 'existing-datasets-ablation.tex', '42_ablation_results.tex', 'labeled_gallery_creation.tex', 'thresholds-benchmarks.tex']\n",
      "2211-13807 2211-13807v2 ['main.tex', 'macros.tex', '0-abstract.tex', '1-introduction.tex', 'gallery-enrichment.tex', '2-related_works.tex', '3-method.tex', 'face_and_pose_match.tex', 'track_to_score.tex', '4-the_42Street_dataset.tex', '5-experiments.tex', 'datasets-stats.tex', 'table-1-method-improvment.tex', 'table-2-PRCC-LTCC.tex', 'table-4-LaST.tex', 'table-VC-Clothes.tex', 'table-3-CCVID.tex', 'CAL_generalization.tex', '42street_datasample.tex', '6-ablation.tex', '42_street_overview_results_test.tex', '42_ablation_results.tex', '7-ethics.tex', '8-limitations.tex', '9-conclusion.tex', '10-acknowledgements.tex', '11-appendix.tex', 'query_enriched_growth.tex', 'sup-alpha-ablation.tex', 'thresholds-benchmarks.tex', 'threshold-selection.tex', 'similarity_matrix_calculation.tex', '42_street_open_set.tex']\n",
      "2211-13808 2211-13808v1 ['sn-article.tex']\n",
      "2211-13809 2211-13809v1 ['RSD2_v5.tex']\n",
      "2211-13810 2211-13810v1 ['main.tex']\n",
      "2211-13811 2211-13811v1 ['aa.tex']\n",
      "2211-13813 2211-13813v1 ['Formatting-Instructions-LaTeX-2022.tex']\n",
      "2211-13813 2211-13813v2 ['Formatting-Instructions-LaTeX-2022.tex']\n",
      "2211-13814 2211-13814v1 ['main.tex']\n",
      "2211-13814 2211-13814v2 ['main.tex']\n",
      "2211-13815 2211-13815v1 ['neurips_2022.tex']\n",
      "2211-13816 2211-13816v1 ['manuscript.tex']\n",
      "2211-13817 2211-13817v2 ['4-mapping-results.tex']\n",
      "2211-13817 2211-13817v3 ['elsarticle-template.tex']\n",
      "2211-13818 2211-13818v1 ['Globecom_Revised.tex']\n",
      "2211-13819 2211-13819v1 ['main.tex']\n",
      "2211-13820 2211-13820v1 ['aanda.tex']\n",
      "2211-13821 2211-13821v1 ['Inductive_and_Inverse_Limits_-_Kamil_Urbas.tex']\n",
      "2211-13822 2211-13822v1 ['main.tex']\n",
      "2211-13822 2211-13822v2 ['main.tex']\n",
      "2211-13823 2211-13823v1 ['main.tex']\n",
      "2211-13824 2211-13824v1 ['grassmann.tex']\n",
      "2211-13824 2211-13824v2 ['grassmann.tex']\n",
      "2211-13824 2211-13824v3 ['grassmann.tex']\n",
      "2211-13824 2211-13824v4 ['grassmann.tex']\n",
      "2211-13826 2211-13826v1 ['rowhamLS.tex']\n",
      "2211-13828 2211-13828v1 ['sddir.tex']\n",
      "2211-13829 2211-13829v1 ['l4dc2023-sample.tex']\n",
      "2211-13829 2211-13829v2 ['final.tex']\n",
      "2211-13830 2211-13830v1 ['main.tex']\n",
      "2211-13831 2211-13831v1 ['dSJT2022.tex']\n",
      "2211-13832 2211-13832v1 ['main.tex', 'supmat.tex']\n",
      "2211-13832 2211-13832v2 ['main.tex', 'supmat.tex']\n",
      "2211-13833 2211-13833v1 ['main.tex']\n",
      "2211-13834 2211-13834v1 ['kappa.tex']\n",
      "2211-13834 2211-13834v2 ['kappa.tex']\n",
      "2211-13835 2211-13835v1 ['Sota_Usuda_FRB20201124A_20221124_arxiv.tex']\n",
      "2211-13836 2211-13836v1 ['main.tex']\n",
      "2211-13837 2211-13837v1 ['main.tex']\n",
      "2211-13838 2211-13838v1 ['main.tex', 'macros.tex', '0_abstract.tex', '1_introduction.tex', '2_background.tex', '3_limitations_motivation.tex', '4_quantization.tex', '5_experiments.tex', '6_efficiency.tex', '7_discussion.tex', '8_conclusion.tex', 'supp.tex']\n",
      "2211-13838 2211-13838v2 ['main.tex', 'macros.tex', '0_abstract.tex', '1_introduction.tex', '2_background.tex', '3_limitations_motivation.tex', '4_quantization.tex', '5_experiments.tex', '6_efficiency.tex', '7_discussion.tex', '8_conclusion.tex', '9_appendix.tex']\n",
      "2211-13839 2211-13839v1 ['bivariate_logsym_arxiv.tex']\n",
      "2211-13839 2211-13839v2 ['Bivariate-LogSym.tex']\n",
      "2211-13840 2211-13840v1 ['manuscript.arxiv.tex']\n",
      "2211-13841 2211-13841v1 ['w51A.tex']\n",
      "2211-13842 2211-13842v1 ['main.tex']\n",
      "2211-13843 2211-13843v1 ['Robosoft.tex']\n",
      "2211-13843 2211-13843v2 ['Robosoft.tex']\n",
      "2211-13844 2211-13844v1 ['main.tex']\n",
      "2211-13845 2211-13845v1 ['main.tex']\n",
      "2211-13846 2211-13846v1 ['root.tex']\n",
      "2211-13846 2211-13846v2 ['root.tex']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pub_results = []\n",
    "\n",
    "ready_items = [\n",
    "    (pub_id, info)\n",
    "    for pub_id, info in scan_result.items()\n",
    "    if info[\"status\"] == \"READY\"\n",
    "]\n",
    "\n",
    "for pub_id, info in tqdm(ready_items, desc=\"Publications\"):\n",
    "    for version in info[\"versions\"]:\n",
    "        version_path = f\"{RAW_ROOT}/{pub_id}/tex/{version}\"\n",
    "\n",
    "        result = resolve_version(\n",
    "            publication_id=pub_id,\n",
    "            version_name=version,\n",
    "            version_path=version_path\n",
    "        )\n",
    "\n",
    "        pub_results.append(result)\n",
    "\n",
    "# Xem kết quả\n",
    "for r in pub_results:\n",
    "    print(r[\"publication_id\"], r[\"version\"], r[\"used_tex_files\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0e8b68",
   "metadata": {},
   "source": [
    "## **LaTeX Hierarchy Parser**\n",
    "\n",
    "We parsed each LaTeX version into a hierarchical tree structure. Sectioning commands were treated as internal nodes, while only sentences, tables, and figures were considered leaf nodes, strictly following the seminar constraints. No deduplication or identifier assignment was performed at this stage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6ce533b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tree_to_cache(pub_id: str, version: str, root: FileNode) -> str:\n",
    "    \"\"\"\n",
    "    Serialize tree to disk and return cache path.\n",
    "    Returns the cache file path for later retrieval.\n",
    "    \"\"\"\n",
    "    cache_path = os.path.join(CACHE_DIR, f\"{pub_id}_{version}.pkl\")\n",
    "    with open(cache_path, \"wb\") as f:\n",
    "        pickle.dump(root, f, protocol=4)\n",
    "    return cache_path\n",
    "\n",
    "def load_tree_from_cache(cache_path: str) -> FileNode:\n",
    "    \"\"\"Load tree from cache file.\"\"\"\n",
    "    with open(cache_path, \"rb\") as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "257bdcc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing LaTeX versions: 100%|██████████| 140/140 [01:28<00:00,  1.58version(s)/s]\n"
     ]
    }
   ],
   "source": [
    "parsed_versions = []\n",
    "\n",
    "\n",
    "for version_info in tqdm(\n",
    "    pub_results,\n",
    "    desc=\"Parsing LaTeX versions\",\n",
    "    unit=\"version(s)\"\n",
    "):\n",
    "    if version_info[\"status\"] != \"RESOLVED\":\n",
    "        continue\n",
    "\n",
    "    version_path = (\n",
    "        f\"{RAW_ROOT}/\"\n",
    "        f\"{version_info['publication_id']}/tex/\"\n",
    "        f\"{version_info['version']}\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        root_node = parse_tex_files(\n",
    "            version_path=version_path,\n",
    "            tex_files=version_info[\"used_tex_files\"]\n",
    "        )\n",
    "\n",
    "        # Save to disk instead of keeping in memory\n",
    "        cache_path = save_tree_to_cache(\n",
    "            pub_id=version_info[\"publication_id\"],\n",
    "            version=version_info[\"version\"],\n",
    "            root=root_node\n",
    "        )\n",
    "\n",
    "        parsed_versions.append({\n",
    "            \"publication_id\": version_info[\"publication_id\"],\n",
    "            \"version\": version_info[\"version\"],\n",
    "            \"cache_path\": cache_path,  # Store path, not tree\n",
    "            \"root\": None  # Placeholder\n",
    "        })\n",
    "\n",
    "        # Explicitly free memory\n",
    "        del root_node\n",
    "        gc.collect()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"[ERROR] {version_info['publication_id']} \"\n",
    "            f\"{version_info['version']}: {e}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f27abdf",
   "metadata": {},
   "source": [
    "## **Deduplication & ID Assignment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "af7970a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deduplicating: 100%|██████████| 94/94 [00:14<00:00,  6.69publication(s)/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: merged 94 publications\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 3 - Deduplication & Merge trees per publication\n",
    "\n",
    "Input:\n",
    "    parsed_versions: list of {\n",
    "        publication_id,\n",
    "        version,\n",
    "        root (hierarchy tree)\n",
    "    }\n",
    "\n",
    "Output:\n",
    "    final_trees: dict[publication_id] -> merged root tree\n",
    "\"\"\"\n",
    "\n",
    "# Group trees by publication_id\n",
    "pub_groups = defaultdict(list)\n",
    "for item in parsed_versions:\n",
    "    pub_groups[item[\"publication_id\"]].append(item)\n",
    "\n",
    "final_trees = {}\n",
    "\n",
    "for pub_id, versions in tqdm(pub_groups.items(), desc=\"Deduplicating\", unit=\"publication(s)\"):\n",
    "    if not versions:\n",
    "        continue\n",
    "    \n",
    "    final_trees[pub_id] = {}\n",
    "    # Sort versions\n",
    "    versions.sort(key=lambda x: int(x[\"version\"].split(\"v\")[-1]) if \"v\" in x[\"version\"] else 0)\n",
    "\n",
    "    # Load Base Tree\n",
    "    base_info = versions[0]\n",
    "    with open(base_info[\"cache_path\"], \"rb\") as f:\n",
    "        base_root = pickle.load(f)\n",
    "\n",
    "    # Normalize Base\n",
    "    fast_normalize_and_id(base_root, base_info[\"version\"])\n",
    "    \n",
    "    if len(versions) > 1:\n",
    "        content_index = {}\n",
    "        deduplicate_tree(base_root, content_index)\n",
    "        final_trees[pub_id][base_info['version']] = base_root\n",
    "        \n",
    "        for v_info in versions[1:]:\n",
    "            # Normalize Base\n",
    "            with open(v_info[\"cache_path\"], \"rb\") as f:\n",
    "                root = pickle.load(f)\n",
    "            fast_normalize_and_id(root, base_info[\"version\"])\n",
    "            deduplicate_tree(root, content_index)\n",
    "\n",
    "            final_trees[pub_id][v_info['version']] = base_root\n",
    "    \n",
    "    else:\n",
    "        final_trees[pub_id][base_info['version']] = base_root\n",
    "    \n",
    "    # Periodic GC to prevent memory fragmentation on large datasets\n",
    "    if len(final_trees) % 50 == 0:\n",
    "        gc.collect()\n",
    "\n",
    "print(f\"DONE: merged {len(final_trees)} publications\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c3d4e584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample publication: 2211-13748\n",
      "Root node type: document\n",
      "Number of children: 16\n"
     ]
    }
   ],
   "source": [
    "# Example: Inspect one of the final trees\n",
    "sample_pub = next(iter(final_trees))\n",
    "sample_tree = next(iter(final_trees[sample_pub]))\n",
    "root = final_trees[sample_pub][sample_tree]\n",
    "\n",
    "print(\"Sample publication:\", sample_pub)\n",
    "print(\"Root node type:\", root.node_type)\n",
    "print(\"Number of children:\", len(root.children))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0581f8",
   "metadata": {},
   "source": [
    "## **Export to JSON**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b676ef5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exporting JSON: 100%|██████████| 94/94 [00:04<00:00, 19.63pub/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported 94 publications to: 23127453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Export Structured Publication JSON\n",
    "\n",
    "Input:\n",
    "    final_trees: dict[publication_id] -> root_node\n",
    "\n",
    "Output:\n",
    "    Student ID/\n",
    "        <pub_id>/\n",
    "            <pub_id>.json\n",
    "            metadata.json\n",
    "            references.json\n",
    "        ...\n",
    "\"\"\"\n",
    "\n",
    "# Export statistics\n",
    "export_count = 0\n",
    "missing_metadata = []\n",
    "missing_references = []\n",
    "\n",
    "for pub_id, root in tqdm(final_trees.items(), desc=\"Exporting JSON\", unit=\"pub\"):\n",
    "    \n",
    "    # Create publication subdirectory\n",
    "    pub_output_dir = os.path.join(OUTPUT_DIR, pub_id)\n",
    "    os.makedirs(pub_output_dir, exist_ok=True)\n",
    "    \n",
    "    # Export Content Tree (Parsed Hierarchy)\n",
    "    content_json = {\n",
    "        \"publication_id\": pub_id,\n",
    "        \"content_tree\": serialize_node(root)\n",
    "    }\n",
    "    \n",
    "    content_path = os.path.join(pub_output_dir, f\"{pub_id}.json\")\n",
    "    with open(content_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(content_json, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # Copy Metadata (Original from Semantic Scholar)\n",
    "    raw_metadata_path = os.path.join(RAW_ROOT, pub_id, \"metadata.json\")\n",
    "    metadata_path = os.path.join(pub_output_dir, \"metadata.json\")\n",
    "    \n",
    "    if os.path.exists(raw_metadata_path):\n",
    "        with open(raw_metadata_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            metadata = json.load(f)\n",
    "        with open(metadata_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "    else:\n",
    "        missing_metadata.append(pub_id)\n",
    "    \n",
    "    # Copy References (Original from Semantic Scholar)\n",
    "    raw_references_path = os.path.join(RAW_ROOT, pub_id, \"references.json\")\n",
    "    references_path = os.path.join(pub_output_dir, \"references.json\")\n",
    "    \n",
    "    if os.path.exists(raw_references_path):\n",
    "        with open(raw_references_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            references = json.load(f)\n",
    "        with open(references_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(references, f, ensure_ascii=False, indent=2)\n",
    "    else:\n",
    "        missing_references.append(pub_id)\n",
    "    \n",
    "    export_count += 1\n",
    "\n",
    "print(f\"Exported {export_count} publications to: {OUTPUT_DIR}\")\n",
    "\n",
    "if missing_metadata:\n",
    "    print(f\"\\nWARNING: {len(missing_metadata)} publication(s) missing metadata.json\")\n",
    "    print(f\"   {', '.join(missing_metadata[:5])}\" + (\" ...\" if len(missing_metadata) > 5 else \"\"))\n",
    "\n",
    "if missing_references:\n",
    "    print(f\"\\nWARNING: {len(missing_references)} publication(s) missing references.json\")\n",
    "    print(f\"   {', '.join(missing_references[:5])}\" + (\" ...\" if len(missing_references) > 5 else \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc87624e",
   "metadata": {},
   "source": [
    "## **Extract References**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c35e35",
   "metadata": {},
   "source": [
    "### Goal\n",
    "\n",
    "Batch process publications to **extract**, **deduplicate**, and **export** references into standardized `.bib` files while maintaining a global key mapping.\n",
    "\n",
    "### Workflow\n",
    "\n",
    "1. **Iterate through pub_results**\n",
    "* Skip entries where `status == \"NO_TEX\"` or the directory is missing.\n",
    "* Identify target files (`.bib`, `.bbl`, `.tex`) in the version path.\n",
    "\n",
    "\n",
    "2. **Extract & Deduplicate**\n",
    "* Execute `extract_references_from_tex_files()` to gather `local_raw_refs`.\n",
    "* Apply `deduplicate_references_with_mapping()` to produce `local_dedup_list` and `local_key_map`.\n",
    "\n",
    "\n",
    "3. **ID Assignment & Export**\n",
    "* Assign a unique `ref_id` to each entry via `generate_semantic_id()`.\n",
    "* Write formatted entries to `OUTPUT_DIR/<pub_id>/refs.bib` using `export_to_bibtex()`.\n",
    "\n",
    "\n",
    "4. **State Management & Cleanup**\n",
    "* Store results in `deduplicated_references` and update `global_key_mapping`.\n",
    "* Track statistics (`total_raw_found`, `count_exported`, `extract_errors`).\n",
    "* Trigger `gc.collect()` to optimize memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "74254e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Pipeline: 100%|██████████| 140/140 [00:18<00:00,  7.60pub/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pipeline Complete!\n",
      "  - Total Raw Refs Processed: 73939\n",
      "  - Publications with Refs: 82\n",
      "  - Exported .bib files: 120\n",
      "  - Extraction Errors: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "deduplicated_references = {}\n",
    "global_key_mapping = {} \n",
    "global_used_ids = set() \n",
    "\n",
    "extract_errors = 0\n",
    "count_exported = 0\n",
    "total_raw_found = 0\n",
    "processed_count = 0\n",
    "\n",
    "for version_info in tqdm(pub_results, desc=\"Processing Pipeline\", unit=\"pub\"):\n",
    "    \n",
    "    if version_info.get(\"status\") == \"NO_TEX\":\n",
    "        continue\n",
    "\n",
    "    pub_id = version_info[\"publication_id\"]\n",
    "    version = version_info[\"version\"]\n",
    "    version_path = f\"{RAW_ROOT}/{pub_id}/tex/{version}\"\n",
    "    \n",
    "    if not os.path.exists(version_path):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        all_files_in_dir = os.listdir(version_path)\n",
    "        target_files = [\n",
    "            f for f in all_files_in_dir \n",
    "            if f.lower().endswith(('.bib', '.bbl', '.tex'))\n",
    "        ]\n",
    "        \n",
    "        local_raw_refs = extract_references_from_tex_files(\n",
    "            version_path=version_path,\n",
    "            tex_files=target_files \n",
    "        )\n",
    "        \n",
    "        if not local_raw_refs:\n",
    "            continue\n",
    "            \n",
    "        total_raw_found += len(local_raw_refs)\n",
    "\n",
    "        local_dedup_list, local_key_map = deduplicate_references_with_mapping(local_raw_refs)\n",
    "        \n",
    "        save_dir = os.path.join(OUTPUT_DIR, pub_id)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        output_path = os.path.join(save_dir, \"refs.bib\")\n",
    "        \n",
    "        try:\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                for ref in local_dedup_list:\n",
    "                    ref['ref_id'] = generate_semantic_id(ref, global_used_ids)\n",
    "                    \n",
    "                    bib_string = export_to_bibtex(ref)\n",
    "                    f.write(bib_string)\n",
    "            \n",
    "            count_exported += 1\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Export failed for {pub_id}: {e}\")\n",
    "\n",
    "        deduplicated_references[pub_id] = local_dedup_list\n",
    "        global_key_mapping.update(local_key_map)\n",
    "        \n",
    "    except Exception as e:\n",
    "        extract_errors += 1\n",
    "\n",
    "    if 'local_raw_refs' in locals():\n",
    "        del local_raw_refs\n",
    "    \n",
    "    processed_count += 1\n",
    "    if processed_count % 50 == 0:\n",
    "        gc.collect()\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\nPipeline Complete!\")\n",
    "print(f\"  - Total Raw Refs Processed: {total_raw_found}\")\n",
    "print(f\"  - Publications with Refs: {len(deduplicated_references)}\")\n",
    "print(f\"  - Exported .bib files: {count_exported}\")\n",
    "print(f\"  - Extraction Errors: {extract_errors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58f4370",
   "metadata": {},
   "source": [
    "## **Reference Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba7d696",
   "metadata": {},
   "source": [
    "### Goal\n",
    "Build two cleaned datasets per publication:\n",
    "- **`cleaned_bibtex`**: cleaned versions of deduplicated BibTeX references (while preserving key identifiers)\n",
    "- **`cleaned_arxiv`**: cleaned arXiv reference metadata loaded from `references.json`\n",
    "\n",
    "### Workflow\n",
    "1. **Clean deduplicated BibTeX references**\n",
    "   - For each `pub_id` in `deduplicated_references`:\n",
    "     - Loop through its references and run `clean_bibtex_entry(ref)`\n",
    "     - Preserve important identifiers from the original reference:\n",
    "       - `ref_id`, `key`, `all_keys`\n",
    "     - Collect successfully cleaned entries into `cleaned_bibtex[pub_id]`\n",
    "     - Skip any reference that raises an exception (silent continue)\n",
    "\n",
    "2. **Load and clean arXiv truth references (`references.json`)**\n",
    "   - For each `pub_id` in `cleaned_bibtex`:\n",
    "     - Look for `RAW_ROOT/<pub_id>/references.json`\n",
    "     - If present:\n",
    "       - Load the JSON dict `{arxiv_id: meta}`\n",
    "       - Inject `meta[\"arxiv_id\"] = arxiv_id`\n",
    "       - Clean each record with `clean_arxiv_reference(meta)`\n",
    "       - Store results in `cleaned_arxiv[pub_id]`\n",
    "     - If missing: set `cleaned_arxiv[pub_id] = []`\n",
    "     - If loading/cleaning fails: warn and fall back to an empty list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9731aac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning BibTeX: 100%|██████████| 82/82 [00:17<00:00,  4.70pub/s]\n",
      "Loading & Cleaning arXiv Truth: 100%|██████████| 82/82 [00:00<00:00, 126.58pub/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DONE: 66534 cleaned BibTeX entries vs 1544 cleaned arXiv entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned_bibtex = {}\n",
    "\n",
    "for pub_id in tqdm(deduplicated_references.keys(), desc=\"Cleaning BibTeX\", unit=\"pub\"):\n",
    "    cleaned_entries = []\n",
    "    for ref in deduplicated_references[pub_id]:\n",
    "        try:\n",
    "            cleaned_ref = clean_bibtex_entry(ref)\n",
    "            cleaned_ref['ref_id'] = ref.get('ref_id', '')\n",
    "            cleaned_ref['key'] = ref.get('key', '')\n",
    "            cleaned_ref['all_keys'] = ref.get('all_keys', [])\n",
    "            \n",
    "            cleaned_entries.append(cleaned_ref)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "            \n",
    "    cleaned_bibtex[pub_id] = cleaned_entries\n",
    "\n",
    "cleaned_arxiv = {}\n",
    "\n",
    "for pub_id in tqdm(cleaned_bibtex.keys(), desc=\"Loading & Cleaning arXiv Truth\", unit=\"pub\"):\n",
    "    ref_json_path = os.path.join(RAW_ROOT, pub_id, \"references.json\")\n",
    "    \n",
    "    if os.path.exists(ref_json_path):\n",
    "        try:\n",
    "            with open(ref_json_path, 'r', encoding='utf-8') as f:\n",
    "                arxiv_raw = json.load(f)\n",
    "            \n",
    "            cleaned_entries = []\n",
    "            for arxiv_id, meta in arxiv_raw.items():\n",
    "                meta['arxiv_id'] = arxiv_id \n",
    "                cleaned_ref = clean_arxiv_reference(meta)\n",
    "                cleaned_entries.append(cleaned_ref)\n",
    "            \n",
    "            cleaned_arxiv[pub_id] = cleaned_entries\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Error loading references.json for {pub_id}: {e}\")\n",
    "            cleaned_arxiv[pub_id] = []\n",
    "    else:\n",
    "        cleaned_arxiv[pub_id] = []\n",
    "\n",
    "# Summary\n",
    "total_bib = sum(len(x) for x in cleaned_bibtex.values())\n",
    "total_arxiv = sum(len(x) for x in cleaned_arxiv.values())\n",
    "print(f\"\\nDONE: {total_bib} cleaned BibTeX entries vs {total_arxiv} cleaned arXiv entries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c3c237",
   "metadata": {},
   "source": [
    "## **Labeling & Dataset Construction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6847920a",
   "metadata": {},
   "source": [
    "### **Manual Labeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1d4b4a",
   "metadata": {},
   "source": [
    "#### **Build Manual Pairs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1302d63b",
   "metadata": {},
   "source": [
    "##### Goal\n",
    "Create **manual labeling datasets** (JSON files) for a small set of publications by generating **BibTeX ↔ arXiv candidate pairs** with precomputed match scores—unless enough JSON files already exist.\n",
    "\n",
    "##### Workflow\n",
    "1. **Check whether manual-labeling files already exist**\n",
    "   - If `MANUAL_DIR` exists, count `*.json` files inside it.\n",
    "   - If there are **≥ 5 files**, reuse them (no regeneration).\n",
    "   - If fewer than 5, mark `generated = False` to regenerate.\n",
    "\n",
    "2. **Select candidate publications to label (only when regenerating)**\n",
    "   - Loop through `cleaned_bibtex`:\n",
    "     - Get corresponding `cleaned_arxiv[pub_id]`.\n",
    "     - Skip papers where either side has **< 20 entries**.\n",
    "   - For each BibTeX entry, call `find_best_match(bib, arxiv_entries, threshold=0.7)` and count how many matches exist.\n",
    "   - Keep only publications with **≥ 20 potential matches** and store stats (`bib_count`, `arxiv_count`, `potential_matches`).\n",
    "\n",
    "3. **Pick the final papers to generate tasks for**\n",
    "   - Sort candidate publications by `potential_matches` descending.\n",
    "   - Select **5 papers** using `candidate_papers[1:6]` (i.e., skipping the top 1 and taking the next 5).\n",
    "   - Warn if fewer than 5 papers meet the criteria.\n",
    "\n",
    "4. **Generate labeling tasks (pair candidates + scores)**\n",
    "   - For each selected `pub_id`:\n",
    "     - For every BibTeX entry, compute a match score against **every** arXiv entry using `compute_match_score(bib, arxiv)`.\n",
    "     - Build a pair record containing:\n",
    "       - `label` (empty, to be filled manually)\n",
    "       - `score` (rounded)\n",
    "       - BibTeX metadata (key/title/authors/year)\n",
    "       - arXiv metadata (id/title/authors/year)\n",
    "     - Sort candidates by `score` (descending) and add them to a global `tasks` list for that publication.\n",
    "\n",
    "5. **Export to JSON**\n",
    "   - Save all generated pairs to `MANUAL_DIR/<pub_id>.json`.\n",
    "   - Print the file path and total number of pairs created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "97201e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 file JSON in folder 'manual_labeling'\n"
     ]
    }
   ],
   "source": [
    "generated = True\n",
    "\n",
    "if os.path.exists(MANUAL_DIR):\n",
    "    existing_files = glob.glob(os.path.join(MANUAL_DIR, \"*.json\"))\n",
    "    \n",
    "    if len(existing_files) >= 5:\n",
    "        print(f\"Found {len(existing_files)} file JSON in folder '{MANUAL_DIR}'\")\n",
    "    else:\n",
    "        print(f\"Folder exists but only has {len(existing_files)} files. Will regenerate\")\n",
    "        generated = False\n",
    "\n",
    "if not generated:\n",
    "    \n",
    "    candidate_papers = []\n",
    "\n",
    "    for pub_id, bib_entries in tqdm(cleaned_bibtex.items(), desc=\"Filtering Papers\"):\n",
    "        arxiv_entries = cleaned_arxiv.get(pub_id, [])\n",
    "        \n",
    "        if len(bib_entries) < 20 or len(arxiv_entries) < 20:\n",
    "            continue\n",
    "\n",
    "        potential_matches = 0\n",
    "        for bib in bib_entries:\n",
    "            match = find_best_match(bib, arxiv_entries, threshold=0.7) \n",
    "            if match:\n",
    "                potential_matches += 1\n",
    "            \n",
    "        if potential_matches >= 20:\n",
    "            candidate_papers.append({\n",
    "                'pub_id': pub_id,\n",
    "                'bib_count': len(bib_entries),\n",
    "                'arxiv_count': len(arxiv_entries),\n",
    "                'potential_matches': potential_matches\n",
    "            })\n",
    "\n",
    "    candidate_papers.sort(key=lambda x: x['potential_matches'], reverse=True)\n",
    "    selected_papers = candidate_papers[1:6]\n",
    "\n",
    "    if len(selected_papers) < 5:\n",
    "        print(f\"Only found {len(selected_papers)} papers meeting the criteria\")\n",
    "\n",
    "    os.makedirs(MANUAL_DIR, exist_ok=True)\n",
    "\n",
    "    for paper in selected_papers:\n",
    "        pub_id = paper['pub_id']\n",
    "        bib_pool = cleaned_bibtex[pub_id]\n",
    "        arxiv_pool = cleaned_arxiv[pub_id]\n",
    "        \n",
    "        tasks = []\n",
    "        \n",
    "        for bib in bib_pool:\n",
    "            \n",
    "            current_bib_candidates = []\n",
    "            \n",
    "            for arxiv in arxiv_pool:\n",
    "                \n",
    "                score = compute_match_score(bib, arxiv)\n",
    "                \n",
    "                pair_data = {\n",
    "                    \"label\": '', \n",
    "                    \"score\": round(score, 4),\n",
    "                    \n",
    "                    # Info BibTeX (Source)\n",
    "                    \"bib_key\": bib['key'],\n",
    "                    \"bib_title\": bib.get('normalized_title', ''),\n",
    "                    \"bib_author\": \", \".join(bib.get('normalized_authors', [])),\n",
    "                    \"bib_year\": bib.get('normalized_year', ''),\n",
    "                    \n",
    "                    # Info ArXiv (Target)\n",
    "                    \"arxiv_id\": arxiv['arxiv_id'],\n",
    "                    \"arxiv_title\": arxiv.get('normalized_title', ''),\n",
    "                    \"arxiv_author\": \", \".join(arxiv.get('normalized_authors', [])),\n",
    "                    \"arxiv_year\": arxiv.get('normalized_year', '')\n",
    "                }\n",
    "                \n",
    "                current_bib_candidates.append(pair_data)\n",
    "            \n",
    "            current_bib_candidates.sort(key=lambda x: x['score'], reverse=True)\n",
    "            \n",
    "            tasks.extend(current_bib_candidates)\n",
    "\n",
    "        file_path = os.path.join(MANUAL_DIR, f\"{pub_id}.json\")\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(tasks, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "        print(f\"  -> Created: {file_path} (Total pairs: {len(tasks)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81215cd",
   "metadata": {},
   "source": [
    "#### **Read Manual Pairs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f326c336",
   "metadata": {},
   "source": [
    "##### Goal\n",
    "Load human-labeled JSON files from `manual_labeling/` and convert them into a structured dataset of **(BibTeX, arXiv-candidate) pairs** with **binary labels**:\n",
    "- `label = 1` for the manually verified match\n",
    "- `label = 0` for all other arXiv candidates (negatives)\n",
    "\n",
    "##### Workflow\n",
    "1. **Load manual labeling JSON files**\n",
    "   - Collect all `manual_labeling/*.json`.\n",
    "   - If none are found, raise an error.\n",
    "   - For each file, infer `pub_id` from the filename and load `labeled_data`.\n",
    "\n",
    "2. **Build lookup pools for the publication**\n",
    "   - Retrieve `bib_pool` from `cleaned_bibtex[pub_id]` and `arxiv_pool` from `cleaned_arxiv[pub_id]`.\n",
    "   - Build fast lookup dicts:\n",
    "     - `bib_dict = {bib_key → bib_entry}`\n",
    "     - `arxiv_dict = {arxiv_id → arxiv_entry}`\n",
    "\n",
    "3. **Extract positive matches**\n",
    "   - Scan `labeled_data` and keep only items with `label == 1`.\n",
    "   - Store valid matches in `positive_matches` as `{bib_key: matched_arxiv_id}` (only if both IDs exist in current pools).\n",
    "\n",
    "4. **Expand each positive match into one-vs-all labeled pairs**\n",
    "   - For each `(bib_key → matched_arxiv_id)`:\n",
    "     - Create a shared `base` row with cleaned BibTeX metadata (title/authors/year/ref_id/all_keys).\n",
    "     - Loop through **every** `arxiv_entry` in `arxiv_pool` and create a pair:\n",
    "       - If `arxiv_id == matched_arxiv_id` → `label = 1`, `pair_type = \"manual_verified\"`\n",
    "       - Else → `label = 0`, `pair_type = \"manual_negative\"`\n",
    "     - Append each pair row into `manual_pairs`.\n",
    "   - Print per-publication stats: number of verified bib entries, and total pos/neg pairs generated.\n",
    "\n",
    "5. **Initialize a helper structure**\n",
    "   - Create `FIXED_MANUAL_DATA` as a dict keyed by each `pub_id` from the JSON filenames (empty nested dicts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "89aac0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2211-13755: 33 bib entries → 33 pos, 1584 neg pairs\n",
      "Loaded 2211-13757: 37 bib entries → 37 pos, 2035 neg pairs\n",
      "Loaded 2211-13760: 22 bib entries → 22 pos, 1122 neg pairs\n",
      "Loaded 2211-13766: 21 bib entries → 21 pos, 1008 neg pairs\n",
      "Loaded 2211-13767: 23 bib entries → 23 pos, 1150 neg pairs\n",
      "\n",
      "Total Manual Pairs Loaded: 7035\n",
      "Positive Labels (1): 136\n",
      "Negative Labels (0): 6899\n",
      "Ratio (pos:neg): 1:50.7\n"
     ]
    }
   ],
   "source": [
    "manual_pairs = []\n",
    "json_files = glob.glob(\"manual_labeling/*.json\")\n",
    "\n",
    "if not json_files:\n",
    "    raise FileNotFoundError(\"Not found any JSON files in folder 'manual_labeling'\")\n",
    "\n",
    "for file_path in json_files:\n",
    "    filename = os.path.basename(file_path)\n",
    "    pub_id = filename.replace(\".json\", \"\")\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        labeled_data = json.load(f)\n",
    "    \n",
    "    bib_pool = cleaned_bibtex.get(pub_id, [])\n",
    "    arxiv_pool = cleaned_arxiv.get(pub_id, [])\n",
    "    \n",
    "    bib_dict = {b['key']: b for b in bib_pool}\n",
    "    arxiv_dict = {a['arxiv_id']: a for a in arxiv_pool}\n",
    "    \n",
    "    positive_matches = {}  # {bib_key: arxiv_id}\n",
    "    \n",
    "    for item in labeled_data:\n",
    "        if item['label'] == 1:\n",
    "            bib_key = item['bib_key']\n",
    "            arxiv_id = item['arxiv_id']\n",
    "            \n",
    "            if bib_key in bib_dict and arxiv_id in arxiv_dict:\n",
    "                positive_matches[bib_key] = arxiv_id\n",
    "    \n",
    "    count_pos = 0\n",
    "    count_neg = 0\n",
    "    \n",
    "    for bib_key, matched_arxiv_id in positive_matches.items():\n",
    "        bib_entry = bib_dict[bib_key]\n",
    "        \n",
    "        base = {\n",
    "            'pub_id': pub_id,\n",
    "            'bib_key': bib_key,\n",
    "            'bib_ref_id': bib_entry.get('ref_id'),\n",
    "            'all_keys': bib_entry.get('all_keys', []),\n",
    "            'bib_title_clean': bib_entry.get('normalized_title'),\n",
    "            'bib_authors_clean': \", \".join(bib_entry.get('normalized_authors', [])),\n",
    "            'bib_author_tokens': str(bib_entry.get('author_tokens', [])),\n",
    "            'bib_year': bib_entry.get('normalized_year'),\n",
    "            'source': 'manual',\n",
    "        }\n",
    "        \n",
    "        for arxiv_entry in arxiv_pool:\n",
    "            arxiv_id = arxiv_entry['arxiv_id']\n",
    "            \n",
    "            # Xác định label\n",
    "            if arxiv_id == matched_arxiv_id:\n",
    "                label = 1\n",
    "                pair_type = 'manual_verified'\n",
    "                count_pos += 1\n",
    "            else:\n",
    "                label = 0\n",
    "                pair_type = 'manual_negative'\n",
    "                count_neg += 1\n",
    "            \n",
    "            row = base.copy()\n",
    "            row.update({\n",
    "                'candidate_arxiv_id': arxiv_id,\n",
    "                'candidate_title_clean': arxiv_entry.get('normalized_title'),\n",
    "                'candidate_authors_clean': \", \".join(arxiv_entry.get('normalized_authors', [])),\n",
    "                'candidate_author_tokens': str(arxiv_entry.get('author_tokens', [])),\n",
    "                'candidate_year': arxiv_entry.get('normalized_year'),\n",
    "                'pair_type': pair_type,\n",
    "                'label': label\n",
    "            })\n",
    "            \n",
    "            manual_pairs.append(row)\n",
    "    \n",
    "    print(f\"Loaded {pub_id}: {len(positive_matches)} bib entries → {count_pos} pos, {count_neg} neg pairs\")\n",
    "\n",
    "FIXED_MANUAL_DATA = {pid.replace(\".json\", \"\"): {} for pid in [os.path.basename(f) for f in json_files]}\n",
    "\n",
    "print(f\"\\nTotal Manual Pairs Loaded: {len(manual_pairs)}\")\n",
    "print(f\"Positive Labels (1): {len([p for p in manual_pairs if p['label'] == 1])}\")\n",
    "print(f\"Negative Labels (0): {len([p for p in manual_pairs if p['label'] == 0])}\")\n",
    "print(f\"Ratio (pos:neg): 1:{len([p for p in manual_pairs if p['label'] == 0]) / max(1, len([p for p in manual_pairs if p['label'] == 1])):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fa24ce",
   "metadata": {},
   "source": [
    "### **Automatic Labeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c93c9ae",
   "metadata": {},
   "source": [
    "#### Goal\n",
    "\n",
    "Identify and select a **random 10% sample** of valid publications (excluding manually labeled ones) to generate **automatic labels** based on high-confidence matches.\n",
    "\n",
    "#### Workflow\n",
    "\n",
    "1. **Define Selection Criteria**\n",
    "* Set matching confidence threshold: `POSITIVE_THRESHOLD = 0.85`.\n",
    "* Set sampling target: `TARGET_PERCENTAGE = 10%`.\n",
    "\n",
    "\n",
    "2. **Filter & Scan Candidates**\n",
    "* Exclude publications already in `FIXED_MANUAL_DATA`.\n",
    "* Perform a **Lightweight Scan**: Iterate through candidates and validate if they contain *at least one* reference match exceeding the threshold using `find_best_match()`.\n",
    "\n",
    "\n",
    "3. **Random Sampling**\n",
    "* Calculate the required `target_count` based on the valid population.\n",
    "* Use `random.sample()` to pick `selected_pub_ids` for the automatic dataset generation phase.\n",
    "\n",
    "\n",
    "4. **Cleanup**\n",
    "* Trigger `gc.collect()` to free memory after the scanning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a27e4971",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightweight Scan: 100%|██████████| 95/95 [00:03<00:00, 27.10it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Selected 10 publications (Target 10%).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "POSITIVE_THRESHOLD = 0.8\n",
    "TARGET_PERCENTAGE = 0.10  # 10%\n",
    "\n",
    "auto_candidate_ids = [p for p in scan_result.keys() if p not in FIXED_MANUAL_DATA]\n",
    "\n",
    "valid_auto_pub_ids = []\n",
    "\n",
    "for pub_id in tqdm(auto_candidate_ids, desc=\"Lightweight Scan\"):\n",
    "    bib_pool = cleaned_bibtex.get(pub_id, [])\n",
    "    arxiv_pool = cleaned_arxiv.get(pub_id, [])\n",
    "    \n",
    "    if not bib_pool or not arxiv_pool:\n",
    "        continue\n",
    "        \n",
    "    is_valid = False\n",
    "    for bib in bib_pool:\n",
    "        match = find_best_match(bib, arxiv_pool, threshold=POSITIVE_THRESHOLD)\n",
    "        if match:\n",
    "            is_valid = True\n",
    "            break \n",
    "    \n",
    "    if is_valid:\n",
    "        valid_auto_pub_ids.append(pub_id)\n",
    "\n",
    "\n",
    "target_count = math.ceil(len(auto_candidate_ids) * TARGET_PERCENTAGE)\n",
    "selected_pub_ids = random.sample(valid_auto_pub_ids, min(len(valid_auto_pub_ids), target_count))\n",
    "\n",
    "print(f\"-> Selected {len(selected_pub_ids)} publications (Target {int(TARGET_PERCENTAGE*100)}%).\")\n",
    "\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a2e784",
   "metadata": {},
   "source": [
    "### **Export CSV**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6343345b",
   "metadata": {},
   "source": [
    "#### Goal\n",
    "\n",
    "**Construct and export the final labeled dataset** to CSV by combining manually verified pairs with automatically generated high-confidence matches.\n",
    "\n",
    "#### Workflow\n",
    "\n",
    "1. **Initialize CSV Output**\n",
    "* Define standard schema (`fieldnames`) covering metadata, tokens, and labels.\n",
    "* Create `OUTPUT_CSV` and write the header.\n",
    "\n",
    "\n",
    "2. **Write Manual Data**\n",
    "* If `manual_pairs` exists, write these verified records directly to the CSV.\n",
    "* Track `count_manual`.\n",
    "\n",
    "\n",
    "3. **Generate & Write Auto Data**\n",
    "* Iterate through `selected_pub_ids`.\n",
    "* For each BibTeX entry:\n",
    "* Identify the **best match** among candidates.\n",
    "* **Labeling Strategy:** Assign `label=1` to the best candidate and `label=0` to all other candidates (creating negative samples).\n",
    "\n",
    "\n",
    "* Write attributes (titles, authors, tokens, year) to CSV.\n",
    "\n",
    "\n",
    "4. **Finalization**\n",
    "* Periodically trigger `gc.collect()` to manage memory.\n",
    "* Print summary statistics (`Total`, `Manual`, `Auto`) upon completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c0bd2b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 7035 manual pairs...\n",
      "Processing 10 auto publications...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto Generation: 100%|██████████| 10/10 [00:01<00:00,  5.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DONE! File saved at: ../src/labeled_dataset.csv\n",
      "Total Rows: 29899\n",
      "  - Manual Pairs: 7035\n",
      "  - Auto Pairs:   22864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_CSV = \"../src/labeled_dataset.csv\"\n",
    "\n",
    "total_written = 0\n",
    "count_manual = 0\n",
    "count_auto = 0\n",
    "\n",
    "fieldnames = [\n",
    "    'pub_id', 'bib_key', 'bib_ref_id', 'all_keys', \n",
    "    'bib_title_clean', 'bib_authors_clean', 'bib_author_tokens', 'bib_year',\n",
    "    'candidate_arxiv_id', 'candidate_title_clean', 'candidate_authors_clean', \n",
    "    'candidate_author_tokens', 'candidate_year',\n",
    "    'source', 'pair_type', 'label'\n",
    "]\n",
    "\n",
    "with open(OUTPUT_CSV, mode='w', encoding='utf-8', newline='') as csv_file:\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    if 'manual_pairs' in locals() and manual_pairs:\n",
    "        print(f\"Writing {len(manual_pairs)} manual pairs...\")\n",
    "        for row in manual_pairs:\n",
    "            writer.writerow(row)\n",
    "            count_manual += 1\n",
    "    else:\n",
    "        print(\"[WARN] The variable 'manual_pairs' does not exist or is empty. Ignore the Manual section.\")\n",
    "\n",
    "    print(f\"Processing {len(selected_pub_ids)} auto publications...\")\n",
    "    \n",
    "    for pub_id in tqdm(selected_pub_ids, desc=\"Auto Generation\"):\n",
    "        bib_pool = cleaned_bibtex.get(pub_id, [])\n",
    "        arxiv_pool = cleaned_arxiv.get(pub_id, [])\n",
    "        \n",
    "        for bib in bib_pool:\n",
    "            \n",
    "            best_score = -1.0\n",
    "            best_arxiv_idx = -1\n",
    "            \n",
    "            for idx, arxiv in enumerate(arxiv_pool):\n",
    "                match = find_best_match(bib, [arxiv], threshold=0.0)\n",
    "                score = match[1] if match else 0.0\n",
    "                \n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_arxiv_idx = idx\n",
    "                \n",
    "            base_row = {\n",
    "                'pub_id': pub_id,\n",
    "                'bib_key': bib.get('key'),\n",
    "                'bib_ref_id': bib.get('ref_id'),\n",
    "                'all_keys': list(bib.get('all_keys', [])),\n",
    "                'bib_title_clean': bib.get('normalized_title'),\n",
    "                'bib_authors_clean': \", \".join(bib.get('normalized_authors', [])),\n",
    "                'bib_author_tokens': str(bib.get('author_tokens', [])),\n",
    "                'bib_year': bib.get('normalized_year'),\n",
    "                'source': 'automatic'\n",
    "            }\n",
    "            \n",
    "            for idx, arxiv in enumerate(arxiv_pool):\n",
    "                label = 1 if idx == best_arxiv_idx else 0\n",
    "                pair_type = 'positive_auto' if label == 1 else 'negative_auto'\n",
    "                \n",
    "                row = base_row.copy()\n",
    "                row.update({\n",
    "                    'candidate_arxiv_id': arxiv.get('arxiv_id'),\n",
    "                    'candidate_title_clean': arxiv.get('normalized_title'),\n",
    "                    'candidate_authors_clean': \", \".join(arxiv.get('normalized_authors', [])),\n",
    "                    'candidate_author_tokens': str(arxiv.get('author_tokens', [])),\n",
    "                    'candidate_year': arxiv.get('normalized_year'),\n",
    "                    'pair_type': pair_type,\n",
    "                    'label': label\n",
    "                })\n",
    "                \n",
    "                writer.writerow(row)\n",
    "                count_auto += 1\n",
    "        \n",
    "        if count_auto % 1000 == 0:\n",
    "            gc.collect()\n",
    "\n",
    "total_written = count_manual + count_auto\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"DONE! File saved at: {OUTPUT_CSV}\")\n",
    "print(f\"Total Rows: {total_written}\")\n",
    "print(f\"  - Manual Pairs: {count_manual}\")\n",
    "print(f\"  - Auto Pairs:   {count_auto}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c4f1bb",
   "metadata": {},
   "source": [
    "## **Feature Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6b43c8",
   "metadata": {},
   "source": [
    "### Goal\n",
    "Ensure the **labeled dataset** is available, then run **feature engineering** to transform raw pair data into model-ready features, and finally **save the featured dataset**.\n",
    "\n",
    "### Workflow\n",
    "1. **Load labeled dataset if not already in memory**\n",
    "   - If `final_df` is `None`, load from `../src/labeled_dataset.csv`.\n",
    "   - If the CSV does not exist, raise an error.\n",
    "\n",
    "2. **Apply feature engineering**\n",
    "   - Call `feature_engineering(final_df)` to compute matching-related features from the labeled pairs.\n",
    "   - Store the result in `df_features`.\n",
    "\n",
    "3. **Export the featured dataset**\n",
    "   - Save `df_features` to `../src/featured_dataset.csv` (no index column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "42c76a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_df is None, loading from CSV...\n"
     ]
    }
   ],
   "source": [
    "# Load labeled dataset if not already in memory\n",
    "print(\"final_df is None, loading from CSV...\")\n",
    "labeled_path = Path(\"../src/labeled_dataset.csv\")\n",
    "if not labeled_path.exists():\n",
    "    raise FileNotFoundError(f\"File not found: {labeled_path}\")\n",
    "\n",
    "final_df = pd.read_csv(labeled_path)\n",
    "    \n",
    "\n",
    "# Apply feature engineering\n",
    "df_features = feature_engineering(final_df)\n",
    "\n",
    "# Save featured dataset\n",
    "featured_path = \"../src/featured_dataset.csv\"\n",
    "df_features.to_csv(featured_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "564eb0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Feature Engineering Complete\n",
      "============================================================\n",
      "  Total samples: 29899\n",
      "  Features created: 5\n",
      "  NaN values: 0\n",
      "  Inf values: 0\n",
      "  Saved to: ../src/featured_dataset.csv\n",
      "============================================================\n",
      "\n",
      " Feature columns:\n",
      "   - label\n",
      "   - pub_id\n",
      "   - bib_key\n",
      "   - candidate_arxiv_id\n",
      "   - source\n",
      "   - Title_Soft_Jaccard\n",
      "   - Title_Length_Diff\n",
      "   - Author_Overlap_Score\n",
      "   - Author_Levenshtein_Ratio\n",
      "   - Year_Diff\n",
      "\n",
      " Sample of featured dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pub_id</th>\n",
       "      <th>bib_key</th>\n",
       "      <th>candidate_arxiv_id</th>\n",
       "      <th>source</th>\n",
       "      <th>Title_Soft_Jaccard</th>\n",
       "      <th>Title_Length_Diff</th>\n",
       "      <th>Author_Overlap_Score</th>\n",
       "      <th>Author_Levenshtein_Ratio</th>\n",
       "      <th>Year_Diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2211-13755</td>\n",
       "      <td>zhong2018open</td>\n",
       "      <td>2203-11483</td>\n",
       "      <td>manual</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.507594</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2211-13755</td>\n",
       "      <td>zhong2018open</td>\n",
       "      <td>2203-02146</td>\n",
       "      <td>manual</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397661</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2211-13755</td>\n",
       "      <td>zhong2018open</td>\n",
       "      <td>2108-10869</td>\n",
       "      <td>manual</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.233410</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2211-13755</td>\n",
       "      <td>zhong2018open</td>\n",
       "      <td>2108-05773</td>\n",
       "      <td>manual</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>39</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.447958</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2211-13755</td>\n",
       "      <td>zhong2018open</td>\n",
       "      <td>2104-04314</td>\n",
       "      <td>manual</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>13</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.323810</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label      pub_id        bib_key candidate_arxiv_id  source  \\\n",
       "0      0  2211-13755  zhong2018open         2203-11483  manual   \n",
       "1      0  2211-13755  zhong2018open         2203-02146  manual   \n",
       "2      0  2211-13755  zhong2018open         2108-10869  manual   \n",
       "3      0  2211-13755  zhong2018open         2108-05773  manual   \n",
       "4      0  2211-13755  zhong2018open         2104-04314  manual   \n",
       "\n",
       "   Title_Soft_Jaccard  Title_Length_Diff  Author_Overlap_Score  \\\n",
       "0            0.153846                 36                   0.0   \n",
       "1            0.181818                 24                   0.0   \n",
       "2            0.166667                 16                   0.0   \n",
       "3            0.153846                 39                   0.0   \n",
       "4            0.166667                 13                   0.2   \n",
       "\n",
       "   Author_Levenshtein_Ratio  Year_Diff  \n",
       "0                  0.507594        4.0  \n",
       "1                  0.397661        4.0  \n",
       "2                  0.233410        3.0  \n",
       "3                  0.447958        3.0  \n",
       "4                  0.323810        3.0  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Feature Engineering Complete\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  Total samples: {len(df_features)}\")\n",
    "print(f\"  Features created: {len([c for c in df_features.columns if c not in ['label', 'pub_id', 'bib_key', 'candidate_arxiv_id', 'source']])}\")\n",
    "# Check for NaN/Inf\n",
    "print(f\"  NaN values: {df_features.isna().sum().sum()}\")\n",
    "print(f\"  Inf values: {np.isinf(df_features.select_dtypes(include=[np.number])).sum().sum()}\")\n",
    "print(f\"  Saved to: {featured_path}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Show feature columns\n",
    "print(\"\\n Feature columns:\")\n",
    "for col in df_features.columns:\n",
    "    print(f\"   - {col}\")\n",
    "print(\"\\n Sample of featured dataset:\")\n",
    "df_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbb8661",
   "metadata": {},
   "source": [
    "## **Train/Valid/Test Split**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9496612d",
   "metadata": {},
   "source": [
    "### Goal\n",
    "Separate publications by labeling source (**manual vs automatic**), then split the full featured dataset into **train/validation/test** sets in a controlled way, and export each split to CSV.\n",
    "\n",
    "### Workflow\n",
    "1. **Collect publication IDs by labeling source**\n",
    "   - Extract unique `pub_id`s where `source == \"manual\"` → `manual_pub_ids`\n",
    "   - Extract unique `pub_id`s where `source == \"automatic\"` → `auto_pub_ids`\n",
    "   - Print how many publications belong to each source group.\n",
    "\n",
    "2. **Split the dataset into train/val/test**\n",
    "   - Call `split_data(df_features, manual_pub_ids, auto_pub_ids)` to create:\n",
    "     - `df_train`, `df_val`, `df_test`\n",
    "   - (The function likely uses the pub_id groups to avoid leakage and control how manual/auto data is distributed.)\n",
    "\n",
    "3. **Save each split**\n",
    "   - Export to:\n",
    "     - `../src/train_dataset.csv`\n",
    "     - `../src/val_dataset.csv`\n",
    "     - `../src/test_dataset.csv`\n",
    "   - Save without the index column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c144bec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual labeled publications: 5\n",
      "Automatic labeled publications: 10\n",
      "Split summary:\n",
      "  Train: 22171 rows, 11 publications\n",
      "  Valid: 5985 rows, 2 publications\n",
      "  Test:  1743 rows, 2 publications\n"
     ]
    }
   ],
   "source": [
    "# Get paper IDs for manual and automatic sources\n",
    "manual_pub_ids = df_features[df_features['source'] == 'manual']['pub_id'].unique().tolist()\n",
    "auto_pub_ids = df_features[df_features['source'] == 'automatic']['pub_id'].unique().tolist()\n",
    "\n",
    "print(f\"Manual labeled publications: {len(manual_pub_ids)}\")\n",
    "print(f\"Automatic labeled publications: {len(auto_pub_ids)}\")\n",
    "\n",
    "# Split data\n",
    "df_train, df_val, df_test = split_data(df_features, manual_pub_ids=manual_pub_ids, auto_pub_ids=auto_pub_ids)\n",
    "\n",
    "train_path = \"../src/train_dataset.csv\"\n",
    "val_path = \"../src/val_dataset.csv\"\n",
    "test_path = \"../src/test_dataset.csv\"\n",
    "df_train.to_csv(train_path, index=False)\n",
    "df_val.to_csv(val_path, index=False)\n",
    "df_test.to_csv(test_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58933d3",
   "metadata": {},
   "source": [
    "## **Model Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639c329f",
   "metadata": {},
   "source": [
    "### Goal\n",
    "Train a **learning-to-rank model** (via `train_ranker`) to predict the correct arXiv match for each BibTeX entry, using selected engineered features and optional **negative sampling** to handle class imbalance.\n",
    "\n",
    "### Workflow\n",
    "1. **Initialize training configuration**\n",
    "   - Enable negative sampling: `USE_NEGATIVE_SAMPLING = True`\n",
    "   - Sampling parameters:\n",
    "     - `K_NEGATIVES = 15`: number of negatives sampled per positive (or per query group, depending on implementation)\n",
    "     - `HARD_RATIO = 0.6`: fraction of sampled negatives chosen as “hard” (high-score / more confusing negatives)\n",
    "\n",
    "2. **Define and validate feature columns**\n",
    "   - Specify `FEATURE_COLS` (title/author/year features).\n",
    "   - Filter to keep only those actually present in `df_train` → `available_features`.\n",
    "   - Set training target label: `TARGET = \"label\"`.\n",
    "\n",
    "3. **Train the ranker**\n",
    "   - Call `train_ranker(df_train, df_val, features=available_features, target=TARGET, ...)`\n",
    "   - Pass negative sampling settings (`use_negative_sampling`, `k_negatives`, `hard_ratio`) to control training data construction.\n",
    "\n",
    "4. **Save the trained model**\n",
    "   - Persist the trained model to `../src/xgb_ranker.joblib` using `save_model(...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "88c53852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting Model Training\n",
      "============================================================\n",
      "Using 5 features: ['Title_Soft_Jaccard', 'Title_Length_Diff', 'Author_Overlap_Score', 'Author_Levenshtein_Ratio', 'Year_Diff']\n",
      "\n",
      "Data imbalance BEFORE sampling:\n",
      "  Positives: 947\n",
      "  Negatives: 21224\n",
      "  Ratio (pos:neg): 1:22.4\n",
      "\n",
      "Applying negative sampling to training data...\n",
      "\n",
      "Negative Sampling Summary:\n",
      "  Original: 22171 rows\n",
      "  Sampled:  10582 rows\n",
      "  Reduction: 52.3%\n",
      "  Positives: 947\n",
      "  Negatives: 9635\n",
      "  Ratio (pos:neg): 1:10.2\n",
      "Best iteration: 11\n",
      "Best score: 0.7385407993539469\n",
      "Model saved to: ../src\\xgb_ranker.joblib\n",
      "\n",
      "============================================================\n",
      "Training Complete\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Starting Model Training\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "USE_NEGATIVE_SAMPLING = True\n",
    "\n",
    "K_NEGATIVES = 15\n",
    "\n",
    "HARD_RATIO = 0.6\n",
    "\n",
    "# Define feature columns\n",
    "FEATURE_COLS = [\n",
    "    # Title features\n",
    "    \"Title_Soft_Jaccard\",\n",
    "    \"Title_Length_Diff\",\n",
    "    # Author features\n",
    "    \"Author_Overlap_Score\",\n",
    "    \"Author_Levenshtein_Ratio\",\n",
    "    # Year features\n",
    "    \"Year_Diff\"\n",
    "]\n",
    "\n",
    "# Get available features in the DataFrame\n",
    "available_features = [f for f in FEATURE_COLS if f in df_train.columns]\n",
    "print(f\"Using {len(available_features)} features: {available_features}\")\n",
    "\n",
    "TARGET = \"label\"\n",
    "\n",
    "# Check data imbalance before sampling\n",
    "print(f\"\\nData imbalance BEFORE sampling:\")\n",
    "print(f\"  Positives: {len(df_train[df_train[TARGET]==1])}\")\n",
    "print(f\"  Negatives: {len(df_train[df_train[TARGET]==0])}\")\n",
    "print(f\"  Ratio (pos:neg): 1:{len(df_train[df_train[TARGET]==0])/max(1, len(df_train[df_train[TARGET]==1])):.1f}\")\n",
    "\n",
    "# Train with configured negative sampling\n",
    "model = train_ranker(\n",
    "    df_train, \n",
    "    df_val, \n",
    "    features=available_features, \n",
    "    target=TARGET,\n",
    "    use_negative_sampling=USE_NEGATIVE_SAMPLING,\n",
    "    k_negatives=K_NEGATIVES,\n",
    "    hard_ratio=HARD_RATIO\n",
    ")\n",
    "\n",
    "# Save model\n",
    "model_path = save_model(model, \"../src\", filename=\"xgb_ranker.joblib\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Training Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0171fadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Importance:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Author_Levenshtein_Ratio</td>\n",
       "      <td>0.559681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Title_Soft_Jaccard</td>\n",
       "      <td>0.188750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Author_Overlap_Score</td>\n",
       "      <td>0.117087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Year_Diff</td>\n",
       "      <td>0.068390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Title_Length_Diff</td>\n",
       "      <td>0.066093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    feature  importance\n",
       "3  Author_Levenshtein_Ratio    0.559681\n",
       "0        Title_Soft_Jaccard    0.188750\n",
       "2      Author_Overlap_Score    0.117087\n",
       "4                 Year_Diff    0.068390\n",
       "1         Title_Length_Diff    0.066093"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Feature Importance\n",
    "print(\"\\nFeature Importance:\")\n",
    "importance_df = get_feature_importance(model, available_features)\n",
    "display(importance_df)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['feature'], importance_df['importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1989cb42",
   "metadata": {},
   "source": [
    "## **Prediction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa63b9c",
   "metadata": {},
   "source": [
    "### Goal\n",
    "Run the trained ranking model to produce **Top-5 arXiv predictions** for each BibTeX entry on **test/validation/train** splits, then build the corresponding **ground-truth mappings** to support evaluation.\n",
    "\n",
    "### Workflow\n",
    "1. **Predict rankings for each dataset split**\n",
    "   - Call `predict_rankings(model, df_*, available_features, top_k=5)` for:\n",
    "     - `df_test` → `test_predictions`\n",
    "     - `df_val` → `val_predictions`\n",
    "     - `df_train` → `train_predictions`\n",
    "   - Output format is a nested dictionary:\n",
    "     - `{pub_id: {bib_key: [top_k_candidate_arxiv_ids]}}`\n",
    "\n",
    "2. **Extract ground truth labels**\n",
    "   - Convert each labeled DataFrame into ground-truth mappings using `extract_groundtruth(df_*)`:\n",
    "     - `df_test` → `test_groundtruth`\n",
    "     - `df_val` → `val_groundtruth`\n",
    "     - `df_train` → `train_groundtruth`\n",
    "   - Ground truth typically maps each publication (and its bib keys) to the correct arXiv ID(s).\n",
    "\n",
    "3. **Print summary statistics**\n",
    "   - For each split, print:\n",
    "     - total number of ground-truth references extracted (sum of lengths)\n",
    "     - number of publications represented in the ground-truth dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4d8e045e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting on test set...\n",
      "Predicting on validation set...\n",
      "Predicting on train set...\n",
      "\n",
      "Groundtruth extracted:\n",
      "  Test: 51 references across 2 publications\n",
      "  Valid: 128 references across 2 publications\n",
      "  Train: 947 references across 11 publications\n"
     ]
    }
   ],
   "source": [
    "# Predict on test set (returns nested dict: {pub_id: {bib_key: [candidates]}})\n",
    "print(\"Predicting on test set...\")\n",
    "test_predictions = predict_rankings(model, df_test, available_features, top_k=5)\n",
    "\n",
    "# Predict on validation set\n",
    "print(\"Predicting on validation set...\")\n",
    "val_predictions = predict_rankings(model, df_val, available_features, top_k=5)\n",
    "\n",
    "# Predict on train set\n",
    "print(\"Predicting on train set...\")\n",
    "train_predictions = predict_rankings(model, df_train, available_features, top_k=5)\n",
    "\n",
    "test_groundtruth = extract_groundtruth(df_test)\n",
    "val_groundtruth = extract_groundtruth(df_val)\n",
    "train_groundtruth = extract_groundtruth(df_train)\n",
    "\n",
    "print(f\"\\nGroundtruth extracted:\")\n",
    "print(f\"  Test: {sum(len(v) for v in test_groundtruth.values())} references across {len(test_groundtruth)} publications\")\n",
    "print(f\"  Valid: {sum(len(v) for v in val_groundtruth.values())} references across {len(val_groundtruth)} publications\")\n",
    "print(f\"  Train: {sum(len(v) for v in train_groundtruth.values())} references across {len(train_groundtruth)} publications\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9d07e5",
   "metadata": {},
   "source": [
    "## **Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c678f12b",
   "metadata": {},
   "source": [
    "### Goal\n",
    "Convert nested prediction/ground-truth structures into a flat format suitable for evaluation, then compute **MRR (Mean Reciprocal Rank)** on the **test set**.\n",
    "\n",
    "### Workflow\n",
    "1. **Flatten predictions and ground truth**\n",
    "   - Convert nested predictions (`test_predictions`) into a flat structure using `flatten_nested_dict(...)`.\n",
    "   - Convert nested ground truth (`test_groundtruth`) into a flat structure using `flatten_groundtruth(...)`.\n",
    "   - This makes both outputs align in a consistent key/value format for metric computation.\n",
    "\n",
    "2. **Evaluate using MRR**\n",
    "   - Compute MRR with:\n",
    "     - `calculate_mrr(test_predictions_flat, test_groundtruth_flat)`\n",
    "   - Print the final score as `Test MRR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b662ba28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Evaluating on TEST set\n",
      "======================================================================\n",
      "  Test MRR: 0.9026\n"
     ]
    }
   ],
   "source": [
    "# Flatten for evaluation\n",
    "test_predictions_flat = flatten_nested_dict(test_predictions)\n",
    "test_groundtruth_flat = flatten_groundtruth(test_groundtruth)\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"=\"*70)\n",
    "print(\"Evaluating on TEST set\")\n",
    "print(\"=\"*70)\n",
    "test_mrr = calculate_mrr(test_predictions_flat, test_groundtruth_flat)\n",
    "print(f\"  Test MRR: {test_mrr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619a70c5",
   "metadata": {},
   "source": [
    "## **Export pred.json files**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1fe1c7",
   "metadata": {},
   "source": [
    "### Goal\n",
    "Run the trained ranker on the **entire featured dataset** (train + validation + test), then export a standardized `pred.json` file **per publication** containing:\n",
    "- which split the publication belongs to (`partition`)\n",
    "- its ground truth labels (`groundtruth`)\n",
    "- the model’s Top-K predictions (`prediction`)\n",
    "\n",
    "### Workflow\n",
    "1. **Load the full featured dataset**\n",
    "   - Read `../src/featured_dataset.csv` into `df_all`.\n",
    "\n",
    "2. **Collect publication IDs by split**\n",
    "   - Build sets of `pub_id` from the already-created splits:\n",
    "     - `train_pub_ids`, `val_pub_ids`, `test_pub_ids`\n",
    "\n",
    "3. **Predict rankings for all records**\n",
    "   - Call `predict_rankings(model, df_all, available_features, top_k=5)`\n",
    "   - Output is nested: `{pub_id: {bib_key: [top_k_candidates]}}`\n",
    "\n",
    "4. **Extract ground truth for all data**\n",
    "   - Call `extract_groundtruth(df_all)` to get nested ground truth per publication.\n",
    "\n",
    "5. **Export `pred.json` per publication**\n",
    "   - For each `pub_id` in `all_predictions`:\n",
    "     - Create output folder: `OUTPUT_DIR/<pub_id>/`\n",
    "     - Determine the publication’s split using `get_partition(...)`\n",
    "     - Build a JSON object:\n",
    "       - `\"partition\"`: train/valid/test/unknown\n",
    "       - `\"groundtruth\"`: `all_groundtruth.get(pub_id, {})`\n",
    "       - `\"prediction\"`: model predictions for that pub\n",
    "     - Save to `OUTPUT_DIR/<pub_id>/pred.json`\n",
    "     - Update counters in `exported` by partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a875c639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partition(pub_id, test_pub_ids, val_pub_ids, train_pub_ids):\n",
    "    \"\"\"Determine which partition a publication belongs to\"\"\"\n",
    "    if pub_id in test_pub_ids:\n",
    "        return \"test\"\n",
    "    elif pub_id in val_pub_ids:\n",
    "        return \"valid\"\n",
    "    elif pub_id in train_pub_ids:\n",
    "        return \"train\"\n",
    "    else:\n",
    "        return \"unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e02211cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting on all data (Train, Valid and Test set)...\n",
      "\n",
      "============================================================\n",
      "Export Complete\n",
      "============================================================\n",
      "  pred.json files exported:\n",
      "    - Train: 11\n",
      "    - Valid: 2\n",
      "    - Test:  2\n",
      "  Output folder: 23127453/\n"
     ]
    }
   ],
   "source": [
    "# Load all data (Train, Valid and Test set) to predict\n",
    "df_all = pd.read_csv(\"../src/featured_dataset.csv\")\n",
    "\n",
    "# Get pub_ids for each partition\n",
    "train_pub_ids = set(df_train['pub_id'].unique())\n",
    "val_pub_ids = set(df_val['pub_id'].unique())\n",
    "test_pub_ids = set(df_test['pub_id'].unique())\n",
    "\n",
    "# Predict rankings for ALL data (returns nested dict: {pub_id: {bib_key: [candidates]}})\n",
    "print(\"Predicting on all data (Train, Valid and Test set)...\")\n",
    "all_predictions = predict_rankings(model, df_all, available_features, top_k=5)\n",
    "\n",
    "# Extract groundtruth (nested dict: {pub_id: {bib_key: arxiv_id}})\n",
    "all_groundtruth = extract_groundtruth(df_all)\n",
    "\n",
    "# Export pred.json for each publication\n",
    "exported = {\"train\": 0, \"valid\": 0, \"test\": 0, \"unknown\": 0}\n",
    "for pub_id, predictions in all_predictions.items():\n",
    "    pub_folder = Path(OUTPUT_DIR + \"/\" + pub_id)\n",
    "    pub_folder.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Determine correct partition for this publication\n",
    "    partition = get_partition(pub_id, test_pub_ids, val_pub_ids, train_pub_ids)\n",
    "    \n",
    "    # Build pred.json\n",
    "    pred_json = {\n",
    "        \"partition\": partition,\n",
    "        \"groundtruth\": all_groundtruth.get(pub_id, {}),\n",
    "        \"prediction\": predictions\n",
    "    }\n",
    "    \n",
    "    # Export to file\n",
    "    output_file = pub_folder / \"pred.json\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(pred_json, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    exported[partition] += 1\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Export Complete\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  pred.json files exported:\")\n",
    "print(f\"    - Train: {exported['train']}\")\n",
    "print(f\"    - Valid: {exported['valid']}\")\n",
    "print(f\"    - Test:  {exported['test']}\")\n",
    "print(f\"  Output folder: {OUTPUT_DIR}/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
