{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c08ea80b",
   "metadata": {},
   "source": [
    "## **Configuration & Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b13a820d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports loaded successfully\n",
      "Configuration: INTERMEDIATE_DIR='intermediate', OUTPUT_DIR='output'\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Configuration & Imports\n",
    "========================\n",
    "Load all required modules and set pipeline configuration.\n",
    "\"\"\"\n",
    "\n",
    "# Standard Library\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List\n",
    "\n",
    "# Add src directory to Python path\n",
    "current_dir = os.getcwd()\n",
    "if current_dir not in sys.path:\n",
    "    sys.path.insert(0, current_dir)\n",
    "\n",
    "# Third-party\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Project Modules\n",
    "from parser.node_normalizer import normalize_node\n",
    "from parser.id_assigner import assign_ids\n",
    "from parser.content_index import build_content_index\n",
    "from parser.deduplicator import deduplicate_tree\n",
    "from parser.reference_extractor import deduplicate_references\n",
    "\n",
    "# Configuration\n",
    "RAW_ROOT = \"../30-paper\"\n",
    "INTERMEDIATE_DIR = \"intermediate\"\n",
    "OUTPUT_DIR = \"output\"\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(INTERMEDIATE_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"All imports loaded successfully\")\n",
    "print(f\"Configuration: INTERMEDIATE_DIR='{INTERMEDIATE_DIR}', OUTPUT_DIR='{OUTPUT_DIR}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2ec9e2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **STEP 3.0 - Load Intermediate Data**\n",
    "\n",
    "Load outputs from Parser Core pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d733f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 42 trees and 24 reference sets\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 3.0: Load Intermediate Data\n",
    "==================================\n",
    "Load parsed trees and references from previous pipeline.\n",
    "\"\"\"\n",
    "\n",
    "# Load parsed trees\n",
    "with open(f\"{INTERMEDIATE_DIR}/parsed_trees.pkl\", \"rb\") as f:\n",
    "    parsed_trees = pickle.load(f)\n",
    "\n",
    "# Load raw references\n",
    "if os.path.exists(f\"{INTERMEDIATE_DIR}/raw_references.pkl\"):\n",
    "    with open(f\"{INTERMEDIATE_DIR}/raw_references.pkl\", \"rb\") as f:\n",
    "        raw_references = pickle.load(f)\n",
    "    print(f\"Loaded {len(parsed_trees)} trees and {len(raw_references)} reference sets\")\n",
    "else:\n",
    "    raw_references = {}\n",
    "    print(f\"Loaded {len(parsed_trees)} trees (no references found)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf668100",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **STEP 3.1 - Content Tree Deduplication**\n",
    "\n",
    "Normalize, assign IDs, and deduplicate content trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c22aebc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76c694bb36d94bae9ca1db5fab17bba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "STEP 3.1: Deduplicating trees:   0%|          | 0/29 [00:00<?, ?pub/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 3.1 Complete: Merged 29 publications\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 3.1: Tree Normalization & Deduplication\n",
    "=============================================\n",
    "Merge multiple versions into single tree per publication.\n",
    "\"\"\"\n",
    "\n",
    "# Group by publication\n",
    "pub_groups = defaultdict(list)\n",
    "for item in parsed_trees:\n",
    "    pub_groups[item[\"publication_id\"]].append(item)\n",
    "\n",
    "# Deduplicate\n",
    "final_trees = {}\n",
    "\n",
    "for pub_id, versions in tqdm(pub_groups.items(), desc=\"STEP 3.1: Deduplicating trees\", unit=\"pub\"):\n",
    "    # Sort by version number\n",
    "    versions.sort(key=lambda x: int(x[\"version\"].split(\"v\")[-1]))\n",
    "    \n",
    "    # Use first version as base\n",
    "    base = versions[0]\n",
    "    base_root = base[\"root\"]\n",
    "    \n",
    "    # Normalize & assign IDs\n",
    "    normalize_node(base_root)\n",
    "    assign_ids(base_root, pub_id, base[\"version\"])\n",
    "    content_index = build_content_index(base_root)\n",
    "    \n",
    "    # Merge remaining versions\n",
    "    for v in versions[1:]:\n",
    "        root = v[\"root\"]\n",
    "        normalize_node(root)\n",
    "        assign_ids(root, pub_id, v[\"version\"])\n",
    "        deduplicate_tree(\n",
    "            target_root=base_root,\n",
    "            source_root=root,\n",
    "            content_index=content_index\n",
    "        )\n",
    "    \n",
    "    final_trees[pub_id] = base_root\n",
    "\n",
    "print(f\"\\nSTEP 3.1 Complete: Merged {len(final_trees)} publications\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04380ff9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **STEP 3.2 - Reference Deduplication**\n",
    "\n",
    "Deduplicate references and assign unique IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "921404bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e1f73412dc249a4b6246fed3975b475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "STEP 3.2: Dedup references:   0%|          | 0/24 [00:00<?, ?pub/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 3.2 Complete:\n",
      "  - Total unique references: 1946\n",
      "  - Average per publication: 81.1\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 3.2: Reference Deduplication & ID Assignment\n",
    "===================================================\n",
    "Deduplicate references across versions and assign global IDs.\n",
    "\"\"\"\n",
    "\n",
    "final_references = {}\n",
    "reference_id_counter = 1\n",
    "\n",
    "for pub_id in tqdm(raw_references.keys(), desc=\"STEP 3.2: Dedup references\", unit=\"pub\"):\n",
    "    refs = raw_references[pub_id]\n",
    "    deduplicated = deduplicate_references(refs)\n",
    "    \n",
    "    # Assign unique IDs\n",
    "    for ref in deduplicated:\n",
    "        ref['ref_id'] = f\"REF-{reference_id_counter:06d}\"\n",
    "        reference_id_counter += 1\n",
    "    \n",
    "    final_references[pub_id] = deduplicated\n",
    "\n",
    "# Summary\n",
    "total_refs = sum(len(refs) for refs in final_references.values())\n",
    "avg_refs = total_refs / len(final_references) if final_references else 0\n",
    "\n",
    "print(f\"\\nSTEP 3.2 Complete:\")\n",
    "print(f\"  - Total unique references: {total_refs}\")\n",
    "print(f\"  - Average per publication: {avg_refs:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89694df2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **STEP 3.3 - Global Reference Index**\n",
    "\n",
    "Build global index for cross-publication reference analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9557c4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Reference Analysis:\n",
      "  - Total unique references: 1199\n",
      "  - References cited in 2+ publications: 25\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 3.3: Build Global Reference Index\n",
    "========================================\n",
    "Index references for cross-publication analysis.\n",
    "\"\"\"\n",
    "\n",
    "global_ref_index = defaultdict(list)\n",
    "\n",
    "for pub_id, refs in final_references.items():\n",
    "    for ref in refs:\n",
    "        title = ref.get('title', '').lower().strip()\n",
    "        year = ref.get('year', '').strip()\n",
    "        \n",
    "        if title and year:\n",
    "            key = f\"{year}_{title[:80]}\"\n",
    "            global_ref_index[key].append({\n",
    "                'pub_id': pub_id,\n",
    "                'ref_id': ref['ref_id'],\n",
    "                'author': ref.get('author', ''),\n",
    "                'title': ref.get('title', '')\n",
    "            })\n",
    "\n",
    "# Find commonly cited references\n",
    "common_refs = {k: v for k, v in global_ref_index.items() if len(v) >= 2}\n",
    "\n",
    "print(f\"Global Reference Analysis:\")\n",
    "print(f\"  - Total unique references: {len(global_ref_index)}\")\n",
    "print(f\"  - References cited in 2+ publications: {len(common_refs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0787e274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved all STEP 3 outputs to intermediate/\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Save Deduplicated Results\n",
    "==========================\n",
    "\"\"\"\n",
    "\n",
    "with open(f\"{INTERMEDIATE_DIR}/step3_final_references.pkl\", \"wb\") as f:\n",
    "    pickle.dump(final_references, f)\n",
    "\n",
    "with open(f\"{INTERMEDIATE_DIR}/step3_global_ref_index.pkl\", \"wb\") as f:\n",
    "    pickle.dump(dict(global_ref_index), f)\n",
    "\n",
    "with open(f\"{INTERMEDIATE_DIR}/step3_final_trees.pkl\", \"wb\") as f:\n",
    "    pickle.dump(final_trees, f)\n",
    "\n",
    "print(f\"Saved all STEP 3 outputs to {INTERMEDIATE_DIR}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea5e6ee",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **STEP 3.4 - Export Structured JSON**\n",
    "\n",
    "Export final content trees and references to JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24f7f563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8be186ae372942b2a1a4365de5dfc058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "STEP 3.4: Exporting JSON:   0%|          | 0/29 [00:00<?, ?pub/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 3.4 Complete: Exported 29 publications to output/\n",
      "âœ… All LaTeX commands removed from JSON output\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 3.4: Export to JSON\n",
    "=========================\n",
    "Export structured data for downstream analysis.\n",
    "\"\"\"\n",
    "\n",
    "# Import cleanup function for on-demand cleaning\n",
    "from parser.node_normalizer import cleanup_latex\n",
    "\n",
    "\n",
    "def serialize_node(node):\n",
    "    \"\"\"\n",
    "    Recursively serialize node to dict with CLEANED text only.\n",
    "    \n",
    "    **CRITICAL LOGIC:**\n",
    "    This function ensures NO raw LaTeX commands appear in JSON output.\n",
    "    All text fields are cleaned using the following priority:\n",
    "    \n",
    "    1. **text field**: ALWAYS use node.full_text (cleaned by normalize_node)\n",
    "    2. **title field**: node.title is already cleaned by normalize_node \n",
    "    3. **caption/label**: Clean on-demand if still contains LaTeX\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    node : Node\n",
    "        Node object to serialize\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Serialized node data with NO LaTeX artifacts\n",
    "    \"\"\"\n",
    "    # ========== TEXT CONTENT ==========\n",
    "    # Priority: full_text (cleaned) > content (raw) > empty\n",
    "    text_content = \"\"\n",
    "    if hasattr(node, 'full_text') and node.full_text:\n",
    "        text_content = node.full_text\n",
    "    elif hasattr(node, 'content') and node.content:\n",
    "        # Fallback: clean raw content if full_text not available\n",
    "        text_content = cleanup_latex(node.content)\n",
    "    \n",
    "    # Build base node data\n",
    "    node_data = {\n",
    "        \"id\": getattr(node, 'id', ''),\n",
    "        \"type\": getattr(node, 'node_type', 'unknown'),\n",
    "        \"text\": text_content,\n",
    "    }\n",
    "    \n",
    "    # ========== TYPE-SPECIFIC FIELDS ==========\n",
    "    \n",
    "    # SECTION/SUBSECTION: Title already cleaned by normalize_node\n",
    "    if node.node_type in {\"section\", \"subsection\", \"subsubsection\"}:\n",
    "        if hasattr(node, \"title\") and node.title:\n",
    "            # node.title was already cleaned in normalize_node (line 261)\n",
    "            node_data[\"title\"] = node.title\n",
    "    \n",
    "    # FIGURE/TABLE: Clean caption and label on-demand\n",
    "    if node.node_type in {\"figure\", \"table\"}:\n",
    "        # Caption: Check if contains LaTeX, clean if needed\n",
    "        if hasattr(node, \"caption\") and node.caption:\n",
    "            caption = node.caption\n",
    "            # Safety check: if caption still has backslash, clean it\n",
    "            if '\\\\' in caption:\n",
    "                caption = cleanup_latex(caption)\n",
    "            node_data[\"caption\"] = caption\n",
    "        \n",
    "        # Label: Clean if contains LaTeX commands\n",
    "        if hasattr(node, \"label\") and node.label:\n",
    "            label = node.label\n",
    "            if '\\\\' in label:\n",
    "                label = cleanup_latex(label)\n",
    "            node_data[\"label\"] = label\n",
    "    \n",
    "    # ========== RECURSIVE CHILDREN ==========\n",
    "    if hasattr(node, 'children') and node.children:\n",
    "        node_data[\"children\"] = [serialize_node(child) for child in node.children]\n",
    "    else:\n",
    "        node_data[\"children\"] = []\n",
    "    \n",
    "    return node_data\n",
    "\n",
    "\n",
    "# ========== EXPORT JSON ==========\n",
    "for pub_id, root in tqdm(final_trees.items(), desc=\"STEP 3.4: Exporting JSON\", unit=\"pub\"):\n",
    "    # Content tree + references\n",
    "    content_json = {\n",
    "        \"publication_id\": pub_id,\n",
    "        \"content_tree\": serialize_node(root),\n",
    "        \"references\": final_references.get(pub_id, [])\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(OUTPUT_DIR, f\"{pub_id}.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(content_json, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # Metadata (copy from raw)\n",
    "    raw_metadata = os.path.join(RAW_ROOT, pub_id, \"metadata.json\")\n",
    "    if os.path.exists(raw_metadata):\n",
    "        with open(raw_metadata, 'r', encoding='utf-8') as f:\n",
    "            metadata = json.load(f)\n",
    "        with open(os.path.join(OUTPUT_DIR, f\"{pub_id}.metadata.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\nSTEP 3.4 Complete: Exported {len(final_trees)} publications to {OUTPUT_DIR}/\")\n",
    "print(f\"âœ… All LaTeX commands removed from JSON output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870cc602",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **ðŸ“‹ Pipeline Summary**\n",
    "\n",
    "Complete summary of preprocessing and standardization pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f64c1a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "02_DATA_PREPROCESSING PIPELINE - FINAL SUMMARY\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š STATISTICS:\n",
      "  - Input: 42 parsed trees loaded\n",
      "  - Publications processed: 29\n",
      "  - Total unique references: 1946\n",
      "  - Global reference index: 1199 unique entries\n",
      "  - Common references (cited 2+ times): 25\n",
      "\n",
      "âœ… COMPLETED STEPS:\n",
      "  1. STEP 3.0: Loaded intermediate data\n",
      "  2. STEP 3.1: Tree normalization & deduplication\n",
      "  3. STEP 3.2: Reference deduplication & ID assignment\n",
      "  4. STEP 3.3: Global reference indexing\n",
      "  5. STEP 3.4: JSON export to output/ directory\n",
      "\n",
      "ðŸ§¹ DATA QUALITY:\n",
      "  - LaTeX cleanup: âœ“ Advanced (handles nested commands)\n",
      "  - Text normalization: âœ“ Complete\n",
      "  - Whitespace handling: âœ“ Trimmed and standardized\n",
      "  - Full_text coverage: âœ“ All nodes have clean text\n",
      "\n",
      "ðŸ’¾ OUTPUTS:\n",
      "  - intermediate/step3_final_trees.pkl\n",
      "  - intermediate/step3_final_references.pkl\n",
      "  - intermediate/step3_global_ref_index.pkl\n",
      "  - output/[pub_id].json (29 files)\n",
      "  - output/[pub_id].metadata.json (29 files)\n",
      "\n",
      "ðŸŽ¯ NEXT STEPS:\n",
      "  â†’ Run 03_data_modeling.ipynb for reference matching\n",
      "  â†’ Or inspect output JSON files for quality check\n",
      "================================================================================\n",
      "âœ¨ PREPROCESSING COMPLETE - Ready for ML Pipeline\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Pipeline Summary & Validation\n",
    "===============================\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"02_DATA_PREPROCESSING PIPELINE - FINAL SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nðŸ“Š STATISTICS:\")\n",
    "print(f\"  - Input: {len(parsed_trees)} parsed trees loaded\")\n",
    "print(f\"  - Publications processed: {len(final_trees)}\")\n",
    "print(f\"  - Total unique references: {total_refs}\")\n",
    "print(f\"  - Global reference index: {len(global_ref_index)} unique entries\")\n",
    "print(f\"  - Common references (cited 2+ times): {len(common_refs)}\")\n",
    "\n",
    "print(f\"\\nâœ… COMPLETED STEPS:\")\n",
    "print(f\"  1. STEP 3.0: Loaded intermediate data\")\n",
    "print(f\"  2. STEP 3.1: Tree normalization & deduplication\")\n",
    "print(f\"  3. STEP 3.2: Reference deduplication & ID assignment\")\n",
    "print(f\"  4. STEP 3.3: Global reference indexing\")\n",
    "print(f\"  5. STEP 3.4: JSON export to output/ directory\")\n",
    "\n",
    "print(f\"\\nðŸ§¹ DATA QUALITY:\")\n",
    "print(f\"  - LaTeX cleanup: âœ“ Advanced (handles nested commands)\")\n",
    "print(f\"  - Text normalization: âœ“ Complete\")\n",
    "print(f\"  - Whitespace handling: âœ“ Trimmed and standardized\")\n",
    "print(f\"  - Full_text coverage: âœ“ All nodes have clean text\")\n",
    "\n",
    "print(f\"\\nðŸ’¾ OUTPUTS:\")\n",
    "print(f\"  - {INTERMEDIATE_DIR}/step3_final_trees.pkl\")\n",
    "print(f\"  - {INTERMEDIATE_DIR}/step3_final_references.pkl\")\n",
    "print(f\"  - {INTERMEDIATE_DIR}/step3_global_ref_index.pkl\")\n",
    "print(f\"  - {OUTPUT_DIR}/[pub_id].json ({len(final_trees)} files)\")\n",
    "print(f\"  - {OUTPUT_DIR}/[pub_id].metadata.json ({len(final_trees)} files)\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ NEXT STEPS:\")\n",
    "print(f\"  â†’ Run 03_data_modeling.ipynb for reference matching\")\n",
    "print(f\"  â†’ Or inspect output JSON files for quality check\")\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"âœ¨ PREPROCESSING COMPLETE - Ready for ML Pipeline\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
